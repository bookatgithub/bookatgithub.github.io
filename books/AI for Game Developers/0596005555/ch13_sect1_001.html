<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>13.1 What is a Bayesian Network?</title>
<link rel="STYLESHEET" type="text/css" href="images/style.css">
<link rel="STYLESHEET" type="text/css" href="images/docsafari.css">
</head>
<body >
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#e6e6e6">
<tr style="background-image: url(images/tile_back.gif);">
<td class="v2" align="left" width="30%">
<a href="ch13.html"><img src="images/previous.gif" width="70" height="19" border="0" align="absmiddle" alt="Previous Section"></a>
</td>
<td class="v2" align="center" width="40%">
<a href="main.html" style="color:white;text-decoration:none;text-underline:none">&nbsp;&lt;&nbsp;Day Day Up&nbsp;&gt;&nbsp;</a>
</td>
<td class="v2" align="right" width="30%">
<a href="ch13_sect1_002.html"><img src="images/next.gif" width="70" height="19" border="0" align="absmiddle" alt="Next Section"></a>
</td>
</tr>
</table>
<br>
<table width="100%" border="0" cellspacing="0" cellpadding="0"><tr><td valign="top"><A NAME="ch13_sect1_001"></A>
<H3 class="docSection1Title" id="">13.1 What is a Bayesian Network?</H3>
<P class="docText"><span class="docEmphasis">Bayesian networks</span> are graphs that compactly represent the relationship between random variables for a given problem. These graphs aid in performing reasoning or decision making in the face of uncertainty. Such reasoning relies heavily on Bayes' rule, which we discussed in <A class="docLink" HREF="ch12.html#ch12">Chapter 12</A>. In this chapter, we use simple Bayesian networks to model specific game scenarios that require NPCs to make decisions given uncertain information about the game world. Before looking at some specific examples, let's go over the details of Bayesian networks.</P>
<A NAME="ch13_sect2_001"></A>
<H4 class="docSection2Title">13.2.1 Structure</H4>
<P class="docText">Bayesian networks consist of nodes representing random variables and arcs or links representing the causal relationship between variables. <A class="docLink" HREF="#ch13_fig01">Figure 13-1</A> shows an example Bayesian network. Imagine a game in which an NPC can encounter a chest that can be locked or unlocked. Whether it is locked depends on whether it contains treasure or whether it is trapped.</P>
<A NAME="ch13_fig01"></A><p><CENTER>
<H5 class="docFigureTitle">Figure 13-1. Example Bayesian network</H5>
<IMG BORDER="0" id="" SRC="images/0596005555/figs/ch13_fig01.jpg" ALT="figs/ch13_fig01.jpg">
</CENTER></p><br>
<P class="docText">In this example, the nodes labeled <span class="docEmphasis">T</span>, <span class="docEmphasis">Tr</span>, and <span class="docEmphasis">L</span> represent random variables (often referred to as events in the probabilistic sense). The arrows connecting each node represent causal relationships. You can think of nodes at the tail of the arrows as parents and nodes at the head of the arrows as children. Here, parents cause children. For example, <A class="docLink" HREF="#ch13_fig01">Figure 13-1</A> shows that <span class="docEmphasis">Locked</span> is caused by <span class="docEmphasis">Trapped</span> or <span class="docEmphasis">Treasure</span> or both <span class="docEmphasis">Trapped</span> and <span class="docEmphasis">Treasure</span>. You should be aware that this causal relationship is probabilistic and not certain. For example, the chest being <span class="docEmphasis">Trapped</span> does not necessarily always cause the chest to be <span class="docEmphasis">Locked</span>. There's a certain probability that <span class="docEmphasis">Trapped</span> might cause <span class="docEmphasis">Locked</span>, but it's possible that <span class="docEmphasis">Trapped</span> might not cause <span class="docEmphasis">Locked</span>.</P>
<P class="docText">You measure the strength of the connections between events in terms of probabilities. Each node has an associated conditional probability table that gives the probability of any outcome of the child event given all possible combinations of outcomes of its parents. For our purposes, we're going to consider discrete events only. What we mean here is that any event, any variable, can take on any one of a set of discrete values. These values are assumed to be mutually exclusive and exhaustive. For example, <span class="docEmphasis">Locked</span> can take on values of <span class="docEmphasis">TRUE</span> or <span class="docEmphasis">FALSE</span>.</P>
<P class="docText">Let's assume that <span class="docEmphasis">Trapped</span> can be either <span class="docEmphasis">TRUE</span> or <span class="docEmphasis">FALSE</span>. Let's also assume <span class="docEmphasis">Treasure</span> can be either <span class="docEmphasis">TRUE</span> or <span class="docEmphasis">FALSE</span>. If <span class="docEmphasis">Locked</span> can take on the values <span class="docEmphasis">TRUE</span> or <span class="docEmphasis">FALSE</span>, we need a conditional probability table for <span class="docEmphasis">Locked</span> that gives the probability of <span class="docEmphasis">Locked</span> being <span class="docEmphasis">TRUE</span> given every combination of values for <span class="docEmphasis">Trapped</span> and <span class="docEmphasis">Treasure</span>, and the probability of <span class="docEmphasis">Locked</span> being false given every combination of values for <span class="docEmphasis">Trapped</span> and <span class="docEmphasis">Treasure</span>. <A class="docLink" HREF="#ch13_tab01">Table 13-1</A> summarizes the conditional probability table for <span class="docEmphasis">Locked</span> in this case.</P>
<A NAME="ch13_tab01"></A><P><TABLE CELLSPACING="0" FRAME="hsides" RULES="all" CELLPADDING="4" WIDTH="100%"><CAPTION><h5 class="docTableTitle">Table 13.1. Example conditional probability table</h5></CAPTION><COLGROUP><COL><COL><COL><COL></COLGROUP><THEAD><TR><TH class="thead">&nbsp;</TH><TH class="thead">&nbsp;</TH><TH class="bottomBorder thead" align="center" colspan="2"><P class="docText">Probability of <span class="docEmphasis">Locked</span></P></TH></TR><TR><TH class="thead"><P class="docText">Value of <span class="docEmphasis">Trapped</span></P></TH><TH class="thead"><P class="docText">Value of <span class="docEmphasis">Treasure</span></P></TH><TH class="thead"><P class="docText">L = TRUE</P></TH><TH class="thead"><P class="docText">L = FALSE</P></TH></TR></THEAD><TR><TD class="docTableCell"><P class="docText">T</P></TD><TD class="docTableCell"><P class="docText">T</P></TD><TD class="docTableCell"><P class="docText">P(L|T<img src=images/ent/U2229.GIF border=0>Tr)</P></TD><TD class="docTableCell"><P class="docText">P(~L|T<img src=images/ent/U2229.GIF border=0>Tr)</P></TD></TR><TR><TD class="docTableCell"><P class="docText">T</P></TD><TD class="docTableCell"><P class="docText">F</P></TD><TD class="docTableCell"><P class="docText">P(L|T<img src=images/ent/U2229.GIF border=0>~Tr)</P></TD><TD class="docTableCell"><P class="docText">P(~L|T<img src=images/ent/U2229.GIF border=0>~Tr)</P></TD></TR><TR><TD class="docTableCell"><P class="docText">F</P></TD><TD class="docTableCell"><P class="docText">T</P></TD><TD class="docTableCell"><P class="docText">P(L|~T<img src=images/ent/U2229.GIF border=0>Tr)</P></TD><TD class="docTableCell"><P class="docText">P(~L|~T<img src=images/ent/U2229.GIF border=0>Tr)</P></TD></TR><TR><TD class="docTableCell"><P class="docText">F</P></TD><TD class="docTableCell"><P class="docText">F</P></TD><TD class="docTableCell"><P class="docText">P(L|~T<img src=images/ent/U2229.GIF border=0>~Tr)</P></TD><TD class="docTableCell"><P class="docText">P(~L|~T<img src=images/ent/U2229.GIF border=0>~Tr)</P></TD></TR></TABLE></P><br>
<P class="docText">In this table, the first two columns show all combinations of values for <span class="docEmphasis">Trapped</span> and <span class="docEmphasis">Treasure</span>. The third column shows the probability that <span class="docEmphasis">Locked=TRUE</span> given each combination of values for <span class="docEmphasis">Trapped</span> and <span class="docEmphasis">Treasure</span>, while the last column shows the probability that <span class="docEmphasis">Locked=FALSE</span> given each combination of values for <span class="docEmphasis">Trapped</span> and <span class="docEmphasis">Treasure</span>. The tilde symbol, ~, as used here indicates the conjugate of a binary event. If <span class="docEmphasis">P(T)</span> is the probability of <span class="docEmphasis">Trapped</span> being equal to <span class="docEmphasis">TRUE</span>, <span class="docEmphasis">P(~T)</span> is the probability of <span class="docEmphasis">Trapped</span> being equal to <span class="docEmphasis">FALSE</span>. Note that each of the three events considered here are binary in that they each can take on only one of two values. This yields the 2 x 4 set of conditional probabilities for <span class="docEmphasis">Locked</span>, as shown in <A class="docLink" HREF="#ch13_tab01">Table 13-1</A>.</P>
<P class="docText">As the number of possible values for these events gets larger, or as the number of parent nodes for a given child node goes up, the number of entries in the conditional probability table for the child node increases exponentially. This is one of the biggest deterrents for using Bayesian methods in games. Not only does it become difficult to determine all of these conditional probabilities, but also as the size of the network increases, the computational requirements become prohibitive for real-time games. (Technically Bayesian networks are considered NP-hard, which means they are computationally too expensive for large numbers of nodes.)</P>
<P class="docText">Keep in mind that every child node will require a conditional probability table. So-called <span class="docEmphasis">root nodes</span>, nodes that don't have parents—events <span class="docEmphasis">Trapped</span> and <span class="docEmphasis">Treasure</span> in this example—don't have conditional probability tables. Instead, they have what are called <span class="docEmphasis">prior probability tables</span> which contain the probabilities of these events taking on each of their possible values. The term <span class="docEmphasis">prior</span> used here means that these are probabilities for root nodes before we make adjustments to the probabilities given new information somewhere else in the network. Updated probabilities given new information are called <span class="docEmphasis">posterior probabilities</span>. We'll see examples of this sort of calculation later.</P>
<P class="docText">The complexity we discussed here is a major incentive for keeping Bayesian networks for use in games simple and specific. For example, theoretically you could construct a Bayesian network to control every aspect of an NPC unit. You could have nodes in the network representing decisions to chase or evade, and still other nodes to represent turn left, turn right, and so on. The trouble with this approach is that the networks become incredibly complex and difficult to set up, solve, and test. Further, the required conditional probability tables become so large that you'd have to resort to some form of training to figure them out rather than specify them. We don't advocate this approach.</P>
<P class="docText">As we mentioned in <A class="docLink" HREF="ch01.html#ch01">Chapter 1</A>, and as we'll discuss in <A class="docLink" HREF="ch14.html#ch14">Chapter 14</A> on neural networks, we recommend that you use Bayesian methods for very specific decision-making problems and leave the other AI tasks to other methods that are better suited for them. Why use a Bayesian network to steer a chasing unit when reliable, easy, and robust deterministic methods are available for that task? Use the Bayesian network to decide whether to chase or evade and let other algorithms take over to handle the actual chasing or evading.</P>

<A NAME="ch13_sect2_002"></A>
<H4 class="docSection2Title">13.2.2 Inference</H4>
<P class="docText">You can make three basic types of reasoning or inference using Bayesian networks. For this discussion, we'll refer to the simple networks shown in <A class="docLink" HREF="#ch13_fig02">Figure 13-2</A>.</P>
<A NAME="ch13_fig02"></A><p><CENTER>
<H5 class="docFigureTitle">Figure 13-2. Simple networks</H5>
<IMG BORDER="0" id="" SRC="images/0596005555/figs/ch13_fig02.jpg" ALT="figs/ch13_fig02.jpg">
</CENTER></p><br>
<P class="docText">The network on the left is called a <span class="docEmphasis">causal chain</span>, in this case a three-node chain. The network on the upper right is called a <span class="docEmphasis">common cause network</span>. It's also more commonly referred to as a <span class="docEmphasis">na&iuml;ve Bayesian network</span> or <span class="docEmphasis">Bayesian classifier</span>. The network on the lower right is called a <span class="docEmphasis">common effect network</span>. The three basic types of reasoning are as follows:</P>
<DL class="docList"><br><p><DT><I><span class="docPubcolor">Diagnostic reasoning</span></I></DT></p>
<DD><P class="docList">Diagnostic reasoning is probably the most common type of reasoning using Bayesian networks. This sort of reasoning, along with Bayesian classifier networks, is used heavily in medical diagnostics. For example, referring to the network on the upper right in <A class="docLink" HREF="#ch13_fig02">Figure 13-2</A>, <span class="docEmphasis">A</span> would be a disease and <span class="docEmphasis">B</span> and <span class="docEmphasis">C</span> symptoms. Given the symptoms presented, the doctor could make inferences as to the probability of the disease being present.</P></DD><br><p><DT><I><span class="docPubcolor">Predictive reasoning</span></I></DT></p>
<DD><P class="docList">Predictive reasoning involves making inferences about effects given information on causes. For example, referring to the network on the left in <A class="docLink" HREF="#ch13_fig02">Figure 13-2</A>, if we know something about <span class="docEmphasis">A</span>, which causes <span class="docEmphasis">B</span>, we can make some inferences about the probability of <span class="docEmphasis">B</span> occurring.</P></DD><br><p><DT><I><span class="docPubcolor">Explaining away</span></I></DT></p>
<DD><P class="docList">Explaining away involves common cause networks, as shown on the lower right in <A class="docLink" HREF="#ch13_fig02">Figure 13-2</A>. Let's assume the nodes are all binary—that is, true or false. If we know <span class="docEmphasis">C</span> is true and we know <span class="docEmphasis">A</span> and <span class="docEmphasis">B</span> cause <span class="docEmphasis">C</span>, given that <span class="docEmphasis">C</span> is true we would raise the probability that <span class="docEmphasis">A</span> and <span class="docEmphasis">B</span> also are true. However, say we later learn that <span class="docEmphasis">B</span> is true; this implies that the probability of <span class="docEmphasis">A</span> occurring actually decreases. This brings up some interesting characteristics of Bayesian networks—namely, independence and conditional dependence.</P></DD></DL>
<P class="docText">The structure on the lower right in <A class="docLink" HREF="#ch13_fig02">Figure 13-2</A> implies that events <span class="docEmphasis">A</span> and <span class="docEmphasis">B</span> are independent of each other. There's no causal link between <span class="docEmphasis">A</span> and <span class="docEmphasis">B</span>. However, if we learn something about <span class="docEmphasis">C</span> and then something about <span class="docEmphasis">A</span> or <span class="docEmphasis">B</span>, we do affect the probability of <span class="docEmphasis">A</span> or <span class="docEmphasis">B</span>. In our example, learning that <span class="docEmphasis">C</span> is true and that <span class="docEmphasis">B</span> also is true lowers the probability that <span class="docEmphasis">A</span> is true even though <span class="docEmphasis">A</span> and <span class="docEmphasis">B</span> are independent events.</P>
<P class="docText">Now consider the network shown on the left in <A class="docLink" HREF="#ch13_fig02">Figure 13-2</A>. In this case, we see that <span class="docEmphasis">A</span> causes <span class="docEmphasis">B</span> which in turn causes <span class="docEmphasis">C</span>. If we learn the state of <span class="docEmphasis">B</span>, we can make inferences on the state of <span class="docEmphasis">C</span> irrespective of the state of <span class="docEmphasis">A</span>. <span class="docEmphasis">A</span> has no influence on our belief of event <span class="docEmphasis">C</span> if we know the state of <span class="docEmphasis">B</span>. In Bayesian lingo, node <span class="docEmphasis">B</span> blocks <span class="docEmphasis">A</span> from affecting <span class="docEmphasis">C</span>.</P>
<P class="docText">Another form of independence present in Bayesian networks is <span class="docEmphasis">d-separation</span>. Look back at the network shown in <A class="docLink" HREF="#ch13_fig01">Figure 13-1</A>. Instead of one node blocking another node, as in the previous discussion, you could have a situation in which a node blocks clusters of nodes. In <A class="docLink" HREF="#ch13_fig01">Figure 13-1</A>, <span class="docEmphasis">C</span> causes <span class="docEmphasis">D</span> and <span class="docEmphasis">D</span> causes <span class="docEmphasis">E</span> and <span class="docEmphasis">F</span>, but <span class="docEmphasis">A</span> and <span class="docEmphasis">B</span> cause <span class="docEmphasis">C</span>. However, if we learn the state of <span class="docEmphasis">C</span>, <span class="docEmphasis">A</span> and <span class="docEmphasis">B</span> have no effect on <span class="docEmphasis">D</span> and thus no effect on <span class="docEmphasis">E</span> and <span class="docEmphasis">F</span>. Likewise, if we learn something about the state of <span class="docEmphasis">D</span>, nodes <span class="docEmphasis">A</span>, <span class="docEmphasis">B</span>, and <span class="docEmphasis">C</span> become irrelevant to <span class="docEmphasis">E</span> and <span class="docEmphasis">F</span>. Identifying these independence situations is helpful when trying to solve Bayesian networks because we can treat parts of a network separately, simplifying some computations.</P>
<P class="docText">Actually solving, or making inferences, using Bayesian networks involves calculating probabilities using the rules we discussed in <A class="docLink" HREF="ch12.html#ch12">Chapter 12</A>. We're going to show you how to do this for simple networks in the examples that follow. We should point out, though, that some general-purpose methods for solving complex Bayesian networks we aren't going to cover. These networks include the popular message passing algorithm (see the second reference cited at the end of this chapter) as well as other approximate stochastic methods. Many of these methods don't seem appropriate for real-time games because computation requirements are large. Here, again, we recommend that you keep Bayesian networks simple if you're going to use them in games. Of course, you don't have to listen to us, but by keeping them simple, you can use them where they are best suited for specific tasks and let other methods do their job. This will make your testing and debugging job easier because you can isolate the complicated AI code from the rest of your AI code.</P>


<ul></ul></td></tr></table>
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#e6e6e6">
<tr style="background-image: url(images/tile_back.gif);">
<td class="v2" align="left" width="30%">
<a href="ch13.html"><img src="images/previous.gif" width="70" height="19" border="0" align="absmiddle" alt="Previous Section"></a>
</td>
<td class="v2" align="center" width="40%">
<a href="main.html" style="color:white;text-decoration:none;text-underline:none">&nbsp;&lt;&nbsp;Day Day Up&nbsp;&gt;&nbsp;</a>
</td>
<td class="v2" align="right" width="30%">
<a href="ch13_sect1_002.html"><img src="images/next.gif" width="70" height="19" border="0" align="absmiddle" alt="Next Section"></a>
</td>
</tr>
</table>
</body>
</html>
