<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<link href="styles/globalstyle.css" type="text/css" rel="stylesheet"/>
<link title="medium" href="styles/two.css" type="text/css" rel="stylesheet"/>
<script src="scripts/main.js" type="text/javascript" language="javascript"></script>
<title>Chapter 14. How Elegant Code Evolves with Hardware The Case of Gaussian Elimination</title>
</head><body>
<DIV class=h3 style="MARGIN-TOP: 0px; PADDING-BOTTOM: 1em; WIDTH: 100%; PADDING-TOP: 0px"><SPAN>How Elegant Code Evolves with Hardware The Case of Gaussian Elimination</SPAN><SPAN> &gt; The Effects of Computer Architectures on Matrix Algorithms</SPAN></DIV>
<DIV></DIV>
<DIV>
<DIV><A name=how_elegant_code_evolves_with_hardware_the_case_of_gaussian_></A>
<H2 class=docChapterTitle id=title-ID0EMKDK>14. How Elegant Code Evolves with Hardware The Case of Gaussian Elimination</H2>
<P class=docText><SPAN class=docEmphasis><A name="Jack Dongarra"></A>Jack Dongarra and Piotr Luszczek</SPAN> <A name=idx-CHP-14-1087></A><A name=idx-CHP-14-1088></A><A name=idx-CHP-14-1089></A><A name=idx-CHP-14-1090></A><A name=idx-CHP-14-1091></A></P>
<P class=docText><SPAN class=docEmphSmaller><A name="of advanced"></A>The increasing availability of advanced-architecture computers</SPAN><A name="had a"></A>, at affordable costs, has had a significant effect on all spheres <A name=idx-CHP-14-1092></A><A name="In this"></A>of scientific computation. In this chapter, we'll show the need for designers of computing algorithms to make expeditious and substantial adaptations to algorithms, in reaction to architecture changes, by closely examining one simple but important algorithm in mathematical <A name=idx-CHP-14-1093></A><A name="the solution"></A>software: Gaussian elimination for the solution of linear systems of equations.</P>
<P class=docText>At the <A name=idx-CHP-14-1094></A>application level, science has to be captured in <A name=idx-CHP-14-1095></A><A name="as software"></A>mathematical models, which in turn are expressed algorithmically and ultimately encoded as software. At the software level, there is a continuous tension between performance and portability on the one hand, and understandability of the underlying code. We'll examine these issues and look at trade-offs that have been made over time. Linear algebra—in particular, the solution of linear systems of equations—lies at the heart of most calculations in scientific computing. This chapter focuses on some of the recent developments in linear algebra software designed to exploit advanced-architecture computers over the decades.</P>
<P class=docText><A name="two broad"></A>There are two broad classes of algorithms: those <A name=idx-CHP-14-1096></A><A name="those for"></A>for dense matrices and those for <A name=idx-CHP-14-1097></A>sparse matrices. A matrix is called <SPAN class=docEmphasis>sparse</SPAN><A name="number of"></A> if it contains a substantial number of zero elements. For sparse matrices, radical savings in space and execution time can be achieved through specialized <A name=idx-CHP-14-1098></A><A name="storage and"></A>storage and algorithms. To narrow our discussion and keep it simple, we will look only at the <SPAN class=docEmphasis>dense matrix problem</SPAN><A name="defined as"></A> (a dense matrix is defined as one with few zero elements).<A name=idx-CHP-14-1099></A></P>
<P class=docText><A name="work in"></A>Much of the work in developing linear algebra software for advanced-architecture computers is motivated by the need to solve large problems on the fastest computers available. In this chapter, we'll discuss the development of standards for linear algebra software, the building blocks for software libraries, and aspects of algorithm design as influenced by the opportunities for parallel implementation. We'll explain <A name=idx-CHP-14-1100></A><A name="this work"></A>motivations for this work, and say a bit about future directions.</P>
<P class=docText><A name="of dense"></A>As representative examples of dense matrix routines, we will consider <A name=idx-CHP-14-1101></A>Gaussian elimination, or <A name=idx-CHP-14-1102></A>LU factorization. This examination, spanning <A name=idx-CHP-14-1103></A><A name="most important"></A>hardware and software advances over the past 30 years, will highlight the most important factors that must be considered in designing linear algebra software for advanced-architecture computers. We use these factorization routines for illustrative purposes not only because they are relatively simple, but also because of their importance in several scientific and engineering applications that make use of boundary element methods. These applications include electromagnetic scattering and computational fluid dynamics problems.</P>
<P class=docText><A name="years have"></A>The past 30 years have seen a great deal of activity in the area of algorithms and software for solving linear algebra problems. The goal of achieving high performance in <A name=idx-CHP-14-1104></A><A name="across platforms"></A>code that is portable across platforms has largely been realized by the identification of linear algebra kernels, the Basic Linear Algebra Subprograms (<A name=idx-CHP-14-1105></A><A name="ScaLAPACK libraries"></A>BLAS). We will discuss the LINPACK, LAPACK, and ScaLAPACK libraries, which are expressed in successive levels of the BLAS. See the section "Further Reading" at the end of this chapter for discussions of these libraries.</P><A name=the_effects_of_computer_architectures_on_matrix_algorithms></A>
<H3 class=docSection1Title id=-100000>14.1. The Effects of Computer Architectures on Matrix Algorithms</H3>
<P class=docText><A name="The key"></A>The key motivation in the design of efficient linear algebra algorithms for advanced-architecture computers involves the storage and <A name=idx-CHP-14-1106></A>retrieval of <A name=idx-CHP-14-1107></A><A name="minimize the"></A>data. Designers wish to minimize the frequency with which data moves between different levels of the memory hierarchy. Once data is in registers or the fastest cache, all processing required for this data should be performed before it gets evicted back to the main memory. Thus, the main algorithmic approach for exploiting both <A name=idx-CHP-14-1108></A><A name="in our"></A>vectorization and parallelism in our implementations uses <SPAN class=docEmphasis>block-partitioned algorithms</SPAN><A name="conjunction with"></A>, particularly in conjunction with highly tuned kernels for performing matrix-vector and matrix-matrix operations (the <A name=idx-CHP-14-1109></A>Level-2 and <A name=idx-CHP-14-1110></A><A name="that the"></A>Level-3 BLAS). Block partitioning means that the data is divided into blocks, each of which should fit within a cache memory or a vector register file.<A name=idx-CHP-14-1111></A><A name=idx-CHP-14-1112></A><A name=idx-CHP-14-1113></A><A name=idx-CHP-14-1114></A></P>
<P class=docText>The <A name=idx-CHP-14-1115></A>computer architectures considered in this chapter are:</P>
<UL>
<LI>
<P class=docList><A name=idx-CHP-14-1116></A>Vector machines</P></LI>
<LI>
<P class=docList>RISC computers <A name=idx-CHP-14-1117></A>with cache hierarchies</P></LI>
<LI>
<P class=docList><A name=idx-CHP-14-1118></A>Parallel systems with distributed memory</P></LI>
<LI>
<P class=docList><A name=idx-CHP-14-1119></A>Multi-core computers</P></LI></UL>
<P class=docText><A name="were able"></A>Vector machines were introduced in the late 1970s and early 1980s. They were able in <A name=idx-CHP-14-1120></A><A name="to perform"></A>one step to perform a single operation on a relatively large number of operands stored <A name=idx-CHP-14-1121></A>in vector registers. <A name=idx-CHP-14-1122></A>Expressing matrix <A name=idx-CHP-14-1123></A><A name="a natural"></A>algorithms as vector-vector operations was a natural fit for this type of machine. However, some of the vector designs had a limited ability to load and store the vector registers in main memory. A technique called <SPAN class=docEmphasis>chaining</SPAN><A name="circumvented by"></A> allowed this limitation to be circumvented by moving data between the registers before accessing main memory. Chaining required <A name=idx-CHP-14-1124></A><A name="recasting linear"></A>recasting linear algebra in terms of matrix-vector operations.</P>
<P class=docText><A name="introduced in"></A>RISC computers were introduced in the late 1980s and early 1990s. While their clock rates might have been comparable to those of the vector machines, the computing speed lagged behind due to their lack of vector registers. Another deficiency was their creation of a deep memory hierarchy with multiple levels of cache memory to alleviate the scarcity of band-width that was, in turn, caused mostly by a limited number of memory banks. The eventual success of this architecture is commonly attributed to the right price point and astonishing improvements in performance over time as predicted by Moore's Law. With RISC computers, the <A name=idx-CHP-14-1125></A><A name="had to"></A>linear algebra algorithms had to be redone yet again. This time, the formulations had to expose as many matrix-matrix operations as possible, which guaranteed good cache reuse.</P>
<P class=docText><A name="way of"></A>A natural way of achieving even greater performance levels with both vector and RISC processors is by connecting them together with a network and letting them cooperate to solve a problem bigger than would be feasible on just one processor. Many <A name=idx-CHP-14-1126></A><A name="this path"></A>hardware configurations followed this path, so the matrix algorithms had to follow yet again as well. It was quickly discovered that good local performance has to be combined with good global partitioning of the matrices and vectors.</P>
<P class=docText><A name="dictated by"></A>Any trivial divisions of matrix data quickly uncovered scalability problems dictated by so-called <A name=idx-CHP-14-1127></A><A name="that the"></A>Amdahl's Law: the observation that the time taken by the sequential portion of a computation provides the minimum bound for the entire execution time, and therefore limits the gains achievable from parallel processing. In other words, unless most computations can be done independently, the point of diminishing returns is reached, and adding more processors to the hardware mix will not result in faster processing.</P>
<P class=docText><A name="the class"></A>For the sake of simplicity, the class of multi-core architectures includes both <A name=idx-CHP-14-1128></A>symmetric multiprocessing (<A name=idx-CHP-14-1129></A>SMP) and <A name=idx-CHP-14-1130></A><A name="as the"></A>single-chip multi-core machines. This is probably an unfair simplification, as the SMP machines usually have better memory systems. But when applied to matrix algorithms, both yield good performance results with very similar algorithmic approaches: these combine local cache reuse and independent computation with explicit control of data dependences.</P></DIV></DIV>
<p>&nbsp;</p><p>&nbsp;</p><!-- 仁·义 -->
</body></html>
