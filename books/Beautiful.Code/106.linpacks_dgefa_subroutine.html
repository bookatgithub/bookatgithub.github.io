<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<link href="styles/globalstyle.css" type="text/css" rel="stylesheet"/>
<link title="medium" href="styles/two.css" type="text/css" rel="stylesheet"/>
<script src="scripts/main.js" type="text/javascript" language="javascript"></script>
<title>Section 14.4. LINPACK's DGEFA Subroutine</title>
</head><body>
<DIV class=h3 style="MARGIN-TOP: 0px; PADDING-BOTTOM: 1em; WIDTH: 100%; PADDING-TOP: 0px"><SPAN>How Elegant Code Evolves with Hardware The Case of Gaussian Elimination</SPAN><SPAN> &gt; LINPACK's DGEFA Subroutine</SPAN></DIV>
<DIV></DIV>
<DIV>
<DIV><A name=linpacks_dgefa_subroutine></A>
<H3 class=docSection1Title id=-100000>14.4. LINPACK's DGEFA Subroutine</H3>
<P class=docText><A name="The performance"></A>The performance issues with the MATLAB version of the <A name=idx-CHP-14-1149></A><A name="the mid"></A>code continued as, in the mid-1970s, vector architectures became available for scientific computations. Vector architectures exploit pipeline processing <A name=idx-CHP-14-1150></A><A name="operations on"></A>by running mathematical operations on arrays of data in a simultaneous or pipelined fashion. Most algorithms in linear algebra can be easily vectorized. Therefore, in the late 1970s there was an effort to standardize vector operations for use in scientific computations. The idea was to define some simple, frequently used operations and implement them on various systems to achieve portability and efficiency. This package came to be known as the Level-1 Basic Linear Algebra Subprograms (BLAS) or <A name=idx-CHP-14-1151></A>Level-1 BLAS.<A name=idx-CHP-14-1152></A></P>
<P class=docText>The term <SPAN class=docEmphasis>Level-1</SPAN><A name="we will"></A> denotes vector-vector operations. As we will see, Level-2 (matrix-vector operations), and Level-3 (matrix-matrix operations) play important roles as well.</P>
<P class=docText><A name="algorithms of"></A>In the 1970s, the algorithms of dense linear algebra were implemented in a systematic way by the <A name=idx-CHP-14-1153></A>LINPACK project. <A name=idx-CHP-14-1154></A><A name="solve linear"></A>LINPACK is a collection of Fortran subroutines that analyze and solve linear equations and linear least-squares problems. The package solves linear systems whose matrices are general, banded, symmetric indefinite, symmetric positive definite, triangular, and tridiagonal square. In addition, the package computes the QR and singular value decompositions of rectangular matrices and applies them to least-squares problems.</P>
<P class=docText>LINPACK uses <A name=idx-CHP-14-1155></A><A name="efficiency by"></A>column-oriented algorithms, which increase efficiency by preserving locality of reference. By column orientation, we mean that the LINPACK code always references arrays down columns, not across rows. This is important since Fortran stores arrays in column-major order. This means that as one proceeds down a column of an array, the memory references proceed sequentially through memory. Thus, if a program references an item in a particular block, the next reference is likely to be in the same block.</P>
<P class=docText><A name="was kept"></A>The software in LINPACK was kept machine-independent partly through the introduction of the <A name=idx-CHP-14-1156></A><A name="computation was"></A>Level-1 BLAS routines. Almost all of the computation was done by calling Level-1 BLAS. For each machine, the set of Level-1 BLAS would be implemented in a machine-specific manner to obtain high performance.</P>
<P class=docText><A class=docLink href="javascript:moveTo('linpack_variant_fortran_coding');">Example 14-2</A><A name="shows the"></A> shows the LINPACK <A name=idx-CHP-14-1157></A>implementation of factorization.</P><A name=linpack_variant_fortran_coding></A>
<H5 class=docExampleTitle id=title-ID0EGODK>Example 14-2. LINPACK variant (Fortran 66 coding)</H5>
<P>
<TABLE cellSpacing=0 cellPadding=5 width=* border=1>
<TBODY>
<TR>
<TD>
<DIV class=codeSegmentsExpansionLinks>Code View: <SPAN>Scroll</SPAN> / <A href="javascript:expandCodeSegments()">Show All</A></DIV><PRE class=preFixedHeight>      subroutine dgefa(a,lda,n,ipvt,info)
      integer lda,n,ipvt(1),info
      double precision a(lda,1)
      double precision t
      integer idamax,j,k,kp1,l,nm1
c
c
c     <A name=idx-CHP-14-1158></A>gaussian elimination with partial pivoting
c
     info = 0
     nm1 = n - 1
     if (nm1 .lt. 1) go to 70
     do 60 k = 1, nm1
        kp1 = k + 1 
c 
c       find l = pivot index
c 
        l = idamax(n-k+1,a(k,k),1) + k - 1
        ipvt(k) = l
c
c       zero pivot implies this column already triangularized
c
        if (a(l,k) .eq. 0.0d0) go to 40
c 
c       interchange if necessary
c
        if (l .eq. k) go to 10
           t = a(l,k)
           a(l,k) = a(k,k)
           a(k,k) = t
  10    continue
c
c       compute multipliers
c
        t = -1.0d0/a(k,k)
        call dscal(n-k,t,a(k+1,k),1)
c       
c       row <A name=idx-CHP-14-1159></A>elimination with column indexing
c
        do 30 j = kp1, n
           t = a(l,j)
           if (l .eq. k) go to 20
              a(l,j) = a(k,j)
              a(k,j)= t
  20        continue
            call daxpy(n-k,t,a(k+1,k),1,a(k+1,j),1)
  30      continue
       go to 50 
  40   continue
          info = k 
  50   continue
  60 continue
  70 continue
     ipvt(n) = n
     if (a(n,n) .eq. 0.0d0) info = n
     return
     end


					    </PRE><BR></TD></TR></TBODY></TABLE></P>
<P class=docText><A name="and IDAMAX"></A>The Level-1 BLAS subroutines DAXPY, DSCAL, and IDAMAX are <A name=idx-CHP-14-1160></A><A name="routine DGEFA"></A>used in the routine DGEFA. The main difference between <A class=docLink href="105.a_simple_version.html#simple_variant_matlab_coding">Example 14-1</A> and <A name=idx-CHP-14-1161></A><A class=docLink href="javascript:moveTo('linpack_variant_fortran_coding');">Example 14-2</A><A name="programming language"></A> (other than the programming language and the interchange of loop indexes) is the use of routine DAXPY to <A name=idx-CHP-14-1162></A><A name="loop of"></A>encode the inner loop of the method.</P>
<P class=docText>It was presumed that the <A name=idx-CHP-14-1163></A><A name="the computer"></A>BLAS operations would be implemented in an efficient, machine-specific way suitable for the computer on which the subroutines were executed. On a vector computer, this could translate into a simple, single vector operation. This would avoid leaving the optimization up to the compiler and explicitly exposing a performance-critical operation.</P>
<P class=docText><A name="the beauty"></A>In a sense, then, the beauty of the original <A name=idx-CHP-14-1164></A><A name="the use"></A>code was regained with the use of a new vocabulary to describe the algorithms: the BLAS. Over time, the BLAS became a widely adopted standard and were most likely the first to enforce two key aspects of software: modularity and portability. Again, these are taken for granted today, but at the time they were not. One could have the cake of compact algorithm representation and eat it, too, because the resulting Fortran code was portable.</P>
<P class=docText><A name="can be"></A>Most algorithms in linear algebra can be easily vectorized. However, to gain the most out of such architectures, simple <A name=idx-CHP-14-1165></A><A name="vectorization is"></A>vectorization is usually not enough. Some vector computers are limited by having only one path between memory and the vector registers. This creates a bottleneck if a program loads a vector from memory, performs some arithmetic operations, and then stores the results. In order to achieve top performance, the scope of the vectorization must be expanded to facilitate chaining operations together and to minimize data movement, in addition to using vector operations. <A name=idx-CHP-14-1166></A><A name="in terms"></A>Recasting the algorithms in terms of matrix-vector operations makes it easy for a vectorizing compiler to achieve these goals.</P>
<P class=docText><A name="became more"></A>Thus, as computer architectures became more complex in the design of their memory hierarchies, it became necessary to increase the scope of the BLAS routines from Level-1 to Level-2 and Level-3.</P></DIV></DIV>
<p>&nbsp;</p><p>&nbsp;</p><!-- 仁·义 -->
</body></html>
