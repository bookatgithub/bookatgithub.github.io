<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<link href="styles/globalstyle.css" type="text/css" rel="stylesheet"/>
<link title="medium" href="styles/two.css" type="text/css" rel="stylesheet"/>
<script src="scripts/main.js" type="text/javascript" language="javascript"></script>
<title>Section 16.3. Scaling Up to Thousands of Devices</title>
</head><body>
<DIV class=h3 style="MARGIN-TOP: 0px; PADDING-BOTTOM: 1em; WIDTH: 100%; PADDING-TOP: 0px"><SPAN>The Linux Kernel Driver Model: The Benefits of Working Together</SPAN><SPAN> &gt; Scaling Up to Thousands of Devices</SPAN></DIV>
<DIV></DIV>
<DIV>
<DIV><A name=scaling_up_to_thousands_of_devices></A>
<H3 class=docSection1Title id=-100000>16.3. Scaling Up to Thousands of Devices</H3>
<P class=docText><A name=As></A>As <A name=idx-CHP-16-1361></A><A name="everything from"></A>Linux runs on everything from cellphones, radio-controlled helicopters, desktops, and servers to 73 percent of the world's largest supercomputers, scaling the driver model was very important and always in the backs of our minds. As development progressed, it was nice to see that the basic structures used to hold devices, <TT>struct kobject</TT> and <TT>struct devices</TT><A name="number of"></A>, were relatively small. The number of devices connected to most systems is directly proportional to the size of the system. So small, embedded systems had only a few—one to ten— different devices connected and in their device tree. Larger "enterprise" systems had many more devices connected, but these systems also had a lot of memory to spare, so the increased number of devices was still only a very small proportion of the kernel's overall <A name=idx-CHP-16-1362></A>memory usage.<A name=idx-CHP-16-1363></A></P>
<P class=docText><A name="scaling model"></A>This comfortable scaling model, unfortunately, was found to be completely false when it came to one class of "enterprise" system, the s390 mainframe computer. This computer could run Linux in a virtual partition (up to 1,024 instances at the same time on a single machine) and had a huge number of different storage devices connected to it. Overall, the system had a lot of memory, but each virtual partition would have only a small slice of that memory. Each virtual partition wanted to see all different storage devices (20,000 could be typical), while only being allocated a few hundred megabytes of RAM.</P>
<P class=docText><A name="device tree"></A>On these systems, the device tree was quickly found to suck up a huge percentage of memory that was never released back to the user processes. It was time to put the driver model on a diet, and some very smart IBM kernel developers went to work on the problem.</P>
<P class=docText><A name="the main"></A>What the developers found was initially surprising. It turned out that the main <TT>struct device</TT><A name="only around"></A> structure was only around 160 bytes (for a 32-bit processor). With 20,000 devices in the system, that amounted to only 3 to 4 MB of RAM being used, a very manageable usage of memory. The big memory hog was the RAM-based filesystem mentioned earlier, sysfs, which showed all of these devices to userspace. For every device, <SPAN class=docEmphasis>sysfs</SPAN> created both a <TT>struct inode</TT> and a <TT>struct dentry</TT><A name="heavy structures"></A>. These are both fairly heavy structures, with the <TT>struct inode</TT> weighing in around 256 bytes and <TT>struct dentry</TT> about 140 bytes.<SUP class=docFootnote><A class=docLink href="javascript:moveTo('CHP-16-FNOTE-4');">[§]</A></SUP><A name=idx-CHP-16-1364></A><A name=idx-CHP-16-1365></A><A name=I_indexterm16_tt359></A></P>
<BLOCKQUOTE>
<P class=docFootnote><SUP><A name=CHP-16-FNOTE-4>[§]</A></SUP><A name="since been"></A> Both of these structures have since been shrunk, and therefore are smaller in current kernel versions.</P></BLOCKQUOTE>
<P class=docText><A name="For every"></A>For every <TT>struct device</TT>, at least one <TT>struct dentry</TT> and one <TT>struct inode</TT><A name="different copies"></A> were being created. Generally, many different copies of these filesystem structures were created, one for every virtual file per device in the system. As an example, a single block device would create about 10 different virtual files, so that meant that a single structure of 160 bytes would end up using 4 KB. In a system of 20,000 devices, about 80 MB were wasted on the virtual filesystem. This memory was consumed by the kernel, unable to be used by any user programs, even if they never wanted to look at the information stored in <SPAN class=docEmphasis>sysfs</SPAN>.</P>
<P class=docText><A name="this was"></A>The solution for this was to rewrite the <SPAN class=docEmphasis>sysfs</SPAN><A name="put these"></A> code to put these <TT>struct inode</TT> and <TT>struct dentry</TT><A name="creating them"></A> structures in the kernel's caches, creating them on the fly when the filesystem was accessed. The solution was just a matter of dynamically creating the directories and files on the fly as a user walked through the tree, instead of preallocating everything when the device was originally created. Because these structures are in the main caches of the kernel, if memory pressure is placed on the system by userspace programs, or other parts of the kernel, the caches are freed and the memory is returned to those who need it at the time. This was all done by touching the backend <SPAN class=docEmphasis>sysfs</SPAN> code, and not the main <TT>struct device</TT> structures.</P></DIV></DIV>
<p>&nbsp;</p><p>&nbsp;</p><!-- 仁·义 -->
</body></html>
