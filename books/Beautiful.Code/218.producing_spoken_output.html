<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<link href="styles/globalstyle.css" type="text/css" rel="stylesheet"/>
<link title="medium" href="styles/two.css" type="text/css" rel="stylesheet"/>
<script src="scripts/main.js" type="text/javascript" language="javascript"></script>
<title>Section 31.1. Producing Spoken Output</title>
</head><body>
<DIV class=h3 style="MARGIN-TOP: 0px; PADDING-BOTTOM: 1em; WIDTH: 100%; PADDING-TOP: 0px"><SPAN>Emacspeak: The Complete Audio Desktop</SPAN><SPAN> &gt; Producing Spoken Output</SPAN></DIV>
<DIV></DIV>
<DIV>
<DIV><A name=emacspeak_the_complete_audio_desktop></A>
<H2 class=docChapterTitle id=title-ID0EWKEK>31. Emacspeak: The Complete Audio Desktop</H2>
<P class=docText><SPAN class=docEmphasis>T. V. Raman</SPAN> <A name=idx-CHP-31-2630></A></P>
<P class=docText><SPAN class=docEmphSmaller><A name="A desktop"></A>A desktop is a workspace that one uses to organize the tools of one's trade</SPAN><A name="rich visual"></A>. Graphical desktops provide rich visual interaction for performing day-to-day computing tasks; the goal of the <SPAN class=docEmphasis>audio desktop</SPAN><A name="similar efficiencies"></A> is to enable similar efficiencies in an eyes-free environment. Thus, the primary goal of an audio desktop is to use the expressiveness of auditory output (both verbal and nonverbal) to enable the end user to perform a full range of computing tasks:</P>
<UL>
<LI>
<P class=docList><A name="the full"></A>Communication through the full range of electronic messaging services</P></LI>
<LI>
<P class=docList><A name="local documents"></A>Ready access to local documents on the client and global documents on the Web</P></LI>
<LI>
<P class=docList>Ability to develop software effectively in an eyes-free environment</P></LI></UL>
<P class=docText>The <A name=idx-CHP-31-2631></A><A name="effective auditory"></A>Emacspeak audio desktop was motivated by the following insight: to provide effective auditory renderings of information, one needs to start from the actual information being presented, rather than a visual presentation of that information. This had earlier led me to develop <A name=idx-CHP-31-2632></A><A name="For Technical"></A>AsTeR, Audio System For Technical Readings (<A class=docLink href="http://emacspeak.sf.net/raman/aster/aster-toplevel.html" target=_blank>http://emacspeak.sf.net/raman/aster/aster-toplevel.html</A><A name="was to"></A>). The primary motivation then was to apply the lessons learned in the context of aural documents to user interfaces—after all, the document <SPAN class=docEmphasis>is</SPAN> the interface.</P>
<P class=docText><A name="to merely"></A>The primary goal was not to merely carry the visual interface over to the auditory modality, but rather to create an eyes-free user interface that is both pleasant and productive to use.</P>
<P class=docText><A name="Contrast this"></A>Contrast this with the traditional screen-reader approach where GUI widgets such as sliders and tree controls are directly translated to <A name=idx-CHP-31-2633></A><A name="direct translation"></A>spoken output. Though such direct translation can give the appearance of providing full eyes-free access, the resulting auditory user interface can be inefficient to use.</P>
<P class=docText><A name="that the"></A>These prerequisites meant that the environment selected for the audio desktop needed:</P>
<UL>
<LI>
<P class=docList><A name="set of"></A>A core set of speech and nonspeech audio output services</P></LI>
<LI>
<P class=docList><A name="of pre"></A>A rich suite of pre-existing applications to speech-enable</P></LI>
<LI>
<P class=docList>Access to application context to produce contextual feedback</P></LI></UL><A name=producing_spoken_output></A>
<H3 class=docSection1Title id=-100000>31.1. Producing Spoken Output</H3>
<P class=docText>I started implementing <A name=idx-CHP-31-2634></A><A name="and my"></A>Emacspeak in October 1994. The target environments were a Linux laptop and my office workstation. To produce speech output, I used a <A name=idx-CHP-31-2635></A>DECTalk Express (a <A name=idx-CHP-31-2636></A><A name="on the"></A>hardware speech synthesizer) on the laptop and a software version of the DECTalk on the office workstation.<A name=idx-CHP-31-2637></A></P>
<P class=docText><A name="to design"></A>The most natural way to design the system to leverage both speech options was to first implement a speech server that abstracted away the distinction between the two output solutions. The speech server abstraction has withstood the test of time well; I was able to add support for the IBM ViaVoice engine later, in 1999. Moreover, the simplicity of the client/server API has enabled open source programmers to implement <A name=idx-CHP-31-2638></A>speech servers for other speech engines.</P>
<P class=docText><A name=idx-CHP-31-2639></A><A name="in the"></A>Emacspeak speech servers are implemented in the <A name=idx-CHP-31-2640></A><A name="TCL language"></A>TCL language. The speech server for the DECTalk Express communicated with the hardware synthesizer over a serial line. As an example, the command to speak a string of text was a <TT>proc</TT><A name="string argument"></A> that took a string argument and wrote it to the serial device. A simplified version of this looks like:</P><PRE>	proc tts_say {text} {puts -nonewline $tts(write) "$text"}
</PRE><BR>
<P class=docText><A name="for the"></A>The speech server for the software DECTalk implemented an equivalent, simplified <TT>tts_say</TT><A name="looks like"></A> version that looks like:</P><PRE>	proc say {text} {_say "$text"}
</PRE><BR>
<P class=docText>where <TT>_say</TT><A name="C implementation"></A> calls the underlying C implementation provided by the DECTalk software.</P>
<P class=docText><A name="speech servers"></A>The net result of this design was to create separate speech servers for each available engine, where each speech server was a simple script that invoked TCL's default readeval-print loop after loading in the relevant definitions. The client/server API therefore came down to the client (Emacspeak) launching the appropriate speech server, caching this connection, and invoking server commands by issuing appropriate procedure calls over this connection.</P>
<P class=docText><A name="so far"></A>Notice that so far I have said nothing explicit about how this client/server connection was opened; this late-binding proved beneficial later when it came to making Emacspeak network-aware. Thus, the initial implementation worked by the <A name=idx-CHP-31-2641></A><A name="the speech"></A>Emacspeak client communicating to the speech server using <TT>stdio</TT><A name="the network"></A>. Later, making this client/server communication go over the network required the addition of a few lines of code that opened a server socket and connected <TT>stdin/stdout</TT><A name="to the"></A> to the resulting connection.</P>
<P class=docText><A name="and relying"></A>Thus, designing a clean client/server abstraction, and relying on the power of Unix I/O, has made it trivial to later run Emacspeak on a remote machine and have it connect back to a speech server running on a local client. This enables me to run Emacspeak inside <SPAN class=docEmphasis>screen</SPAN><A name="and access"></A> on my work machine, and access this running session from anywhere in the world. Upon connecting, I have the remote Emacspeak session connect to a speech server on my laptop, the audio equivalent of setting up X to use a remote display.</P></DIV></DIV>
<p>&nbsp;</p><p>&nbsp;</p><!-- 仁·义 -->
</body></html>
