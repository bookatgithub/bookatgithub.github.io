<html xmlns="http://www.w3.org/1999/xhtml">
   
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      
      <title>Chapter 15. Error-Correcting Codes</title>
      
      <link href="9780133085013.css" rel="stylesheet" type="text/css" />
      
      <link href="page-template.xpgt" rel="stylesheet" type="application/vnd.adobe-page-template+xml" />
      
      <meta content="urn:uuid:767ce45a-85bb-4273-91f5-801bab42906b" name="Adept.expected.resource" />
      
   </head>
   
   <body>
      
      <h2><a id="page_331"></a><a id="ch15"></a>Chapter 15. Error-Correcting Codes
      </h2>
      
      <h3><a id="ch15lev1"></a><strong>15–1 Introduction</strong></h3>
      
      <p class="noindent">This section is a brief introduction to the theory and practice of error-correcting
         codes (ECCs). We limit our attention to binary forward error-correcting (FEC) block
         codes. This means that the symbol alphabet consists of just two symbols (which we
         denote 0 and 1), that the receiver can correct a transmission error without asking
         the sender for more information or for a retransmission, and that the transmissions
         consist of a sequence of fixed length blocks, called <em>code words</em>.
      </p>
      
      <p class="indent"><a href="ch15.html#ch15lev2">Section 15–2</a> describes the code independently discovered by R. W. Hamming and M. J. E. Golay before
         1950 [Ham]. This code is single error-correcting (SEC), and a simple extension of
         it, also discovered by Hamming, is single error-correcting and, simultaneously, double
         error-detecting (SEC-DED).
      </p>
      
      <p class="indent"><a href="ch15.html#ch15lev4">Section 15–4</a> steps back and asks what is possible in the area of forward error correction. Still
         sticking to binary FEC block codes, the basic question addressed is: for a given block
         length (or <em>code length</em>) and level of error detection and correction capability, how many different code
         words can be encoded?
      </p>
      
      <p class="indent"><a href="ch15.html#ch15lev2">Section 15–2</a> is for readers who are primarily interested in learning the basics of how ECC works
         in computer memories. <a href="ch15.html#ch15lev4">Section 15–4</a> is for those who are interested in the mathematics of the subject, and who might
         be interested in the challenge of an unsolved mathematical problem.
      </p>
      
      <p class="indent">The reader is cautioned that over the past 50 years ECC has become a very big subject.
         Many books have been published on it and closely related subjects [Hill, LC, MS, and
         Roman, to mention a few]. Here we just scratch the surface and introduce the reader
         to two important topics and to some of the terminology used in this field. Although
         much of the subject of error-correcting codes relies very heavily on the notations
         and results of linear algebra, and, in fact, is a very nice application of that abstract
         theory, we avoid it here for the benefit of those who are not familiar with that theory.
      </p>
      
      <p class="indent">The following notation is used throughout this chapter. The terms are defined in subsequent
         sections.
      </p>
      
      <p class="indentpre"><em>m</em> Number of “information” or “message” bits<br /><em>k</em> Number of parity-check bits (“check bits,” for short)<br /><em>n</em> Code length, <em>n</em> = <em>m</em> + <em>k</em><br /><strong><em>u</em></strong> Information bit vector, <strong><em>u</em></strong><sub>0</sub>, <strong><em>u</em></strong><sub>1</sub>, ... <strong><em>u</em></strong><sub><em>m</em> – 1</sub><br /><strong><em>p</em></strong> Parity check bit vector, <strong><em>p</em></strong><sub>0</sub>, <strong><em>p</em></strong><sub>1</sub>, ..., <strong><em>p</em></strong><sub><em>k</em> – 1</sub><br /><strong><em>s</em></strong> Syndrome vector, <strong><em>s</em></strong><sub>0</sub>, <strong><em>s</em></strong><sub>1</sub>, ..., <strong><em>s</em></strong><sub><em>k</em> – 1</sub></p>
      
      <h3><a id="page_332"></a><a id="ch15lev2"></a><strong>15–2 The Hamming Code</strong></h3>
      
      <p class="noindent">Hamming’s development [Ham] is a very direct construction of a code that permits correcting
         single-bit errors. He assumes that the data to be transmitted consists of a certain
         number of <em>information bits</em> <strong><em>u</em></strong>, and he adds to these a number of <em>check bits</em> <strong><em>p</em></strong>, such that if a block is received that has at most one bit in error, then <strong><em>p</em></strong> identifies the bit that is in error (which might be one of the check bits). Specifically,
         in Hamming’s code, <strong><em>p</em></strong> is interpreted as an integer that is 0 if no error occurred, and otherwise is the
         1-origin index of the bit that is in error. Let <em>m</em> be the number of information bits, and <em>k</em> the number of check bits used. Because the <em>k</em> check bits must check themselves as well as the information bits, the value of <strong><em>p</em></strong>, interpreted as an integer, must range from 0 to <em>m</em> + <em>k</em>, which is <em>m</em> + <em>k</em> + 1 distinct values. Because <em>k</em> bits can distinguish 2<sup><em>k</em></sup> cases, we must have
      </p>
      
      <div class="image"><img alt="Image" src="graphics/332equ01.jpg" /></div>
      
      <p class="noindent">This is known as the <em>Hamming rule</em>. It applies to any single-error correcting (SEC) binary FEC block code in which all
         of the transmitted bits must be checked. The check bits will be interspersed among
         the information bits in a manner described below.
      </p>
      
      <p class="indent">Because <strong><em>p</em></strong> indexes the bit (if any) that is in error, the least significant bit of <strong><em>p</em></strong> must be 1 if the erroneous bit is in an odd position, and 0 if it is in an even position
         or if there is no error. A simple way to achieve this is to let the least significant
         bit of <strong><em>p, p</em></strong><sub>0</sub>, be an even parity check on the odd positions of the block and to put <strong><em>p</em></strong><sub>0</sub> in an odd position. The receiver then checks the parity of the odd positions (including
         that of <strong><em>p</em></strong><sub>0</sub>). If the result is 1, an error has occurred in an odd position, and if the result
         is 0, either no error occurred or an error occurred in an even position. This satisfies
         the condition that <strong><em>p</em></strong> should be the index of the erroneous bit, or be 0 if no error occurred.
      </p>
      
      <p class="indent">Similarly, let the next-from-least significant bit of <strong><em>p, p</em></strong><sub>1</sub>, be an even parity check of positions 2, 3, 6, 7, 10, 11, ... (in binary, 10, 11,
         110, 111, 1010, 1011, ...), and put <strong><em>p</em></strong><sub>1</sub> in one of these positions. Those positions have a 1 in their second-from-least significant
         binary position number. The receiver checks the parity of these positions (including
         the position of <strong><em>p</em></strong><sub>1</sub>). If the result is 1, an error occurred in one of those positions, and if the result
         is 0, either no error occurred or an error occurred in some other position.
      </p>
      
      <p class="indent">Continuing, the third-from-least significant check bit, <strong><em>p</em></strong><sub>2</sub>, is made an even parity check on those positions that have a 1 in their third-from-least
         significant position number, namely positions 4, 5, 6, 7, 12, 13, 14, 15, 20, ...,
         and <strong><em>p</em></strong><sub>2</sub> is put in one of those positions.
      </p>
      
      <p class="indent">Putting the check bits in power-of-two positions (1, 2, 4, 8, ...) has the advantage
         that they are independent. That is, the sender can compute <strong><em>p</em></strong><sub>0</sub> independent of <strong><em>p</em></strong><sub>1</sub>, <strong><em>p</em></strong><sub>2</sub>, ... and, more generally, it can compute each check bit independent of the others.
      </p>
      
      <p class="indent"><a id="page_333"></a>As an example, let us develop a single error-correcting code for <em>m</em> = 4. Solving (1) for <em>k</em> gives <em>k</em> = 3, with equality holding. This means that all 2<sup><em>k</em></sup> possible values of the <em>k</em> check bits are used, so it is particularly efficient. A code with this property is
         called a <em>perfect</em> code.<sup><a id="ch15fna1"></a><a href="footnotes.html#ch15fn1">1</a></sup></p>
      
      <p class="indent">This code is called the (7, 4) Hamming code, which signifies that the code length
         is 7 and the number of information bits is 4. The positions of the check bits <strong><em>p</em></strong><sub><em>i</em></sub> and the information bits <strong><em>u</em></strong><sub><em>i</em></sub> are shown here.
      </p>
      
      <div class="image"><img alt="Image" src="graphics/333fig01.jpg" /></div>
      
      <p class="indent"><a href="ch15.html#ch15tab1">Table 15–1</a> shows the entire code. The 16 rows show all 16 possible information bit configurations
         and the check bits calculated by Hamming’s method.
      </p>
      
      <p class="indent">To illustrate how the receiver corrects a single-bit error, suppose the code word</p>
      
      <p class="center">1001110</p>
      
      <p class="noindent">is received. This is row 4 in <a href="ch15.html#ch15tab1">Table 15–1</a> with bit 6 flipped. The receiver calculates the <em>exclusive or</em> of the bits in odd positions and gets 0. It calculates the <em>exclusive or</em> of bits 2, 3, 6, and 7 and gets 1. Lastly, it calculates the <em>exclusive or</em> of bits 4, 5, 6, and 7 and gets 1. Thus the error indicator, which is called the
         <em>syndrome</em>, is binary 110, or 6. The receiver flips the bit at position 6 to correct the block.
      </p>
      
      <p class="tab-caption"><a id="ch15tab1"></a>T<small>ABLE</small> 15–1. T<small>HE</small> (7,4) H<small>AMMING</small> C<small>ODE</small></p>
      
      <div class="image"><img alt="Image" src="graphics/333tab01.jpg" /></div>
      
      <h4><a id="page_334"></a><strong>A SEC-DED Code</strong></h4>
      
      <p class="noindent">For many applications, a single error-correcting code would be considered unsatisfactory,
         because it accepts all blocks received. A SEC-DED code seems safer, and it is the
         level of correction and detection most often used in computer memories.
      </p>
      
      <p class="indent">The Hamming code can be converted to a SEC-DED code by adding one check bit, which
         is a parity bit (let us assume even parity) on all the bits in the SEC code word.
         This code is called an <em>extended Hamming code</em> [Hill, MS]. It is not obvious that it is SEC-DED. To see that it is, consider <a href="ch15.html#ch15tab2">Table 15–2</a>. It is assumed <em>a priori</em> that either 0, 1, or 2 transmission errors occur. As indicated in <a href="ch15.html#ch15tab2">Table 15–2</a>, if there are no errors, the overall parity (the parity of the entire <em>n</em>-bit received code word) will be even, and the syndrome of the (<em>n</em> – 1) -bit SEC portion of the block will be 0. If there is one error, then the overall
         parity of the received block will be odd. If the error occurred in the overall parity
         bit, then the syndrome will be 0. If the error occurred in some other bit, then the
         syndrome will be nonzero and it will indicate which bit is in error. If there are
         two errors, then the overall parity of the received block will be even. If one of
         the two errors is in the overall parity bit, then the other is in the SEC portion
         of the block. In this case, the syndrome will be nonzero (and will indicate the bit
         in the SEC portion that is in error). If the errors are both in the SEC portion of
         the block, then the syndrome will also be nonzero, although the reason is a bit hard
         to explain.
      </p>
      
      <p class="tab-caption"><a id="ch15tab2"></a>T<small>ABLE</small> 15–2. A<small>DDING A</small> P<small>ARITY</small> B<small>IT TO</small> M<small>AKE A</small> SEC-DED C<small>ODE</small></p>
      
      <div class="image"><img alt="Image" src="graphics/334tab01.jpg" /></div>
      
      <p class="indent">The reason is that there must be a check bit that checks one of the two bit positions,
         but not the other one. The parity of this check bit and the bits it checks will thus
         be odd, resulting in a nonzero syndrome. Why must there be a check bit that checks
         one of the erroneous bits but not the other one? To see this, first suppose one of
         the erroneous bits is in an even position and the other is in an odd position. Then,
         because one of the check bits (<strong><em>p</em></strong><sub>0</sub>) checks all the odd positions and none of the even positions, the parity of the bits
         at the odd positions will be odd, resulting in a nonzero syndrome. More generally,
         suppose the erroneous bits are in positions <em>i</em> and <em>j</em> (with <em>i</em> ≠ <em>j</em>). Then, because the binary representations of <em>i</em> and <em>j</em> must differ in some bit position, one of them has a 1 at that position and the other
         has a 0 at that <a id="page_335"></a>position. The check bit corresponding to this position in the binary integers checks
         the bits at positions in the code word that have a 1 in their position number, but
         not the positions that have a 0 in their position number. The bits covered by that
         check bit will have odd parity, and thus the syndrome will be nonzero. As an example,
         suppose the erroneous bits are in positions 3 and 7. In binary, the position numbers
         are 0...0011 and 0...0111. These numbers differ in the third position from the right,
         and at that position the number 7 has a 1 and the number 3 has a 0. Therefore, the
         bits checked by the third check bit (these are bits 4, 5, 6, 7, 12, 13, 14, 15, ...)
         will have odd parity.
      </p>
      
      <p class="indent">Thus, referring to <a href="ch15.html#ch15tab2">Table 15–2</a>, the overall parity and the syndrome together uniquely identify whether 0, 1, or
         2 errors occurred. In the case of one error, the receiver can correct it. In the case
         of two errors, the receiver cannot tell whether just one of the errors is in the SEC
         portion (in which case it could correct it) or both errors are in the SEC portion
         (in which case an attempt to correct it would result in incorrect information bits).
      </p>
      
      <p class="indent">The overall parity bit could as well be a parity check on only the even positions,
         because the overall parity bit is easily calculated from that and the parity of the
         odd positions (which is the least significant check bit). More generally, the overall
         parity bit could as well be a parity check on the complement set of bits checked by
         any one of the SEC parity bits. This observation might save some gates in hardware.
      </p>
      
      <p class="indent">It should be clear that the Hamming SEC code has minimum redundancy. That is, for
         a given number of information bits, it adds a minimum number of check bits that permit
         single error correction. This is so because by construction, just enough check bits
         are added so that when interpreted as an integer, they can index any bit in the code,
         with one state left over to denote “no errors.” In other words, the code satisfies
         inequality (1). Hamming shows that the SEC-DED code constructed from a SEC code by
         adding one overall parity bit is also of minimum redundancy. His argument is to assume
         that a SEC-DED code exists that has fewer check bits, and he derives from this a contradiction
         to the fact that the starting SEC code had minimum redundancy.
      </p>
      
      <h4><strong>Minimum Number of Check Bits Required</strong></h4>
      
      <p class="noindent">The middle column of <a href="ch15.html#ch15tab3">Table 15–3</a> shows minimal solutions of inequality (1) for a range of values of <em>m</em>. The rightmost column simply shows that one more bit is required for a SEC-DED code.
         From this table one can see, for example, that to provide the SEC-DED level ECC for
         a memory word containing 64 information bits, eight check bits are required, giving
         a total memory word size of 72 bits.
      </p>
      
      <p class="tab-caption"><a id="ch15tab3"></a>T<small>ABLE</small> 15–3. E<small>XTRA</small> B<small>ITS FOR</small> E<small>RROR</small> C<small>ORRECTION</small>/D<small>ETECTION</small></p>
      
      <div class="image"><img alt="Image" src="graphics/336tab01.jpg" /></div>
      
      <h4><strong>Concluding Remarks</strong></h4>
      
      <p class="noindent">In the more mathematically oriented ECC literature, the term “Hamming code” is reserved
         for the perfect codes described above—that is, those with (<em>n, m</em>) = (3, 1), (7, 4), (15, 11), (31, 26), and so on. Similarly, the extended Hamming
         codes are the perfect SEC-DED codes described above. Computer architects and engineers
         often <a id="page_336"></a>use the term to denote any of the codes that Hamming described, and some variations.
         The term “extended” is often understood.
      </p>
      
      <p class="indent">The first IBM computer to use Hamming codes was the IBM Stretch computer (model 7030),
         built in 1961 [LC]. It used a (72, 64) SEC-DED code (not a perfect code). A follow-on
         machine known as Harvest (model 7950), built in 1962, was equipped with 22-track tape
         drives that employed a (22, 16) SEC-DED code. The ECCs found on modern machines are
         usually not Hamming codes, but rather are codes devised for some logical or electrical
         property, such as minimizing the depth of the parity check trees, and making them
         all the same length. Such codes give up Hamming’s simple method of determining which
         bit is in error, and instead use a hardware table lookup.
      </p>
      
      <p class="indent">At the time of this writing (2012), most notebook PCs (personal computers) have no
         error checking in their memory systems. Desktop PCs may have none, or they may have
         a simple parity check. Server-class computers generally have ECC at the SEC-DED level.
      </p>
      
      <p class="indent">In the early solid-state computers equipped with ECC memory, the memory was usually
         in the form of eight check bits and 64 information bits. A memory module (group of
         chips) might be built from, typically, nine 8-bit-wide chips. A word access (72 bits,
         including check bits) fetches eight bits from each of these nine chips. Each chip
         is laid out in such a way that the eight bits accessed for a single word are physically
         far apart. Thus, a word access references 72 bits that are physically somewhat separated.
         With bits interleaved in that way, if a few close-together bits in the same chip are
         altered, as, for example, by an alpha particle or cosmic ray hit, a few words will
         have single-bit errors, which can be corrected. Some larger memories incorporate a
         technology known as <em>Chipkill</em>. This allows the computer to continue to function even if an entire memory chip fails,
         for example, due to loss of power to the chip.
      </p>
      
      <p class="indent">The interleaving technique can be used in communication applications to correct burst
         errors by interleaving the bits in time.
      </p>
      
      <p class="indent"><a id="page_337"></a>Today the organization of ECC memories is often more complicated than simply having
         eight check bits and 64 information bits. Modern server memories might have 16 or
         32 information bytes (128 or 256 bits) checked as a single ECC word. Each DRAM chip
         may store two, three, or four bits in physically adjacent positions. Correspondingly,
         ECC is done on alphabets of four, eight, or 16 characters—a subject not discussed
         here. Because the DRAM chips usually come in 8- or 16-bit-wide configurations, the
         memory module often provides more than enough bits for the ECC function. The extra
         bits might be used for other functions, such as one or two parity bits on the memory
         address. This allows the memory to check that the address it receives is (probably)
         the address that the CPU generated.
      </p>
      
      <p class="indent">In modern server-class machines, ECC might be used in different levels of cache memory,
         as well as in main memory. It might also be used in non-memory areas, such as on busses.
      </p>
      
      <h3><a id="ch15lev3"></a><strong>15–3 Software for SEC-DED on 32 Information Bits</strong></h3>
      
      <p class="noindent">This section describes a code for which encoding and decoding can be efficiently implemented
         in software for a basic RISC. It does single error correction and double error detection
         on 32 information bits. The technique is basically Hamming’s.
      </p>
      
      <p class="indent">We follow Hamming in using check bits in such a way that the receiver can easily (in
         software) determine whether zero, one, or two errors occurred, and if one error occurred
         it can easily correct it. We also follow Hamming in using a single overall parity
         bit to convert a SEC code to SEC-DED, and we assume the check bit values are chosen
         to make even parity on the check bit and the bits it checks. A total of seven check
         bits are required (<a href="ch15.html#ch15tab3">Table 15–3</a>).
      </p>
      
      <p class="indent">Consider first just the SEC property, without DED. For SEC, six check bits are required.
         For implementation in software, the main difficulty with Hamming’s method is that
         it merges the six check bits with the 32 information bits, resulting in a 38-bit quantity.
         We are assuming the implementation is done on a 32-bit machine, and the information
         bits are in a 32-bit word. It would be very awkward for the sender to spread out the
         information bits over a 38-bit quantity and calculate the check bits into the positions
         described by Hamming. The receiver would have similar difficulties. The check bits
         could be moved into a separate word or register, with the 32 information bits kept
         in another word or register. But this gives an irregular range of positions that are
         checked by each check bit. In the scheme to be described, these ranges retain most
         of the regularity that they have in Hamming’s scheme (which ignores word boundaries).
         The regularity leads to simplified calculations.
      </p>
      
      <p class="indent">The positions checked by each check bit are shown in <a href="ch15.html#ch15tab4">Table 15–4</a>. In this table, bits are numbered in the usual little-endian way, with position 0
         being the least significant bit (unlike Hamming’s numbering).
      </p>
      
      <p class="tab-caption"><a id="ch15tab4"></a>T<small>ABLE</small> 15–4. P<small>OSITIONS</small> C<small>HECKED BY THE</small> C<small>HECK</small> B<small>ITS</small></p>
      
      <div class="image"><img alt="Image" src="graphics/338tab01.jpg" /></div>
      
      <p class="indent">Observe that each of the 32 information word bit positions is checked by at least
         two check bits. For example, position 6 is checked by <strong><em>p</em></strong><sub>1</sub> and <strong><em>p</em></strong><sub>2</sub> (and also by <strong><em>p</em></strong><sub>5</sub>). Thus, if two information words differ in one bit position, the code words (information
         plus check bits) differ in at least three positions (the information bit that was
         corrupted and two or more check bits), so the code words are at a distance <a id="page_338"></a>of at least three from one another (see “<a href="ch15.html#ch15lev01">Hamming Distance</a>” on page <a href="ch15.html#page_343">343</a>). Furthermore, if two information words differ in two bit positions, then at least
         one of <strong><em>p</em></strong><sub>0</sub> – <strong><em>p</em></strong><sub>5</sub> checks one of the positions, but not the other, so again the code words will be at
         least a distance of three apart. Therefore, the above scheme represents a code with
         minimum distance three (a SEC code).
      </p>
      
      <p class="indent">Suppose a code word is transmitted to a receiver. Let <strong><em>u</em></strong> denote the information bits received, <strong><em>p</em></strong> denote the check bits received, and <strong><em>s</em></strong> (for syndrome) denote the <em>exclusive or</em> of <strong><em>p</em></strong> and the check bits calculated from <strong><em>u</em></strong> by the receiver. Then, examination of <a href="ch15.html#ch15tab4">Table 15–4</a> reveals that <strong><em>s</em></strong> will be set as shown in <a href="ch15.html#ch15tab5">Table 15–5</a>, for zero or one errors in the code word.
      </p>
      
      <p class="tab-caption"><a id="ch15tab5"></a>T<small>ABLE</small> 15–5. S<small>YNDROME FOR</small> Z<small>ERO OR</small> O<small>NE</small> E<small>RRORS</small></p>
      
      <div class="image"><img alt="Image" src="graphics/338tab02.jpg" /></div>
      
      <p class="indent"><a id="page_339"></a>As an example, suppose information bit <strong><em>u</em></strong><sub>4</sub> is corrupted in transmission. <a href="ch15.html#ch15tab4">Table 15–4</a> shows that <strong><em>u</em></strong><sub>4</sub> is checked by check bits <strong><em>p</em></strong><sub>2</sub> and <strong><em>p</em></strong><sub>5</sub>. Therefore, the check bits calculated by the sender and receiver will differ in <strong><em>p</em></strong><sub>2</sub> and <strong><em>p</em></strong><sub>5</sub>. In this scenario the check bits received are the same as those transmitted, so the
         syndrome will have bits 2 and 5 set—that is, it will be 100100.
      </p>
      
      <p class="indent">If one of the check bits is corrupted in transmission (and no errors occur in the
         information bits), then the check bits received and those calculated by the receiver
         (which equal those calculated by the sender) differ in the check bit that was corrupted,
         and in no other bits, as shown in the last six rows of <a href="ch15.html#ch15tab5">Table 15–5</a>.
      </p>
      
      <p class="indent">The syndromes shown in <a href="ch15.html#ch15tab5">Table 15–5</a> are distinct for all 39 possibilities of no error or a single-bit error anywhere
         in the code word. Therefore, the syndrome identifies whether or not an error occurred,
         and if so, which bit position is in error. Furthermore, if a single-bit error occurred,
         it is fairly easy to calculate which bit is in error (without resorting to a table
         lookup) and to correct it. Here is the logic:
      </p>
      
      <p class="indentpre">If <strong><em>s</em></strong> = <strong>0</strong>, no error occurred.<br />If <strong>s</strong> = <strong>011111, <em>u</em></strong><sub>0</sub> is in error.<br />If <strong><em>s</em></strong> = <strong>1xxxxx</strong>, with <strong>xxxxx</strong> nonzero, the error is in <strong><em>u</em></strong> at position <strong>xxxxx</strong>.
      </p>
      
      <p class="indent">Otherwise, a single bit in <strong><em>s</em></strong> is set, the error is in a check bit, and the correct check bits are given by the
         <em>exclusive or</em> of the syndrome and the received check bits (or by the calculated check bits).
      </p>
      
      <p class="indent">Under the assumption that an error in the check bits need not be corrected, this can
         be expressed as shown here, where <strong><em>b</em></strong> is the bit number to be corrected.
      </p>
      
      <div class="image1"><img alt="Image" src="graphics/339fig01.jpg" /></div>
      
      <p class="indent">There is a hack that changes the second if-then-else construction shown above into
         an assignment statement.
      </p>
      
      <p class="indent">To recognize double-bit errors, an overall parity bit is computed (parity of <strong><em>u</em></strong><sub>31:0</sub> and <strong><em>p</em></strong><sub>5:0</sub>), and put in bit position 6 of <strong><em>p</em></strong> for transmission. Double-bit errors are distinguished by the overall parity being
         correct, but with the syndrome (<strong><em>s</em></strong><sub>5:0</sub>) being nonzero. The reason the syndrome is nonzero is the same as in the case of
         the extended Hamming code, given on page <a href="ch15.html#page_334">334</a>.
      </p>
      
      <p class="indent">Software that implements this code is shown in <a href="ch15.html#ch15fig1">Figures 15–1</a> and <a href="ch15.html#ch15fig2">15–2</a>. We assume the simple case of a sender and a receiver, and the receiver has no need
         to correct an error that occurs in the check bits or in the overall parity bit.
      </p>
      
      <p class="codelink"><a id="page_340"></a><a href="images14.html#p15fig01" id="p15fig01a">Click here to view code image</a></p>
      
      <hr />
      
      <p class="programlisting"><a id="ch15fig1"></a>unsigned int checkbits(unsigned int u) {<br /><br />   /* Computes the six parity check bits for the<br />   "information" bits given in the 32-bit word u. The<br />   check bits are p[5:0]. On sending, an overall parity<br />   bit will be prepended to p (by another process).<br /><br />   Bit Checks these bits of u<br />   p[0] 0, 1, 3, 5, ..., 31 (0 and the odd positions).<br />   p[1] 0, 2-3, 6-7, ..., 30-31 (0 and positions xxx1x).<br />   p[2] 0, 4-7, 12-15, 20-23, 28-31 (0 and posns xx1xx).<br />   p[3] 0, 8-15, 24-31 (0 and positions x1xxx).<br />   p[4] 0, 16-31 (0 and positions 1xxxx).<br />   p[5] 1-31 */<br /><br />   unsigned int p0, p1, p2, p3, p4, p5, p6, p;<br />   unsigned int t1, t2, t3;<br /><br />   // First calculate p[5:0] ignoring u[0].<br />   p0 = u ^ (u &gt;&gt; 2);<br />   p0 = p0 ^ (p0 &gt;&gt; 4);<br />   p0 = p0 ^ (p0 &gt;&gt; 8);<br />   p0 = p0 ^ (p0 &gt;&gt; 16);               // p0 is in posn 1.<br /><br />   t1 = u ^ (u &gt;&gt; 1);<br />   p1 = t1 ^ (t1 &gt;&gt; 4);<br />   p1 = p1 ^ (p1 &gt;&gt; 8);<br />   p1 = p1 ^ (p1 &gt;&gt; 16);               // p1 is in posn 2.<br /><br />   t2 = t1 ^ (t1 &gt;&gt; 2);<br />   p2 = t2 ^ (t2 &gt;&gt; 8);<br />   p2 = p2 ^ (p2 &gt;&gt; 16);               // p2 is in posn 4.<br /><br />   t3 = t2 ^ (t2 &gt;&gt; 4);<br />   p3 = t3 ^ (t3 &gt;&gt; 16);               // p3 is in posn 8.<br /><br />   p4 = t3 ^ (t3 &gt;&gt; 8)                 // p4 is in posn 16.<br /><br />   p5 = p4 ^ (p4 &gt;&gt; 16);               // p5 is in posn 0.<br />   <br />   p = ((p0&gt;&gt;1) &amp; 1)  | ((pl&gt;&gt;l) &amp; 2) | ((p2&gt;&gt;2) &amp; 4) |<br />       ((p3&gt;&gt;5) &amp; 8)  | ((p4&gt;&gt;12) &amp; 16) | ((p5 &amp; 1) <span class="entity">&lt;&lt;</span> 5);<br /><br />   p = p ^ (-(u &amp; 1) &amp; 0x3F);         // Now account for u[0].<br />   return p;<br />}
      </p>
      
      <hr />
      
      <p class="fig-caption">F<small>IGURE</small> 15–1. Calculation of check bits.
      </p>
      
      <p class="codelink"><a id="page_341"></a><a href="images14.html#p15fig02" id="p15fig02a">Click here to view code image</a></p>
      
      <hr />
      
      <p class="programlisting"><a id="ch15fig2"></a>int correct(unsigned int pr, unsigned int *ur) {<br /><br />   /* This function looks at the received seven check<br />   bits and 32 information bits (pr and ur) and<br />   determines how many errors occurred (under the<br />   presumption that it must be 0, 1, or 2). It returns<br />   with 0, 1, or 2, meaning that no errors, one error, or<br />   two errors occurred. It corrects the information word<br />   received (ur) if there was one error in it. */<br /><br />   unsigned int po, p, syn, b;  <br /><br />   po = parity(pr ^ *ur);         // Compute overall parity<br />                                  // of the received data.<br />   p = checkbits(*ur);            // Calculate check bits<br />                                  // for the received info.<br />   syn = p ^ (pr &amp; 0x3F);         // Syndrome (exclusive of<br />                                  // overall parity bit). <br />   if (po == 0) {<br />      if (syn == 0) return 0;     // If no errors, return 0.<br />      else return 2;              // Two errors, return 2.<br />   }<br />                                  // One error occurred.<br />   if (((syn - 1) &amp; syn) == 0)    // If syn has zero or one<br />      return 1;                   // bits set, then the<br />                                  // error is in the check<br />                                  // bits or the overall<br />                                  // parity bit (no<br />                                  // correction required).<br /><br />   // One error, and syn bits 5:0 tell where it is in ur.<br /><br />   b = syn - 31 - (syn &gt;&gt; 5);   // Map syn to range 0 to 31.  <br />// if (syn == 0x1f) b = 0;     // (These two lines equiv.<br />// else b = syn &amp; 0x1f;        // to the one line above.)<br />   *ur = *ur ^ (1 &lt;&lt; b);       // Correct the bit.<br />   return 1;<br />}
      </p>
      
      <hr />
      
      <p class="fig-caption">F<small>IGURE</small> 15–2. The receiver’s actions.
      </p>
      
      <p class="indent"><a id="page_342"></a>To compute the check bits, function <code>checkbits</code> first ignores information bit <strong><em>u</em></strong><sub>0</sub> and computes
      </p>
      
      <div class="image"><img alt="Image" src="graphics/342fig01.jpg" /></div>
      
      <p class="noindent">except omitting line <em>i</em> when computing check bit <strong><em>k</em></strong><sub><em>i</em></sub>, for 0 ≤ <em>i</em> ≤ 4. This puts <strong><em>p</em></strong><sub><em>i</em></sub> in various positions of word <strong><em>x</em></strong>, as shown in <a href="ch15.html#ch15fig1">Figure 15–1</a>. For <strong><em>p</em></strong><sub>5</sub>, all the above assignments are used. This is where the regularity of the pattern
         of bits checked by each check bit pays off; a lot of code commoning can be done. This
         reduces what would be 4×5 + 5 = 25 such assignments to 15, as shown in <a href="ch15.html#ch15fig1">Figure 15–1</a>.
      </p>
      
      <p class="indent">Incidentally, if the computer has an instruction for computing the parity of a word,
         or has the <em>population count</em> instruction (which puts the word parity in the least significant bit of the target
         register), then the regular pattern is not needed. On such a machine, the check bits
         might be computed as
      </p>
      
      <p class="codelink"><a href="images14.html#p342fig02" id="p342fig02a">Click here to view code image</a></p>
      
      <p class="programlisting1">p0 = pop(u ^ 0xAAAAAAAB) &amp; 1;<br />p1 = pop(u &amp; 0xCCCCCCCD) &amp; 1;
      </p>
      
      <p class="noindent">and so forth.</p>
      
      <p class="indent">After packing the six check bits into a single quantity <code>p</code>, the <code>checkbits</code> function accounts for information bit <code>u<sub>0</sub></code> by complementing all six check bits if <code>u</code><sub>0</sub> = 1. (See <a href="ch15.html#ch15tab4">Table 15-4</a>; <code>p</code><sub>5</sub> must be complemented because <code>u</code><sub>0</sub> was erroneously included in the calculation of <code>p</code><sub>5</sub> up to this point.)
      </p>
      
      <h3><a id="ch15lev4"></a><strong>15–4 Error Correction Considered More Generally</strong></h3>
      
      <p class="noindent">This section continues to focus on the binary FEC block codes, but a little more generally
         than the codes described in <a href="ch15.html#ch15lev2">Section 15–2</a>. We drop the assumption that the block consists of a set of “information” bits and
         a distinct set of “check” bits, and any implication that the number of code words
         must be a power of 2. We also consider levels of error correction and detection capability
         greater than SEC and SEC-DED. For example, suppose you want a double error-correcting
         code for a binary representation of decimal digits. If the code has 16 code words
         (with ten being used to represent the decimal digits and six being unused), the length
         of the code words must be at least 11 bits. But if a code with only 10 code words
         is used, the code words can be of length 10 bits. (This is shown in <a href="ch15.html#ch15tab8">Table 15–8</a> on page <a href="ch15.html#page_351">351</a>, in the column for <em>d</em> = 5, as is explained below.)
      </p>
      
      <p class="indent"><a id="page_343"></a>A <em>code</em> is simply a set of <em>code words</em>, and for our purposes the code words are binary strings all of the same length which,
         as mentioned above, is called the <em>code length</em>. The number of code words in the set is called the <em>code size</em>. We make no interpretation of the code words; they might represent alphanumeric characters
         or pixel values in a picture, for example.
      </p>
      
      <p class="indent">As a trivial example, a code might consist of the binary integers from 0 to 7, with
         each bit repeated three times:
      </p>
      
      <p class="center">{000000000, 000000111, 000111000, 000111111, 111000000, ... 111111111}.</p>
      
      <p class="indent">Another example is the <em>two-out-of-five</em> code, in which each code word has exactly two 1-bits:
      </p>
      
      <p class="center">{00011, 00101, 00110, 01001, 01010, 01100, 10001, 10010, 10100, 11000}.</p>
      
      <p class="noindent">The code size is 10, and thus it is suitable for representing decimal digits. Notice
         that if code word 00110 is considered to represent decimal 0, then the remaining values
         can be decoded into digits 1 through 9 by giving the bits weights of 6, 3, 2, 1, and
         0, in left-to-right order.
      </p>
      
      <p class="indent">The <em>code rate</em> is a measure of the efficiency of a code. For a code like Hamming’s, this can be
         defined as the number of information bits divided by the code length. For the Hamming
         code discussed above, it is 4/7 ≈ 0.57. More generally, the code rate is defined as
         the log base 2 of the code size, divided by the code length. The simple codes above
         have rates of log<sub>2</sub>(8)/9 ≈ 0.33 and log<sub>2</sub>(10)/5 ≈ 0.66, respectively.
      </p>
      
      <h4 id="ch15lev01"><strong>Hamming Distance</strong></h4>
      
      <p class="noindent">The central concept in the theory of ECC is that of <em>Hamming distance</em>. The Hamming distance between two words (of equal length) is the number of bit positions
         in which they differ. Put another way, it is the population count of the <em>exclusive or</em> of the two words. It is appropriate to call this a distance function because it satisfies
         the definition of a distance function used in linear algebra:
      </p>
      
      <div class="image"><img alt="Image" src="graphics/343fig01.jpg" /></div>
      
      <p class="noindent">Here <em>d</em>(<strong><em>x, y</em></strong>) denotes the Hamming distance between code words <strong><em>x</em></strong> and <strong><em>y</em></strong>, which for brevity we will call simply the <em>distance</em> between <strong><em>x</em></strong> and <strong><em>y</em></strong>.
      </p>
      
      <p class="indent">Suppose a code has a minimum distance of 1. That is, there are two words <strong><em>x</em></strong> and <strong><em>y</em></strong> in the set that differ in only one bit position. Clearly, if <strong><em>x</em></strong> were transmitted and the bit that makes it distinct from <strong><em>y</em></strong> were flipped due to a transmission error, then the receiver could not distinguish
         between receiving <strong><em>x</em></strong> with a certain bit in error <a id="page_344"></a>and receiving <strong><em>y</em></strong> with no errors. Hence in such a code it is impossible to detect even a 1-bit error,
         in general.
      </p>
      
      <p class="indent">Suppose now that a code has a minimum distance of 2. Then if just one bit is flipped
         in transmission, an invalid code word is produced, and thus the receiver can (in principle)
         detect the error. If two bits are flipped, a valid code word might be transformed
         into another valid code word. Thus, double-bit errors cannot be detected. Furthermore,
         single-bit errors cannot be <em>corrected</em>. This is because if a received word has one bit in error, then there may be two code
         words that are one bit-change away from the received word, and the receiver has no
         basis for deciding which is the original code word.
      </p>
      
      <p class="indent">The code obtained by appending a single parity bit is in this category. It is shown
         below for the case of three information bits (<em>m</em> = 3). The rightmost bit is the parity bit, chosen to make even parity on all four
         bits. The reader may verify that the minimum distance between code words is 2.
      </p>
      
      <div class="image"><img alt="Image" src="graphics/344fig01.jpg" /></div>
      
      <p class="indent">Actually, adding a single parity bit permits detecting any odd number of errors, but
         when we say that a code permits detecting <em>k</em>-bit errors, we mean <em>all</em> errors up to <em>k</em> bits.
      </p>
      
      <p class="indent">Now consider the case in which the minimum distance between code words is 3. If any
         one or two bits is flipped in transmission, an invalid code word results. If just
         one bit is flipped, the receiver can (we imagine) try flipping each of the received
         bits one at a time, and in only one case will a code word result. Hence in such a
         code the receiver can detect and correct a single-bit error. A double-bit error might
         appear to be a single-bit error from another code word, and thus the receiver cannot
         detect double-bit errors.
      </p>
      
      <p class="indent">Similarly, it is easy to reason that if the minimum distance of a code is 4, the receiver
         can correct all single-bit errors and detect all double-bit errors (it is a SEC-DED
         code). As mentioned above, this is the level of capability often used in computer
         memories.
      </p>
      
      <p class="indent"><a href="ch15.html#ch15tab6">Table 15–6</a> summarizes the error-correction and -detection capabilities of a block code based
         on its minimum distance.
      </p>
      
      <p class="tab-caption"><a id="ch15tab6"></a>T<small>ABLE</small> 15–6. N<small>UMBER OF</small> B<small>ITS</small> C<small>ORRECTED</small>/D<small>ETECTED</small></p>
      
      <div class="image"><img alt="Image" src="graphics/345tab01.jpg" /></div>
      
      <p class="indent">Error-correction capability can be traded for error detection. For example, if the
         minimum distance of a code is 3, that redundancy can be used to correct no errors
         but to detect single- or double-bit errors. If the minimum distance is 5, the code
         can be used to correct single-bit errors and detect 3-bit errors, or to correct no
         <a id="page_345"></a>errors but to detect 4-bit errors, and so forth. Whatever is subtracted from the “Correct”
         column of <a href="ch15.html#ch15tab6">Table 15–6</a> can be added to the “Detect” column.
      </p>
      
      <h4><strong>The Main Coding Theory Problem</strong></h4>
      
      <p class="noindent">Up to this point we have asked, “Given a number of information bits <em>m</em> and a desired minimum distance <em>d</em>, how many check bits are required?” In the interest of generality, we will now turn
         this question around and ask, “For a given code length <em>n</em> and minimum distance <em>d</em>, how many code words are possible?” Thus, the number of code words need not be an
         integral power of 2.
      </p>
      
      <p class="indent">Following [Roman] and others, let <em>A</em>(<em>n, d</em>) denote the largest possible code size for a (binary) code with length <em>n</em> and minimum distance <em>d</em>. The remainder of this section is devoted to exploring some of what is known about
         this function. Determining its values has been called <em>the main coding theory problem</em> [Hill, Roman]. Throughout this section we assume that <em>n</em> ≥ <em>d</em> ≥ 1.
      </p>
      
      <p class="indent">It is nearly trivial that</p>
      
      <div class="image"><img alt="Image" src="graphics/345equ01.jpg" /></div>
      
      <p class="noindent">because there are 2<sup><em>n</em></sup> distinct words of length <em>n</em>.
      </p>
      
      <p class="indent">For minimum distance 2, we know from the single parity bit example that <em>A</em>(<em>n</em>, 2) ≥ 2<sup><em>n</em>–1</sup>. But <em>A</em>(<em>n</em>, 2) cannot exceed 2<sup><em>n</em>–1</sup> for the following reason. Suppose there is a code of length <em>n</em> and minimum distance 2 that has more than 2<sup><em>n</em>–1</sup> code words. Delete any one column from the code words. (We envision the code words
         as being arranged in a matrix much like that of <a href="ch15.html#ch15tab1">Table 15–1</a> on page <a href="ch15.html#page_333">333</a>.) This produces a code of length <em>n</em> – 1 and minimum distance at least 1 (deleting a column can reduce the minimum distance
         by at most 1), and of size exceeding 2<sup><em>n</em> – 1</sup>. Thus, it has <em>A</em>(<em>n</em> – 1, 1) &gt; 2<sup><em>n</em> – 1</sup>, contradicting Equation (2). Hence,
      </p>
      
      <p class="center"><em>A</em>(<em>n</em> ,2) = 2<sup><em>n</em> – 1</sup>.
      </p>
      
      <p class="indent"><a id="page_346"></a>That was not difficult. What about <em>A</em>(<em>n</em>, 3)? That is an unsolved problem, in the sense that no formula or reasonably easy
         means of calculating it is known. Of course, many specific values of <em>A</em> (<em>n</em>, 3) are known, and some bounds are known, but the exact value is unknown in most
         cases.
      </p>
      
      <p class="indent">When equality holds in (1), it represents the solution to this problem for the case
         <em>d</em> = 3. Letting <em>n</em> = <em>m</em> + <em>k</em>, (1) can be rewritten
      </p>
      
      <div class="image"><img alt="Image" src="graphics/346equ01.jpg" /></div>
      
      <p class="noindent">Here, <em>m</em> is the number of information bits, so 2<sup><em>m</em></sup> is the maximum number of code words. Hence, we have
      </p>
      
      <div class="image"><img alt="Image" src="graphics/346equ02.jpg" /></div>
      
      <p class="noindent">with equality holding when 2<sup><em>n</em></sup>/(<em>n</em> + 1) is an integer (by Hamming’s construction).
      </p>
      
      <p class="indent">For <em>n</em> = 7, this gives <em>A</em>(7, 3) = 16, which we already know from <a href="ch15.html#ch15lev2">Section 15–2</a>. For <em>n</em> = 3 it gives <em>A</em> (3, 3) ≤ 2, and the limit of 2 can be realized with code words 000 and 111. For <em>n</em> = 4 it gives <em>A</em> (4, 3) ≤ 3.2, and with a little doodling you will see that it is not possible to
         get three code words of length 4 with <em>d</em> = 3. Thus, when equality does not hold in (3), it merely gives an upper bound, quite
         possibly not realizable, on the maximum number of code words.
      </p>
      
      <p class="indent">An interesting relation is that for <em>n</em> ≥ 2,
      </p>
      
      <div class="image"><img alt="Image" src="graphics/346equ03.jpg" /></div>
      
      <p class="noindent">Therefore, adding 1 to the code length at most doubles the number of code words possible
         for the same minimum distance <em>d</em>. To see this, suppose you have a code of length <em>n</em>, distance <em>d</em>, and size <em>A</em>(<em>n, d</em>). Choose an arbitrary column of the code. Either half or more of the code words have
         a 0 in the selected column, or half or more have a 1 in that position. Of these two
         subsets, choose one that has at least <em>A</em>(<em>n, d</em>)/2 code words, form a new code consisting of this subset, and delete the selected
         column (which is either all 0’s or all 1’s). The resulting set of code words has <em>n</em> reduced by 1, has the same distance <em>d</em>, and has at least <em>A</em> (<em>n, d</em>)/2 code words. Thus, <em>A</em>(<em>n</em> – 1, <em>d</em>) ≥ <em>A</em> (<em>n, d</em>)/ 2, from which inequality (4) follows.
      </p>
      
      <p class="indent">A useful relation is that if <em>d</em> is even, then
      </p>
      
      <div class="image"><img alt="Image" src="graphics/346equ04.jpg" /></div>
      
      <p class="noindent">To see this, suppose you have a code <em>C</em> of length <em>n</em> and minimum distance <em>d</em>, with <em>d</em> odd. Form a new code by appending to each word of <em>C</em> a parity bit, let us say to make the parity of each word even. The new code has length
         <em>n</em> + 1 and has the same number of code words as does <em>C</em>. It has minimum distance <em>d</em> + 1. For if two <a id="page_347"></a>words of C are a distance <em>x</em> apart, with <em>x</em> odd, then one word must have even parity and the other must have odd parity. Thus,
         we append a 0 in the first case and a 1 in the second case, which increases the distance
         between the words to <em>x</em> + 1. If <em>x</em> is even, we append a 0 to both words, which does not change the distance between
         them. Because <em>d</em> is odd, all pairs of words that are a distance <em>d</em> apart become distance <em>d</em> + 1 apart. The distance between two words more than <em>d</em> apart either does not change or increases. Therefore the new code has minimum distance
         <em>d</em> + 1. This shows that if <em>d</em> is odd, then <em>A</em>(<em>n</em>+ 1, <em>d</em> + 1) ≥ <em>A</em>(<em>n, d</em>), or, equivalently, <em>A</em>(<em>n, d</em>) ≥ <em>A</em>(<em>n</em> – 1, <em>d</em> – 1) for even <em>d</em> ≥ 2.
      </p>
      
      <p class="indent">Now suppose you have a code of length <em>n</em> and minimum distance <em>d</em> ≥ 2 (<em>d</em> can be odd or even). Form a new code by eliminating any one column. The new code
         has length <em>n</em> – 1, minimum distance at least <em>d</em> – 1, and is the same size as the original code (all the code words of the new code
         are distinct because the new code has minimum distance at least 1). Therefore <em>A</em>(<em>n</em> – 1, <em>d</em> – 1) ≥ <em>A</em> (<em>n, d</em>). This establishes Equation (5).
      </p>
      
      <h4><strong>Spheres</strong></h4>
      
      <p class="noindent">Upper and lower bounds on <em>A</em>(<em>n, d</em>), for any <em>d</em> ≥ 1, can be derived by thinking in terms of <em>n</em>-dimensional spheres. Given a code word, think of it as being at the center of a “sphere”
         of radius <em>r</em>, consisting of all words at a Hamming distance <em>r</em> or less from it.
      </p>
      
      <p class="indent">How many points (words) are in a sphere of radius <em>r</em>? First, consider how many points are in the shell at distance exactly <em>r</em> from the central code word. This is given by the number of ways to choose <em>r</em> different items from <em>n</em>, ignoring the order of choice. We imagine the <em>r</em> chosen bits as being complemented to form a word at distance exactly <em>r</em> from the central point. This “choice” function, often written <span class="middle"><img alt="Image" src="graphics/347fig01.jpg" /></span>, can be calculated from<sup><a id="ch15fna2"></a><a href="footnotes.html#ch15fn2">2</a></sup></p>
      
      <div class="image"><img alt="Image" src="graphics/347equ01.jpg" /></div>
      
      <p class="noindent">Thus, <span class="middle"><img alt="Image" src="graphics/347fig02.jpg" /></span>, <span class="middle"><img alt="Image" src="graphics/347fig03.jpg" /></span>, <span class="middle"><img alt="Image" src="graphics/347fig04.jpg" /></span>, <span class="middle"><img alt="Image" src="graphics/347fig05.jpg" /></span>, and so forth.
      </p>
      
      <p class="indent">The total number of points in a sphere of radius <em>r</em> is the sum of the points in the shells from radius 0 to <em>r</em>:
      </p>
      
      <div class="image"><img alt="Image" src="graphics/347equ02.jpg" /></div>
      
      <p class="noindent">There seems to be no simple formula for this sum [Knu1].</p>
      
      <p class="indent"><a id="page_348"></a>From this it is easy to obtain bounds on <em>A</em>(<em>n, d</em>). First, assume you have a code of length <em>n</em> and minimum distance <em>d</em>, and it consists of <em>M</em> code words. Surround each code word with a sphere, all of the same maximal radius
         such that no two spheres have a point in common. This radius is (<em>d</em> – 1)/2 if <em>d</em> is odd, and is (<em>d</em> – 2)/2 if <em>d</em> is even (see <a href="ch15.html#ch15fig3">Figure 15–3</a>). Because each point is in at most one sphere, the total number of points in the
         <em>M</em> spheres must be less than or equal to the total number of points in the space. That
         is,
      </p>
      
      <div class="image"><img alt="Image" src="graphics/348equ01.jpg" /></div>
      
      <p class="noindent">This holds for any <em>M</em>, hence for <em>M</em> = <em>A</em>(<em>n, d</em>), so that
      </p>
      
      <div class="image"><img alt="Image" src="graphics/348equ02.jpg" /></div>
      
      <p class="noindent">This is known as the <em>sphere-packing bound</em>, or the <em>Hamming bound</em>.
      </p>
      
      <div class="image"><a id="ch15fig3"></a><img alt="Image" src="graphics/15fig03.jpg" /></div>
      
      <p class="fig-caption">F<small>IGURE</small> 15–3. Maximum radius that allows correcting points within a sphere.
      </p>
      
      <p class="indent">The sphere idea also easily gives a lower bound on <em>A</em>(<em>n, d</em>). Assume again that you have a code of length <em>n</em> and minimum distance <em>d</em>, and it has the maximum possible number of code words—that is, it has <em>A</em>(<em>n, d</em>) code words. Surround each code word with a sphere of radius <em>d</em> – 1. Then these spheres must cover <em>all</em> 2<sup><em>n</em></sup> points in the space (possibly overlapping). For if not, there would be a point that
         is at a distance <em>d</em> or more from all code words, and that is impossible because such a point would be
         a code word. Thus, we have a weak form of the <em>Gilbert-Varshamov</em> bound:
      </p>
      
      <div class="image"><img alt="Image" src="graphics/348equ03.jpg" /></div>
      
      <p class="indent">There is the strong form of the G-V bound, which applies to <em>linear</em> codes. Its derivation relies on methods of linear algebra which, important as they
         are to the <a id="page_349"></a>subject of linear codes, are not covered in this short introduction to error-correcting
         codes. Suffice it to say that a linear code is one in which the sum (<em>exclusive or</em>) of any two code words is also a code word. The Hamming code of <a href="ch15.html#ch15tab1">Table 15–1</a> is a linear code. Because the G-V bound is a lower bound on linear codes, it is also
         a lower bound on the unrestricted codes considered here. For large <em>n</em>, it is the best known lower bound on both linear and unrestricted codes.
      </p>
      
      <p class="indent">The strong G-V bound states that <em>A</em>(<em>n, d</em>) ≥ 2<sup><em>m</em></sup>, where <em>m</em> is the largest integer such that
      </p>
      
      <div class="image"><img alt="Image" src="graphics/349equ01.jpg" /></div>
      
      <p class="noindent">That is, it is the value of the right-hand side of this inequality rounded down to
         the next strictly smaller integral power of 2. The “strictness” is important for cases
         such as (<em>n, d</em>) = (8, 3), (16, 3) and (the degenerate case) (6, 7).
      </p>
      
      <p class="indent">Combining these results:</p>
      
      <div class="image"><img alt="Image" src="graphics/349equ02.jpg" /></div>
      
      <p class="noindent">where GP2LT denotes the greatest integral power of 2 (strictly) less than its argument.</p>
      
      <p class="indent"><a href="ch15.html#ch15tab7">Table 15–7</a> gives the values of these bounds for some small values of <em>n</em> and <em>d</em>. A single number in an entry means the lower and upper bounds given by (6) are equal.
      </p>
      
      <p class="tab-caption"><a id="page_350"></a><a id="ch15tab7"></a>T<small>ABLE</small> 15–7. T<small>HE</small> G - V <small>AND</small> H<small>AMMING</small> B<small>OUNDS ON</small> <em>A</em>(<em>n, d</em>)
      </p>
      
      <div class="image"><img alt="Image" src="graphics/350tab01.jpg" /></div>
      
      <p class="indent">If <em>d</em> is even, bounds can be computed directly from (6) or, making use of Equation (5),
         they can be computed from (6) with <em>d</em> replaced with <em>d</em> – 1 and <em>n</em> replaced with <em>n</em> – 1 in the two bounds expressions. It turns out that the latter method always results
         in tighter or equal bounds. Therefore, the entries in <a href="ch15.html#ch15tab7">Table 15–7</a> were calculated only for odd <em>d</em>. To access the table for even <em>d</em>, use the values of <em>d</em> shown in the heading and the values of <em>n</em> shown at the left.
      </p>
      
      <p class="indent">The bounds given by (6) can be seen to be rather loose, especially for large <em>d</em>. The ratio of the upper bound to the lower bound diverges to infinity with increasing
         <em>n</em>. The lower bound is particularly loose. Over a thousand papers have been written
         describing methods to improve these bounds, and the results as of this writing are
         shown in <a href="ch15.html#ch15tab8">Table 15–8</a> [Agrell, Brou; where they differ, <a href="ch15.html#ch15tab8">Table 15–8</a>. shows the tighter bounds].
      </p>
      
      <p class="tab-caption"><a id="page_351"></a><a id="ch15tab8"></a>T<small>ABLE</small> 15–8. B<small>EST</small> K<small>NOWN</small> B<small>OUNDS ON</small> <em>A</em>(<em>n, d</em>)
      </p>
      
      <div class="image"><img alt="Image" src="graphics/351tab01.jpg" /></div>
      
      <p class="indent">The cases of (<em>n, d</em>) = (7, 3), (15, 3), and (23, 7) are <em>perfect</em> codes, meaning that they achieve the upper bound given by (6). This definition is
         a generalization of that given on page <a href="ch15.html#page_333">333</a>. The codes for which <em>n</em> is odd and <em>n</em> = <em>d</em> are also perfect; see exercise 8.
      </p>
      
      <p class="indent">We conclude this chapter by pointing out that the idea of minimum distance over an
         entire code, which leads to the ideas of <em>p</em>-bit error detection and <em>q</em>-bit error correction for some <em>p</em> and <em>q</em>, is not the only criterion for the “power” of a binary FEC block code. For example,
         work has been done on codes aimed at correcting burst errors. [Etzion] has demonstrated
         a (16, 11) code, and others, that can correct any single-bit error and any error in
         two consecutive bits, and is perfect, in a sense not discussed here. It is not capable
         of general double-bit error detection. The (16, 11) extended Hamming code is SEC-DED
         and is perfect. Thus, his code gives up general double-bit error <em>detection</em> in return for double-bit error <em>correction</em> of consecutive bits. This is, of course, interesting because in many applications
         errors are likely to occur in short bursts.
      </p>
      
      <h4><a id="page_352"></a><strong>Exercises</strong></h4>
      
      <p class="question"><a href="ch19_answer.html#ch15ans1" id="ch15ansa1"><strong>1</strong>.</a> Show a Hamming code for <em>m</em> = 3 (make a table similar to <a href="ch15.html#ch15tab1">Table 15-1</a>).
      </p>
      
      <p class="question"><a href="ch19_answer.html#ch15ans2" id="ch15ansa2"><strong>2</strong>.</a> In a certain application of an SEC code, there is no need to correct the check bits.
         Hence the <em>k</em> check bits need only check the information bits, but not themselves. For <em>m</em> information bits, <em>k</em> must be large enough so that the receiver can distinguish <em>m</em> + 1 cases: which of the <em>m</em> bits is in error, or no error occurred. Thus, the number of check bits required is
         given by 2<sup><em>k</em></sup> ≥ <em>m</em> + 1. This is a weaker restriction on <em>k</em> than is the Hamming rule, so it should be possible to construct, for some values
         of <em>m</em>, an SEC code that has fewer check bits than those required by the Hamming rule. Alternatively,
         one could have just one value to signify that an error occurred somewhere in the check
         bits, without specifying where. This would lead to the rule 2<sup><em>k</em></sup> ≥ <em>m</em> + 2.
      </p>
      
      <p class="indenthangingBB">What is wrong with this reasoning?</p>
      
      <p class="question"><a href="ch19_answer.html#ch15ans3" id="ch15ansa3"><strong>3</strong>.</a> (Brain teaser) Given <em>m</em>, how would you find the least <em>k</em> that satisfies inequality (1)?
      </p>
      
      <p class="question"><a href="ch19_answer.html#ch15ans4" id="ch15ansa4"><strong>4</strong>.</a> Show that the Hamming distance function for any binary block code satisfies the triangle
         inequality: if <em>x</em> and <em>y</em> are code vectors and <em>d</em>(<strong><em>x, y</em></strong>) denotes the Hamming distance between them, then
      </p>
      
      <p class="center"><em>d</em> (<strong><em>x, z</em></strong>) ≤ <em>d</em> (<strong><em>x, y</em></strong>) + <em>d</em> (<strong><em>y, z</em></strong>).
      </p>
      
      <p class="question"><a href="ch19_answer.html#ch15ans5" id="ch15ansa5"><strong>5</strong>.</a> Prove: <em>A</em>(2<em>n</em>, 2<em>d</em>) ≥ <em>A</em>(<em>n, d</em>).
      </p>
      
      <p class="question"><a href="ch19_answer.html#ch15ans6" id="ch15ansa6"><strong>6</strong>.</a> Prove the “singleton bound”: <em>A</em>(<em>n, d</em>) ≤ 2<sup><em>n</em> – <em>d</em> + 1</sup>.
      </p>
      
      <p class="question"><a href="ch19_answer.html#ch15ans7" id="ch15ansa7"><strong>7</strong>.</a> Show that the notion of a perfect code as equality in the right-hand portion of inequality
         (6) is a generalization of the Hamming rule.
      </p>
      
      <p class="question"><a href="ch19_answer.html#ch15ans8" id="ch15ansa8"><strong>8</strong>.</a> What is the value of <em>A</em>(<em>n, d</em>) if <em>n</em> = <em>d</em>? Show that for odd <em>n</em>, these codes are perfect.
      </p>
      
      <p class="question"><a href="ch19_answer.html#ch15ans9" id="ch15ansa9"><strong>9</strong>.</a> Show that if <em>n</em> is a multiple of 3 and <em>d</em> = 2<em>n</em>/3, then <em>A</em>(<em>n, d</em>) = 4.
      </p>
      
      <p class="question1"><a href="ch19_answer.html#ch15ans10" id="ch15ansa10"><strong>10</strong>.</a> Show that if <em>d</em> &gt; 2<em>n</em>/3, then <em>A</em>(<em>n, d</em>) = 2.
      </p>
      
      <p class="question1"><a href="ch19_answer.html#ch15ans11" id="ch15ansa11"><strong>11</strong>.</a> A two-dimensional parity check scheme for 64 information bits arranges the information
         bits <strong><em>u</em></strong><sub>0</sub> ... <strong><em>u</em></strong><sub>63</sub> into an 8×8 array, and appends a parity bit to each row and column as shown below.
      </p>
      
      <div class="image"><img alt="Image" src="graphics/352equ01.jpg" /></div>
      
      <p class="indent"><a id="page_353"></a>The <strong><em>r</em></strong><sub><em>i</em></sub> are parity check bits on the rows, and the <strong><em>c</em></strong><sub><em>i</em></sub> are parity check bits on the columns. The “corner” check bit could be parity check
         on the row or the column of check bits (but not both); it is shown as a check on the
         bottom row (check bits <strong><em>c</em></strong><sub>0</sub> through <strong><em>c</em></strong><sub>7</sub>).
      </p>
      
      <p class="indent">Comment on this scheme. In particular, is it SEC-DED? Is its error-detection and -correction
         capability significantly altered if the corner bit <strong><em>r</em></strong><sub>8</sub> is omitted? Is there any simple relation between the value of the corner bit if it’s
         a row sum or a column sum?<a id="page_354"></a></p>
      
   </body>
   
</html>