<html>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>15.3 Elements of dynamic programming</title>
<link rel="STYLESHEET" type="text/css" href="images/xpolecat.css">
<link rel="STYLESHEET" type="text/css" href="images/ie.content.books24x7.css">
</head>
<body >
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#e6e6e6">
<tr style="background-image: url(images/tile_back.gif);">
<td align="left" width="30%">
<div STYLE="MARGIN-LEFT: 0.15in;">
<a href="DDU0088.html"><img src="images/previous.gif" width="70" height="19" border="0" align="absmiddle" alt="Previous Section"></a>
</div></td>
<td align="center" width="40%">
<a href="toc.html" style="color:white;text-decoration:none;text-underline:none">&nbsp;&lt;&nbsp;Day Day Up&nbsp;&gt;&nbsp;</a>
</td>
<td align="right" width="30%">
<div STYLE="MARGIN-RIGHT: 0.15in"><a href="DDU0090.html"><img src="images/next.gif" width="70" height="19" border="0" align="absmiddle" alt="Next Section"></a>
</div></td></tr>
</table>
<hr size="1">
<br>
<div class="chapter">
<a name="ch15"></a>
<div class="section">
<h2 class="first-section-title">
<a name="1062"></a><a name="ch15lev1sec3"></a><span class="section-titlelabel">15.3 </span>Elements of dynamic programming</h2>
<p class="first-para">Although we have just worked through two examples of the dynamic-programming method, you might still be wondering just when the method applies. From an engineering perspective, when should we look for a dynamic-programming solution to a problem? In this section, we examine the two key ingredients that an optimization problem must have in order for dynamic programming to be applicable: optimal substructure and overlapping subproblems. We also look at a variant method, called memoization,<sup>[<a name="N18" href="#ftn.N18">1</a>]</sup> for taking advantage of the overlapping-subproblems property.</p>
<div class="section">
<h4 class="sect4-title">
<a name="1063"></a><a name="ch15lev3sec10"></a>Optimal substructure</h4>
<p class="first-para">The first step in solving an optimization problem by dynamic programming is to characterize the structure of an optimal solution. Recall that a problem exhibits <b class="bold"><i class="emphasis">optimal substructure</i></b> if an optimal solution to the problem contains within it optimal solutions to subproblems. Whenever a problem exhibits optimal substructure, it is a good clue that dynamic programming might apply. (It also might mean that a greedy strategy applies, however. See <a href="DDU0093.html#1143" target="_parent" class="chapterjump">Chapter 16</a>.) In dynamic programming, we build an optimal solution to the problem from optimal solutions to subproblems. Consequently, we must take care to ensure that the range of subproblems we consider includes those used in an optimal solution.</p>
<p class="para">We discovered optimal substructure in both of the problems we have examined in this chapter so far. In <a href="DDU0087.html#1011" target="_parent" class="chapterjump">Section 15.1</a>, we observed that the fastest way through station <i class="emphasis">j</i> of either line contained within it the fastest way through station <i class="emphasis">j</i> - 1 on one line. In <a href="DDU0088.html#1039" target="_parent" class="chapterjump">Section 15.2</a>, we observed that an optimal parenthesization of <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A</i><sub><i class="emphasis">i</i>+1</sub> <i class="emphasis">A<sub>j</sub></i> that splits the product between <i class="emphasis">A<sub>k</sub></i> and <i class="emphasis">A</i><sub><i class="emphasis">k</i>+1</sub> contains within it optimal solutions to the problems of parenthesizing <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A</i><sub><i class="emphasis">i</i>+1</sub> <i class="emphasis">A<sub>k</sub></i> and <i class="emphasis">A</i><sub><i class="emphasis">k</i>+1</sub> <i class="emphasis">A</i><sub><i class="emphasis">k</i>+2</sub> <i class="emphasis">A<sub>j</sub></i>.</p>
<p class="para">You will find yourself following a common pattern in discovering optimal substructure:</p>
<a name="1064"></a><a name="IDX-340"></a>
<ol class="orderedlist">
<li class="first-listitem">
<p class="first-para">You show that a solution to the problem consists of making a choice, such as choosing a preceding assembly-line station or choosing an index at which to split the matrix chain. Making this choice leaves one or more subproblems to be solved.</p>
</li>
<li class="listitem">
<p class="first-para">You suppose that for a given problem, you are given the choice that leads to an optimal solution. You do not concern yourself yet with how to determine this choice. You just assume that it has been given to you.</p>
</li>
<li class="listitem">
<p class="first-para">Given this choice, you determine which subproblems ensue and how to best characterize the resulting space of subproblems.</p>
</li>
<li class="listitem">
<p class="first-para">You show that the solutions to the subproblems used within the optimal solution to the problem must themselves be optimal by using a "cut-and-paste" technique. You do so by supposing that each of the subproblem solutions is not optimal and then deriving a contradiction. In particular, by "cutting out" the nonoptimal subproblem solution and "pasting in" the optimal one, you show that you can get a better solution to the original problem, thus contradicting your supposition that you already had an optimal solution. If there is more than one subproblem, they are typically so similar that the cut-and-paste argument for one can be modified for the others with little effort.</p>
</li>
</ol>
<p class="para">To characterize the space of subproblems, a good rule of thumb is to try to keep the space as simple as possible, and then to expand it as necessary. For example, the space of subproblems that we considered for assembly-line scheduling was the fastest way from entry into the factory through stations <i class="emphasis">S</i><sub>1, <i class="emphasis">j</i></sub> and <i class="emphasis">S</i><sub>2, <i class="emphasis">j</i></sub>. This subproblem space worked well, and there was no need to try a more general space of subproblems.</p>
<p class="para">Conversely, suppose that we had tried to constrain our subproblem space for matrix-chain multiplication to matrix products of the form <i class="emphasis">A</i><sub>1</sub> <i class="emphasis">A</i><sub>2</sub> <i class="emphasis">A<sub>j</sub></i>. As before, an optimal parenthesization must split this product between <i class="emphasis">A<sub>k</sub></i> and <i class="emphasis">A</i><sub><i class="emphasis">k</i>+1</sub> for some 1 <span class="unicode">&le;</span> <i class="emphasis">k</i> <span class="unicode">&le;</span> <i class="emphasis">j</i>. Unless we could guarantee that <i class="emphasis">k</i> always equals <i class="emphasis">j</i> - 1, we would find that we had subproblems of the form <i class="emphasis">A</i><sub>1</sub> <i class="emphasis">A</i><sub>2</sub> <i class="emphasis">A<sub>k</sub></i> and <i class="emphasis">A</i><sub><i class="emphasis">k</i>+1</sub> <i class="emphasis">A</i><sub><i class="emphasis">k</i>+2</sub> <i class="emphasis">A<sub>j</sub></i>, and that the latter subproblem is not of the form <i class="emphasis">A</i><sub>1</sub> <i class="emphasis">A</i><sub>2</sub> <i class="emphasis">A<sub>j</sub></i>. For this problem, it was necessary to allow our subproblems to vary at "both ends," that is, to allow both <i class="emphasis">i</i> and <i class="emphasis">j</i> to vary in the subproblem <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A</i><sub>i+1</sub> <i class="emphasis">A<sub>j</sub></i>.</p>
<p class="para">Optimal substructure varies across problem domains in two ways:</p>
<ol class="orderedlist">
<li class="first-listitem">
<p class="first-para">how many subproblems are used in an optimal solution to the original problem, and</p>
</li>
<li class="listitem">
<p class="first-para">how many choices we have in determining which subproblem(s) to use in an optimal solution.</p>
</li>
</ol>
<p class="para">In assembly-line scheduling, an optimal solution uses just one subproblem, but we must consider two choices in order to determine an optimal solution. To find <a name="1065"></a><a name="IDX-341"></a>the fastest way through station <i class="emphasis">S<sub>i,j</sub></i> , we use <i class="emphasis">either</i> the fastest way through <i class="emphasis">S</i><sub>1</sub>, <sub><i class="emphasis">j</i> -1</sub> <i class="emphasis">or</i> the fastest way through <i class="emphasis">S</i><sub>2, <i class="emphasis">j</i> -1</sub>; whichever we use represents the one subproblem that we must optimally solve. Matrix-chain multiplication for the subchain <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A</i><sub><i class="emphasis">i</i>+1</sub> <i class="emphasis">A<sub>j</sub></i> serves as an example with two subproblems and <i class="emphasis">j</i> - <i class="emphasis">i</i> choices. For a given matrix <i class="emphasis">A<sub>k</sub></i> at which we split the product, we have two subproblems<span class="unicode">-</span>parenthesizing <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A<sub>i</sub></i><sub>+1</sub> <i class="emphasis">A<sub>k</sub></i> and parenthesizing <i class="emphasis">A</i><sub><i class="emphasis">k</i>+1</sub> <i class="emphasis">A</i><sub><i class="emphasis">k</i>+2</sub> <i class="emphasis">A<sub>j</sub></i><span class="unicode">-</span>and we must solve <i class="emphasis">both</i> of them optimally. Once we determine the optimal solutions to subproblems, we choose from among <i class="emphasis">j</i> - <i class="emphasis">i</i> candidates for the index <i class="emphasis">k</i>.</p>
<p class="para">Informally, the running time of a dynamic-programming algorithm depends on the product of two factors: the number of subproblems overall and how many choices we look at for each subproblem. In assembly-line scheduling, we had <span class="unicode">&Theta;</span>(<i class="emphasis">n</i>) subproblems overall, and only two choices to examine for each, yielding a <span class="unicode">&Theta;</span>(<i class="emphasis">n</i>) running time. For matrix-chain multiplication, there were <span class="unicode">&Theta;</span>(<i class="emphasis">n</i><sup>2</sup>) subproblems overall, and in each we had at most <i class="emphasis">n</i> - 1 choices, giving an <i class="emphasis">O</i>(<i class="emphasis">n</i><sup>3</sup>) running time.</p>
<p class="para">Dynamic programming uses optimal substructure in a bottom-up fashion. That is, we first find optimal solutions to subproblems and, having solved the subproblems, we find an optimal solution to the problem. Finding an optimal solution to the problem entails making a choice among subproblems as to which we will use in solving the problem. The cost of the problem solution is usually the subproblem costs plus a cost that is directly attributable to the choice itself. In assembly-line scheduling, for example, first we solved the subproblems of finding the fastest way through stations <i class="emphasis">S</i><sub>1, <i class="emphasis">j</i> -1</sub> and <i class="emphasis">S</i><sub>2, <i class="emphasis">j</i> -1</sub>, and then we chose one of these stations as the one preceding station <i class="emphasis">S<sub>i, j</sub></i>. The cost attributable to the choice itself depends on whether we switch lines between stations <i class="emphasis">j</i> - 1 and <i class="emphasis">j</i>; this cost is <i class="emphasis">a<sub>i, j</sub></i> if we stay on the same line, and it is <i class="emphasis">t<sub>i<i class="emphasis"><span class="unicode">&prime;</span></i>, j</sub></i><sub>-1</sub> + <i class="emphasis">a<sub>i,j</sub></i> , where <i class="emphasis">i</i><span class="unicode">&prime;</span> <span class="unicode">&ne;</span> <i class="emphasis">i</i>, if we switch. In matrix-chain multiplication, we determined optimal parenthesizations of subchains of <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A<sub>i</sub></i><sub>+1</sub> <i class="emphasis">A<sub>j</sub></i> , and then we chose the matrix <i class="emphasis">A<sub>k</sub></i> at which to split the product. The cost attributable to the choice itself is the term <i class="emphasis">p<sub>i</sub></i><sub>-1</sub> <i class="emphasis">p<sub>k</sub></i> <i class="emphasis">p<sub>j</sub></i>.</p>
<p class="para">In <a href="DDU0093.html#1143" target="_parent" class="chapterjump">Chapter 16</a>, we shall examine "greedy algorithms," which have many similarities to dynamic programming. In particular, problems to which greedy algorithms apply have optimal substructure. One salient difference between greedy algorithms and dynamic programming is that in greedy algorithms, we use optimal substructure in a top-down fashion. Instead of first finding optimal solutions to subproblems and then making a choice, greedy algorithms first make a choice<span class="unicode">-</span>the choice that looks best at the time<span class="unicode">-</span>and then solve a resulting subproblem.</p>
<div class="section">
<h5 class="sect5-title">Subtleties</h5>
<p class="first-para">One should be careful not to assume that optimal substructure applies when it does not. Consider the following two problems in which we are given a directed graph <i class="emphasis">G</i> = (<i class="emphasis">V, E</i>) and vertices <i class="emphasis">u</i>, <i class="emphasis">v</i> <span class="unicode">&isin;</span> <i class="emphasis">V</i>.<a name="1066"></a><a name="IDX-342"></a>
</p>
<ul class="simple-list">
<li class="first-listitem">
<p class="para">
<b>Unweighted shortest path:</b> <sup>[<a name="N659" href="#ftn.N659">2</a>]</sup> Find a path from <i class="emphasis">u</i> to <i class="emphasis">v</i> consisting of the fewest edges. Such a path must be simple, since removing a cycle from a path produces a path with fewer edges.</p>
</li>
<li class="listitem">
<p class="para">
<b>Unweighted longest simple path:</b> Find a simple path from <i class="emphasis">u</i> to <i class="emphasis">v</i> consisting of the most edges. We need to include the requirement of simplicity because otherwise we can traverse a cycle as many times as we like to create paths with an arbitrarily large number of edges.</p>
</li>
</ul>
<p class="para">The unweighted shortest-path problem exhibits optimal substructure, as follows. Suppose that <i class="emphasis">u</i> <span class="unicode">&ne;</span> <i class="emphasis">v</i>, so that the problem is nontrivial. Then any path <i class="emphasis">p</i> from <i class="emphasis">u</i> to <i class="emphasis">v</i> must contain an intermediate vertex, say <i class="emphasis">w</i>. (Note that <i class="emphasis">w</i> may be <i class="emphasis">u</i> or <i class="emphasis">v</i>.) Thus we can decompose the path <span class="inlinemediaobject"><img src="images/fig364_01.jpg" height="13" width="32" alt="" border="0"></span> into subpaths <span class="inlinemediaobject"><img src="images/fig364_02.jpg" height="13" width="59" alt="" border="0"></span>. Clearly, the number of edges in <i class="emphasis">p</i> is equal to the sum of the number of edges in <i class="emphasis">p</i><sub>1</sub> and the number of edges in <i class="emphasis">p</i><sub>2</sub>. We claim that if <i class="emphasis">p</i> is an optimal (i.e., shortest) path from <i class="emphasis">u</i> to <i class="emphasis">v</i>, then <i class="emphasis">p</i><sub>1</sub> must be a shortest path from <i class="emphasis">u</i> to <i class="emphasis">w</i>. Why? We use a "cut-and-paste" argument: if there were another path, say <span class="inlinemediaobject"><img src="images/fig364_03.jpg" height="13" width="12" alt="" border="0"></span>, from <i class="emphasis">u</i> to <i class="emphasis">w</i> with fewer edges than <i class="emphasis">p</i><sub>1</sub>, then we could cut out <i class="emphasis">p</i><sub>1</sub> and paste in <span class="inlinemediaobject"><img src="images/fig364_04.jpg" height="14" width="11" alt="" border="0"></span> to produce a path <span class="inlinemediaobject"><img src="images/fig364_05.jpg" height="16" width="57" alt="" border="0"></span> with fewer edges than <i class="emphasis">p</i>, thus contradicting <i class="emphasis">p</i>'s optimality. Symmetrically, <i class="emphasis">p</i><sub>2</sub> must be a shortest path from <i class="emphasis">w</i> to <i class="emphasis">v</i>. Thus, we can find a shortest path from <i class="emphasis">u</i> to <i class="emphasis">v</i> by considering all intermediate vertices <i class="emphasis">w</i>, finding a shortest path from <i class="emphasis">u</i> to <i class="emphasis">w</i> and a shortest path from <i class="emphasis">w</i> to <i class="emphasis">v</i>, and choosing an intermediate vertex <i class="emphasis">w</i> that yields the overall shortest path. In <a href="DDU0157.html#1909" target="_parent" class="chapterjump">Section 25.2</a>, we use a variant of this observation of optimal substructure to find a shortest path between every pair of vertices on a weighted, directed graph.</p>
<p class="para">It is tempting to assume that the problem of finding an unweighted longest simple path exhibits optimal substructure as well. After all, if we decompose a longest simple path <span class="inlinemediaobject"><img src="images/fig364_06.jpg" height="13" width="30" alt="" border="0"></span> into subpaths <span class="inlinemediaobject"><img src="images/fig364_07.jpg" height="13" width="55" alt="" border="0"></span>, then mustn't <i class="emphasis">p</i><sub>1</sub> be a longest simple path from <i class="emphasis">u</i> to <i class="emphasis">w</i>, and mustn't <i class="emphasis">p</i><sub>2</sub> be a longest simple path from <i class="emphasis">w</i> to <i class="emphasis">v</i>? The answer is no! <a class="internaljump" href="#ch15fig04">Figure 15.4</a> gives an example. Consider the path <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">r</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i>, which is a longest simple path from <i class="emphasis">q</i> to <i class="emphasis">t</i>. Is <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">r</i> a longest simple path from <i class="emphasis">q</i> to <i class="emphasis">r</i>? No, for the path <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">s</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i> <span class="unicode">&rarr;</span> <i class="emphasis">r</i> is a simple path that is longer. Is <i class="emphasis">r</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i> a longest simple path from <i class="emphasis">r</i> to <i class="emphasis">t</i>? No again, for the path <i class="emphasis">r</i> <span class="unicode">&rarr;</span> <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">s</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i> is a simple path that is longer.</p>
<div class="figure">
<a name="1067"></a><a name="ch15fig04"></a><span class="figuremediaobject"><img src="images/fig365_01.jpg" height="105" width="113" alt="" border="0"></span>
<br style="line-height: 1">
<span class="figure-title"><span class="figure-titlelabel">Figure 15.4: </span>A directed graph showing that the problem of finding a longest simple path in an unweighted directed graph does not have optimal substructure. The path <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">r</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i> is a longest simple path from <i class="emphasis">q</i> to <i class="emphasis">t</i>, but the subpath <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">r</i> is not a longest simple path from <i class="emphasis">q</i> to <i class="emphasis">r</i>, nor is the subpath <i class="emphasis">r</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i> a longest simple path from <i class="emphasis">r</i> to <i class="emphasis">t</i>.</span>
</div>
<p class="para">This example shows that for longest simple paths, not only is optimal substructure lacking, but we cannot necessarily assemble a "legal" solution to the problem from solutions to subproblems. If we combine the longest simple paths <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">s</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i> <span class="unicode">&rarr;</span> <i class="emphasis">r</i> and <i class="emphasis">r</i> <span class="unicode">&rarr;</span> <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">s</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i>, we get the path <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">s</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i> <span class="unicode">&rarr;</span> <i class="emphasis">r</i> <span class="unicode">&rarr;</span> <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">s</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i>, <a name="1068"></a><a name="IDX-343"></a>which is not simple. Indeed, the problem of finding an unweighted longest simple path does not appear to have any sort of optimal substructure. No efficient dynamic-programming algorithm for this problem has ever been found. In fact, this problem is NP-complete, which<span class="unicode">-</span>as we shall see in <a href="DDU0223.html#3001" target="_parent" class="chapterjump">Chapter 34</a><span class="unicode">-</span>means that it is unlikely that it can be solved in polynomial time.</p>
<p class="para">What is it about the substructure of a longest simple path that is so different from that of a shortest path? Although two subproblems are used in a solution to a problem for both longest and shortest paths, the subproblems in finding the longest simple path are not <b class="bold"><i class="emphasis">independent</i></b>, whereas for shortest paths they are. What do we mean by subproblems being independent? We mean that the solution to one subproblem does not affect the solution to another subproblem of the same problem. For the example of <a class="internaljump" href="#ch15fig04">Figure 15.4</a>, we have the problem of finding a longest simple path from <i class="emphasis">q</i> to <i class="emphasis">t</i> with two subproblems: finding longest simple paths from <i class="emphasis">q</i> to <i class="emphasis">r</i> and from <i class="emphasis">r</i> to <i class="emphasis">t</i>. For the first of these subproblems, we choose the path <i class="emphasis">q</i> <span class="unicode">&rarr;</span> <i class="emphasis">s</i> <span class="unicode">&rarr;</span> <i class="emphasis">t</i> <span class="unicode">&rarr;</span> <i class="emphasis">r</i>, and so we have also used the vertices <i class="emphasis">s</i> and <i class="emphasis">t</i>. We can no longer use these vertices in the second subproblem, since the combination of the two solutions to subproblems would yield a path that is not simple. If we cannot use vertex <i class="emphasis">t</i> in the second problem, then we cannot solve it all, since <i class="emphasis">t</i> is required to be on the path that we find, and it is not the vertex at which we are "splicing" together the subproblem solutions (that vertex being <i class="emphasis">r</i>). Our use of vertices <i class="emphasis">s</i> and <i class="emphasis">t</i> in one subproblem solution prevents them from being used in the other subproblem solution. We must use at least one of them to solve the other subproblem, however, and we must use both of them to solve it optimally. Thus we say that these subproblems are not independent. Looked at another way, our use of resources in solving one subproblem (those resources being vertices) has rendered them unavailable for the other subproblem.</p>
<p class="para">Why, then, are the subproblems independent for finding a shortest path? The answer is that by nature, the subproblems do not share resources. We claim that if a vertex <i class="emphasis">w</i> is on a shortest path <i class="emphasis">p</i> from <i class="emphasis">u</i> to <i class="emphasis">v</i>, then we can splice together <i class="emphasis">any</i> shortest path <span class="inlinemediaobject"><img src="images/fig365_02.jpg" height="12" width="32" alt="" border="0"></span> and <i class="emphasis">any</i> shortest path <span class="inlinemediaobject"><img src="images/fig365_03.jpg" height="12" width="32" alt="" border="0"></span> to produce a shortest path from <i class="emphasis">u</i> to <i class="emphasis">v</i>. We are assured that, other than <i class="emphasis">w</i>, no vertex can appear in both paths <i class="emphasis">p</i><sub>1</sub> <a name="1069"></a><a name="IDX-344"></a>and <i class="emphasis">p</i><sub>2</sub>. Why? Suppose that some vertex <i class="emphasis">x</i> <span class="unicode">&ne;</span> <i class="emphasis">w</i> appears in both <i class="emphasis">p</i><sub>1</sub> and <i class="emphasis">p</i><sub>2</sub>, so that we can decompose <i class="emphasis">p</i><sub>1</sub> as <span class="inlinemediaobject"><img src="images/fig366_01.jpg" height="13" width="61" alt="" border="0"></span> and <i class="emphasis">p</i><sub>2</sub> as <span class="inlinemediaobject"><img src="images/fig366_02.jpg" height="13" width="60" alt="" border="0"></span>. By the optimal substructure of this problem, path <i class="emphasis">p</i> has as many edges as <i class="emphasis">p</i><sub>1</sub> and <i class="emphasis">p</i><sub>2</sub> together; let's say that <i class="emphasis">p</i> has <i class="emphasis">e</i> edges. Now let us construct a path <span class="inlinemediaobject"><img src="images/fig366_03.jpg" height="12" width="56" alt="" border="0"></span> from <i class="emphasis">u</i> to <i class="emphasis">v</i>. This path has at most <i class="emphasis">e</i> - 2 edges, which contradicts the assumption that <i class="emphasis">p</i> is a shortest path. Thus, we are assured that the subproblems for the shortest-path problem are independent.</p>
<p class="last-para">Both problems examined in <a href="DDU0087.html#1011" target="_parent" class="chapterjump">Sections 15.1</a> and <a href="DDU0088.html#1039" target="_parent" class="chapterjump">15.2</a> have independent subproblems. In matrix-chain multiplication, the subproblems are multiplying subchains <i class="emphasis">A<sub>i</sub></i><i class="emphasis">A</i><sub><i class="emphasis">i</i>+1</sub> <i class="emphasis">A<sub>k</sub></i> and <i class="emphasis">A</i><sub><i class="emphasis">k</i>+1</sub><i class="emphasis">A</i><sub><i class="emphasis">k</i>+2</sub> <i class="emphasis">A<sub>j</sub></i>. These subchains are disjoint, so that no matrix could possibly be included in both of them. In assembly-line scheduling, to determine the fastest way through station <i class="emphasis">S<sub>i,j</sub></i>, we look at the fastest ways through stations <i class="emphasis">S</i><sub>1,<i class="emphasis">j</i>-1</sub> and <i class="emphasis">S</i><sub>2,<i class="emphasis">j</i>-1</sub>. Because our solution to the fastest way through station <i class="emphasis">Si, j</i> will include just one of these subproblem solutions, that subproblem is automatically independent of all others used in the solution.</p>

</div>

</div>
<div class="section">
<h4 class="sect4-title">
<a name="1070"></a><a name="ch15lev3sec11"></a>Overlapping subproblems</h4>
<p class="first-para">The second ingredient that an optimization problem must have for dynamic programming to be applicable is that the space of subproblems must be "small" in the sense that a recursive algorithm for the problem solves the same subproblems over and over, rather than always generating new subproblems. Typically, the total number of distinct subproblems is a polynomial in the input size. When a recursive algorithm revisits the same problem over and over again, we say that the optimization problem has <b class="bold"><i class="emphasis">overlapping subproblems</i></b>.<sup>[<a name="N1529" href="#ftn.N1529">3</a>]</sup> In contrast, a problem for which a divide-and-conquer approach is suitable usually generates brand-new problems at each step of the recursion. Dynamic-programming algorithms typically take advantage of overlapping subproblems by solving each subproblem once and then storing the solution in a table where it can be looked up when needed, using constant time per lookup.</p>
<p class="para">In <a href="DDU0087.html#1011" target="_parent" class="chapterjump">Section 15.1</a>, we briefly examined how a recursive solution to assembly-line scheduling makes 2<sup><i class="emphasis">n</i>-<i class="emphasis">j</i></sup> references to <i class="emphasis">f<sub>i</sub></i>[<i class="emphasis">j</i>] for <i class="emphasis">j</i> = 1, 2, ..., <i class="emphasis">n</i>. Our tabular solution takes an exponential-time recursive algorithm down to linear time.</p>
<p class="para">To illustrate the overlapping-subproblems property in greater detail, let us reexamine the matrix-chain multiplication problem. Referring back to <a href="DDU0088.html#1052" target="_parent" class="chapterjump">Figure 15.3</a>, <a name="1071"></a><a name="IDX-345"></a>observe that MATRIX-CHAIN-ORDER repeatedly looks up the solution to subproblems in lower rows when solving subproblems in higher rows. For example, entry <i class="emphasis">m</i>[3, 4] is referenced 4 times: during the computations of <i class="emphasis">m</i>[2, 4], <i class="emphasis">m</i>[1, 4], <i class="emphasis">m</i>[3, 5], and <i class="emphasis">m</i>[3, 6]. If <i class="emphasis">m</i>[3, 4] were recomputed each time, rather than just being looked up, the increase in running time would be dramatic. To see this, consider the following (inefficient) recursive procedure that determines <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>], the minimum number of scalar multiplications needed to compute the matrix-chain product <i class="emphasis">A</i><sub><i class="emphasis">i</i><i class="emphasis"><span class="unicode">&#8229;</span><i class="emphasis">j</i></i></sub> = <i class="emphasis">A<sub>i</sub></i><i class="emphasis">A</i><sub><i class="emphasis">i</i>+1</sub> <span class="unicode">&middot;</span><span class="unicode">&middot;</span><span class="unicode">&middot;</span> <i class="emphasis">A<sub>j</sub></i>. The procedure is based directly on the recurrence (<a href="DDU0088.html#1049" target="_parent" class="chapterjump">15.12</a>).</p>
<div class="informalexample">
<pre class="literallayout-normal">
RECURSIVE-MATRIX-CHAIN(<i class="emphasis">p</i>, <i class="emphasis">i</i>, <i class="emphasis">j</i>)
1 <b class="bold">if</b> <i class="emphasis">i</i> = <i class="emphasis">j</i>
2     <b class="bold">then return</b> 0
3 <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] <span class="unicode">&larr;</span> <span class="unicode">&infin;</span>
4 <b class="bold">for</b> <i class="emphasis">k</i> <span class="unicode">&larr;</span> <i class="emphasis">i</i> <b class="bold">to</b> <i class="emphasis">j</i> - 1
5      <b class="bold">do</b> <i class="emphasis">q</i> <span class="unicode">&larr;</span> RECURSIVE-MATRIX-CHAIN(<i class="emphasis">p</i>, <i class="emphasis">i</i>, <i class="emphasis">k</i>)
                 + RECURSIVE-MATRIX-CHAIN(<i class="emphasis">p</i>,<i class="emphasis">k</i> + 1, <i class="emphasis">j</i>)
                 + <i class="emphasis">p</i><sub><i class="emphasis">i</i>-1</sub> <i class="emphasis">p<sub>k</sub></i> <i class="emphasis">p<sub>j</sub></i>
6         <b class="bold">if</b> <i class="emphasis">q</i> <span class="unicode">&lt;</span> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>]
7            <b class="bold">then</b> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] <span class="unicode">&larr;</span> <i class="emphasis">q</i>
8 <b class="bold">return</b> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>]
</pre>
</div>
<p class="para">
<a class="internaljump" href="#ch15fig05">Figure 15.5</a> shows the recursion tree produced by the call RECURSIVE-MATRIX-CHAIN(<i class="emphasis">p</i>, 1, 4). Each node is labeled by the values of the parameters <i class="emphasis">i</i> and <i class="emphasis">j</i>. Observe that some pairs of values occur many times.</p>
<div class="figure">
<a name="1072"></a><a name="ch15fig05"></a><span class="figuremediaobject"><a href="images/fig367%5F01%5F0%2Ejpg" NAME="IMG_470" target="_parent"><img src="images/fig367_01.jpg" height="141" width="350" alt="Click To expand" border="0"></a></span>
<br style="line-height: 1">
<span class="figure-title"><span class="figure-titlelabel">Figure 15.5: </span>The recursion tree for the computation of RECURSIVE-MATRIX-CHAIN(<i class="emphasis">p</i>, 1, 4). Each node contains the parameters <i class="emphasis">i</i> and <i class="emphasis">j</i>. The computations performed in a shaded subtree are replaced by a single table lookup in MEMOIZED-MATRIX-CHAIN(<i class="emphasis">p</i>, 1, 4).</span>
</div>
<p class="para">In fact, we can show that the time to compute <i class="emphasis">m</i>[1, <i class="emphasis">n</i>] by this recursive procedure is at least exponential in <i class="emphasis">n</i>. Let <i class="emphasis">T</i> (<i class="emphasis">n</i>) denote the time taken by RECURSIVE-<a name="1073"></a><a name="IDX-346"></a>MATRIX-CHAIN to compute an optimal parenthesization of a chain of <i class="emphasis">n</i> matrices. If we assume that the execution of lines 1<span class="unicode">-</span>2 and of lines 6<span class="unicode">-</span>7 each take at least unit time, then inspection of the procedure yields the recurrence</p>
<div class="informalequation">
<span class="equation-image"><img src="images/fig368_01.jpg" height="62" width="343" alt="" border="0"></span>
</div>
<p class="para">Noting that for <i class="emphasis">i</i> = 1, 2, ..., <i class="emphasis">n</i> - 1, each term <i class="emphasis">T</i> <i class="emphasis">(i)</i> appears once as <i class="emphasis">T</i> <i class="emphasis">(k)</i> and once as <i class="emphasis">T</i> (<i class="emphasis">n</i> - <i class="emphasis">k</i>), and collecting the <i class="emphasis">n</i> - 1 1's in the summation together with the 1 out front, we can rewrite the recurrence as</p>
<div class="equation">
<table border="0" cellspacing="0" cellpadding="0">
<tr>
<td valign="top"><span class="equation-label">(15.13)&nbsp;</span></td><td valign="top"><span class="equation-image"><img src="images/fig368_02.jpg" height="43" width="142" alt="" border="0"></span></td>
</tr>
</table>
</div>
<p class="para">We shall prove that <i class="emphasis">T</i> <i class="emphasis">(n)</i> = <span class="unicode">&#8486;</span>(2<sup><i class="emphasis">n</i></sup>) using the substitution method. Specifically, we shall show that <i class="emphasis">T</i> <i class="emphasis">(n)</i> <span class="unicode">&ge;</span> 2<sup><i class="emphasis">n</i>-1</sup> for all <i class="emphasis">n</i> <span class="unicode">&ge;</span> 1. The basis is easy, since <i class="emphasis">T</i> (<i class="emphasis">1</i>) <span class="unicode">&ge;</span> 1 = 2<sup>0</sup>. Inductively, for <i class="emphasis">n</i> <span class="unicode">&ge;</span> 2 we have</p>
<div class="informalequation">
<span class="equation-image"><img src="images/fig368_03.jpg" height="155" width="163" alt="" border="0"></span>
</div>
<p class="para">which completes the proof. Thus, the total amount of work performed by the call RECURSIVE-MATRIX-CHAIN(<i class="emphasis">p</i>, 1, <i class="emphasis">n</i>) is at least exponential in <i class="emphasis">n</i>.</p>
<p class="last-para">Compare this top-down, recursive algorithm with the bottom-up, dynamic-programming algorithm. The latter is more efficient because it takes advantage of the overlapping-subproblems property. There are only <span class="unicode">&Theta;</span>(<i class="emphasis">n</i><sup>2</sup>) different subproblems, and the dynamic-programming algorithm solves each exactly once. The recursive algorithm, on the other hand, must repeatedly resolve each subproblem each time it reappears in the recursion tree. Whenever a recursion tree for the natural recursive solution to a problem contains the same subproblem repeatedly, and the total number of different subproblems is small, it is a good idea to see if dynamic programming can be made to work.</p>

</div>
<div class="section">
<h4 class="sect4-title">
<a name="1074"></a><a name="ch15lev3sec12"></a>Reconstructing an optimal solution</h4>
<p class="first-para">As a practical matter, we often store which choice we made in each subproblem in a table so that we do not have to reconstruct this information from the costs <a name="1075"></a><a name="IDX-347"></a>that we stored. In assembly-line scheduling, we stored in <i class="emphasis">l<sub>i</sub></i> [<i class="emphasis">j</i>] the station preceding <i class="emphasis">S<sub>i,j</sub></i> in a fastest way through <i class="emphasis">S<sub>i,j</sub></i> . Alternatively, having filled in the entire <i class="emphasis">f<sub>i</sub></i>[<i class="emphasis">j</i>] table, we could determine which station precedes <i class="emphasis">S</i><sub>1,<i class="emphasis">j</i></sub> in a fastest way through <i class="emphasis">S<sub>i,j</sub></i> with a little extra work. If <i class="emphasis">f</i><sub>1</sub>[<i class="emphasis">j</i>] = <i class="emphasis">f</i><sub>1</sub>[<i class="emphasis">j</i> - 1] + <i class="emphasis">a</i><sub>1, <i class="emphasis">j</i></sub>, then station <i class="emphasis">S</i><sub>1, <i class="emphasis">j</i> -1</sub> precedes <i class="emphasis">S</i><sub>1, <i class="emphasis">j</i></sub> in a fastest way through <i class="emphasis">S</i><sub>1, <i class="emphasis">j</i></sub>. Otherwise, it must be the case that <i class="emphasis">f</i><sub>1</sub>[<i class="emphasis">j</i>] = <i class="emphasis">f</i><sub>2</sub>[<i class="emphasis">j</i> - 1] + <i class="emphasis">t</i><sub>2, <i class="emphasis">j</i> -1</sub> + <i class="emphasis">a</i><sub>1, <i class="emphasis">j</i></sub>, and so <i class="emphasis">S</i><sub>2, <i class="emphasis">j</i> - 1</sub> precedes <i class="emphasis">S</i><sub>1, <i class="emphasis">j</i></sub>. For assembly-line scheduling, reconstructing the predecessor stations takes only <i class="emphasis">O</i>(1) time per station, even without the <i class="emphasis">l<sub>i</sub></i>[<i class="emphasis">j</i>] table.</p>
<p class="last-para">For matrix-chain multiplication, however, the table <i class="emphasis">s</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] saves us a significant amount of work when reconstructing an optimal solution. Suppose that we did not maintain the <i class="emphasis">s</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] table, having filled in only the table <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] containing optimal subproblem costs. There are <i class="emphasis">j</i> - <i class="emphasis">i</i> choices in determining which subproblems to use in an optimal solution to parenthesizing <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A<sub>i</sub></i><sub>+1</sub> <i class="emphasis">A<sub>j</sub></i>, and <i class="emphasis">j</i> - <i class="emphasis">i</i> is not a constant. Therefore, it would take <span class="unicode">&Theta;</span>(<i class="emphasis">j</i> - <i class="emphasis">i</i>) = <i class="emphasis"><span class="unicode">&omega;</span></i>(1) time to reconstruct which subproblems we chose for a solution to a given problem. By storing in <i class="emphasis">s</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] the index of the matrix at which we split the product <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A</i><sub><i class="emphasis">i</i>+1</sub> <i class="emphasis">A<sub>j</sub></i>, we can reconstruct each choice in <i class="emphasis">O</i>(1) time.</p>

</div>
<div class="section">
<h4 class="sect4-title">
<a name="1076"></a><a name="ch15lev3sec13"></a>Memoization</h4>
<p class="first-para">There is a variation of dynamic programming that often offers the efficiency of the usual dynamic-programming approach while maintaining a top-down strategy. The idea is to <b class="bold"><i class="emphasis">memoize</i></b> the natural, but inefficient, recursive algorithm. As in ordinary dynamic programming, we maintain a table with subproblem solutions, but the control structure for filling in the table is more like the recursive algorithm.</p>
<p class="para">A memoized recursive algorithm maintains an entry in a table for the solution to each subproblem. Each table entry initially contains a special value to indicate that the entry has yet to be filled in. When the subproblem is first encountered during the execution of the recursive algorithm, its solution is computed and then stored in the table. Each subsequent time that the subproblem is encountered, the value stored in the table is simply looked up and returned.<sup>[<a name="N2318" href="#ftn.N2318">4</a>]</sup>
</p>
<p class="para">Here is a memoized version of RECURSIVE-MATRIX-CHAIN:<a name="1077"></a><a name="IDX-348"></a>
</p>
<div class="informalexample">
<pre class="literallayout-normal">
MEMOIZED-MATRIX-CHAIN(<i class="emphasis">p</i>)
1 <i class="emphasis">n</i> <span class="unicode">&larr;</span> <i class="emphasis">length</i>[<i class="emphasis">p</i>] - 1
2 <b class="bold">for</b> <i class="emphasis">i</i> <span class="unicode">&larr;</span> 1 <b class="bold">to</b> <i class="emphasis">n</i>
3      <b class="bold">do for</b> <i class="emphasis">j</i> <span class="unicode">&larr;</span> <i class="emphasis">i</i> <b class="bold">to</b> <i class="emphasis">n</i>
4             <b class="bold">do</b> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] <span class="unicode">&larr;</span> <span class="unicode">&infin;</span>
5 <b class="bold">return</b> LOOKUP-CHAIN(<i class="emphasis">p</i>, 1, <i class="emphasis">n</i>)
</pre>
</div>
<div class="informalexample">
<pre class="literallayout-normal">
LOOKUP-CHAIN(<i class="emphasis">p</i>, <i class="emphasis">i</i>, <i class="emphasis">j</i>)
1 <b class="bold">if</b> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] <span class="unicode">&lt;</span> <span class="unicode">&infin;</span>
2    <b class="bold">then return</b> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>]
3 <b class="bold">if</b> <i class="emphasis">i</i> = <i class="emphasis">j</i>
4     <b class="bold">then</b> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] <span class="unicode">&larr;</span> 0
5     <b class="bold">else for</b> <i class="emphasis">k</i> <span class="unicode">&larr;</span> <i class="emphasis">i</i> <b class="bold">to</b> <i class="emphasis">j</i> - 1
6             <b class="bold">do</b> <i class="emphasis">q</i> <span class="unicode">&larr;</span> LOOKUP-CHAIN(<i class="emphasis">p</i>, <i class="emphasis">i</i>, <i class="emphasis">k</i>)
                        + LOOKUP-CHAIN(<i class="emphasis">p</i>,<i class="emphasis">k</i> + 1, <i class="emphasis">j</i>) + <i class="emphasis">p</i><sub><i class="emphasis">i</i>-1</sub> <i class="emphasis">p<sub>k</sub></i> <i class="emphasis">p<sub>j</sub></i>
7                <b class="bold">if</b> <i class="emphasis">q</i> <span class="unicode">&lt;</span> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>]
8                   <b class="bold">then</b> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] <span class="unicode">&larr;</span> <i class="emphasis">q</i>
9 <b class="bold">return</b> <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>]
</pre>
</div>
<p class="para">MEMOIZED-MATRIX-CHAIN, like MATRIX-CHAIN-ORDER, maintains a table <i class="emphasis">m</i>[1 <span class="unicode">&#8229;</span> <i class="emphasis">n</i>, 1 <span class="unicode">&#8229;</span> <i class="emphasis">n</i>] of computed values of <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>], the minimum number of scalar multiplications needed to compute the matrix <i class="emphasis">A</i><sub><i class="emphasis">i</i><i class="emphasis"><span class="unicode">&#8229;</span></i><i class="emphasis">j</i></sub>. Each table entry initially contains the value <span class="unicode">&infin;</span> to indicate that the entry has yet to be filled in. When the call LOOKUP-CHAIN(<i class="emphasis">p</i>, <i class="emphasis">i</i>, <i class="emphasis">j</i>) is executed, if <i class="emphasis">m</i>[<i class="emphasis">i,</i> <i class="emphasis">j</i>] <span class="unicode">&lt;</span> <span class="unicode">&infin;</span> in line 1, the procedure simply returns the previously computed cost <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] (line 2). Otherwise, the cost is computed as in RECURSIVE-MATRIX-CHAIN, stored in <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>], and returned. (The value <span class="unicode">&infin;</span> is convenient to use for an unfilled table entry since it is the value used to initialize <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] in line 3 of RECURSIVE-MATRIX-CHAIN.) Thus, LOOKUP-CHAIN(<i class="emphasis">p</i>, <i class="emphasis">i</i>, <i class="emphasis">j</i>) always returns the value of <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>], but it computes it only if this is the first time that LOOKUP-CHAIN has been called with the parameters <i class="emphasis">i</i> and <i class="emphasis">j</i>.</p>
<p class="para">
<a class="internaljump" href="#ch15fig05">Figure 15.5</a> illustrates how MEMOIZED-MATRIX-CHAIN saves time compared to RECURSIVE-MATRIX-CHAIN. Shaded subtrees represent values that are looked up rather than computed.</p>
<p class="para">Like the dynamic-programming algorithm MATRIX-CHAIN-ORDER, the procedure MEMOIZED-MATRIX-CHAIN runs in <i class="emphasis">O</i>(<i class="emphasis">n</i><sup>3</sup>) time. Each of <span class="unicode">&Theta;</span>(<i class="emphasis">n</i><sup>2</sup>) table entries is initialized once in line 4 of MEMOIZED-MATRIX-CHAIN. We can categorize the calls of LOOKUP-CHAIN into two types:</p>
<ol class="orderedlist">
<li class="first-listitem">
<p class="first-para">calls in which <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] = <span class="unicode">&infin;</span>, so that lines 3<span class="unicode">-</span>9 are executed, and</p>
</li>
<li class="listitem">
<p class="first-para">calls in which <i class="emphasis">m</i>[<i class="emphasis">i</i>, <i class="emphasis">j</i>] <span class="unicode">&lt;</span> <span class="unicode">&infin;</span>, so that LOOKUP-CHAIN simply returns in line 2.</p>
</li>
</ol>
<a name="1078"></a><a name="IDX-349"></a>
<p class="para">There are <span class="unicode">&Theta;</span>(<i class="emphasis">n</i><sup>2</sup>) calls of the first type, one per table entry. All calls of the second type are made as recursive calls by calls of the first type. Whenever a given call of LOOKUP-CHAIN makes recursive calls, it makes <i class="emphasis">O</i>(<i class="emphasis">n</i>) of them. Therefore, there are <i class="emphasis">O</i>(<i class="emphasis">n</i><sup>3</sup>) calls of the second type in all. Each call of the second type takes <i class="emphasis">O</i>(1) time, and each call of the first type takes <i class="emphasis">O</i>(<i class="emphasis">n</i>) time plus the time spent in its recursive calls. The total time, therefore, is <i class="emphasis">O</i>(<i class="emphasis">n</i><sup>3</sup>). Memoization thus turns an <span class="unicode">&#8486;</span>(2<sup><i class="emphasis">n</i></sup>)-time algorithm into an <i class="emphasis">O</i>(<i class="emphasis">n</i><sup>3</sup>)-time algorithm.</p>
<p class="para">In summary, the matrix-chain multiplication problem can be solved by either a top-down, memoized algorithm or a bottom-up, dynamic-programming algorithm in <i class="emphasis">O</i>(<i class="emphasis">n</i><sup>3</sup>) time. Both methods take advantage of the overlapping-subproblems property. There are only <span class="unicode">&Theta;</span>(<i class="emphasis">n</i><sup>2</sup>) different subproblems in total, and either of these methods computes the solution to each subproblem once. Without memoization, the natural recursive algorithm runs in exponential time, since solved subproblems are repeatedly solved.</p>
<p class="para">In general practice, if all subproblems must be solved at least once, a bottom-up dynamic-programming algorithm usually outperforms a top-down memoized algorithm by a constant factor, because there is no overhead for recursion and less overhead for maintaining the table. Moreover, there are some problems for which the regular pattern of table accesses in the dynamic-programming algorithm can be exploited to reduce time or space requirements even further. Alternatively, if some subproblems in the subproblem space need not be solved at all, the memoized solution has the advantage of solving only those subproblems that are definitely required.</p>
<div class="example">
<span class="example-title"><span class="example-titlelabel">Exercises 15.3-1</span></span><a name="1079"></a><a name="ch15ex11"></a>
<div class="formalbody">
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="Start example" border="0"></b></font></td>
</tr>
</table>
<p class="first-para">Which is a more efficient way to determine the optimal number of multiplications in a matrix-chain multiplication problem: enumerating all the ways of parenthesizing the product and computing the number of multiplications for each, or running RECURSIVE-MATRIX-CHAIN? Justify your answer.</p>
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="End example" border="0"></b></font></td>
</tr>
</table>
<table class="BlankSpace" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td height="16"></td>
</tr>
</table>
</div>
</div>
<div class="example">
<span class="example-title"><span class="example-titlelabel">Exercises 15.3-2</span></span><a name="1080"></a><a name="ch15ex12"></a>
<div class="formalbody">
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="Start example" border="0"></b></font></td>
</tr>
</table>
<p class="first-para">Draw the recursion tree for the MERGE-SORT procedure from <a href="DDU0016.html#90" target="_parent" class="chapterjump">Section 2.3.1</a> on an array of 16 elements. Explain why memoization is ineffective in speeding up a good divide-and-conquer algorithm such as MERGE-SORT.</p>
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="End example" border="0"></b></font></td>
</tr>
</table>
<table class="BlankSpace" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td height="16"></td>
</tr>
</table>
</div>
</div>
<div class="example">
<span class="example-title"><span class="example-titlelabel">Exercises 15.3-3</span></span><a name="1081"></a><a name="ch15ex13"></a>
<div class="formalbody">
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="Start example" border="0"></b></font></td>
</tr>
</table>
<p class="first-para">Consider a variant of the matrix-chain multiplication problem in which the goal is to parenthesize the sequence of matrices so as to maximize, rather than minimize, the number of scalar multiplications. Does this problem exhibit optimal substructure?</p>
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="End example" border="0"></b></font></td>
</tr>
</table>
<table class="BlankSpace" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td height="16"></td>
</tr>
</table>
</div>
</div>
<a name="1082"></a><a name="IDX-350"></a>
<div class="example">
<span class="example-title"><span class="example-titlelabel">Exercises 15.3-4</span></span><a name="1083"></a><a name="ch15ex14"></a>
<div class="formalbody">
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="Start example" border="0"></b></font></td>
</tr>
</table>
<p class="first-para">Describe how assembly-line scheduling has overlapping subproblems.</p>
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="End example" border="0"></b></font></td>
</tr>
</table>
<table class="BlankSpace" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td height="16"></td>
</tr>
</table>
</div>
</div>
<div class="example">
<span class="example-title"><span class="example-titlelabel">Exercises 15.3-5</span></span><a name="1084"></a><a name="ch15ex15"></a>
<div class="formalbody">
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="Start example" border="0"></b></font></td>
</tr>
</table>
<p class="first-para">As stated, in dynamic programming we first solve the subproblems and then choose which of them to use in an optimal solution to the problem. Professor Capulet claims that it is not always necessary to solve all the subproblems in order to find an optimal solution. She suggests that an optimal solution to the matrix-chain multiplication problem can be found by always choosing the matrix <i class="emphasis">A<sub>k</sub></i> at which to split the subproduct <i class="emphasis">A<sub>i</sub></i> <i class="emphasis">A</i><sub><i class="emphasis">i</i>+1</sub> <i class="emphasis">A<sub>j</sub></i> (by selecting <i class="emphasis">k</i> to minimize the quantity <i class="emphasis">p</i><sub><i class="emphasis">i</i>-1</sub> <i class="emphasis">p<sub>k</sub></i> <i class="emphasis">p<sub>j</sub></i>) <i class="emphasis">before</i> solving the subproblems. Find an instance of the matrix-chain multiplication problem for which this greedy approach yields a suboptimal solution.</p>
<table class="BlueLine" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td bgcolor="000080" class="bluecell"><font size="2" face="Arial" color="010100"><b><img src="_.gif" width="1" height="2" alt="End example" border="0"></b></font></td>
</tr>
</table>
<table class="BlankSpace" border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td height="16"></td>
</tr>
</table>
</div>
</div>

</div>

</div>
<div class="footnotes">
<div class="footnote">
<p>
<sup>[<a name="ftn.N18" href="#N18">1</a>]</sup>This is not a misspelling. The word really is <i class="emphasis">memoization</i>, not <i class="emphasis">memorization</i>. <i class="emphasis">Memoization</i> comes from <i class="emphasis">memo</i>, since the technique consists of recording a value so that we can look it up later.</p>
</div>
<div class="footnote">
<p>
<sup>[<a name="ftn.N659" href="#N659">2</a>]</sup>We use the term "unweighted" to distinguish this problem from that of finding shortest paths with weighted edges, which we shall see in <a href="DDU0139.html#1748" target="_parent" class="chapterjump">Chapters 24</a> and <a href="DDU0154.html#1876" target="_parent" class="chapterjump">25</a>. We can use the breadth-first search technique of <a href="DDU0129.html#1579" target="_parent" class="chapterjump">Chapter 22</a> to solve the unweighted problem.</p>
</div>
<div class="footnote">
<p>
<sup>[<a name="ftn.N1529" href="#N1529">3</a>]</sup>It may seem strange that dynamic programming relies on subproblems being both independent and overlapping. Although these requirements may sound contradictory, they describe two different notions, rather than two points on the same axis. Two subproblems of the same problem are independent if they do not share resources. Two subproblems are overlapping if they are really the same subproblem that occurs as a subproblem of different problems.</p>
</div>
<div class="footnote">
<p>
<sup>[<a name="ftn.N2318" href="#N2318">4</a>]</sup>This approach presupposes that the set of all possible subproblem parameters is known and that the relation between table positions and subproblems is established. Another approach is to memoize by using hashing with the subproblem parameters as keys.</p>
</div>
</div>
</div>
</div>
</div>
<br><hr size="1">
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#e6e6e6">
<tr style="background-image: url(images/tile_back.gif);">
<td align="left" width="30%">
<div STYLE="MARGIN-LEFT: 0.15in;">
<a href="DDU0088.html"><img src="images/previous.gif" width="70" height="19" border="0" align="absmiddle" alt="Previous Section"></a>
</div></td>
<td align="center" width="40%">
<a href="toc.html" style="color:white;text-decoration:none;text-underline:none">&nbsp;&lt;&nbsp;Day Day Up&nbsp;&gt;&nbsp;</a>
</td>
<td align="right" width="30%">
<div STYLE="MARGIN-RIGHT: 0.15in"><a href="DDU0090.html"><img src="images/next.gif" width="70" height="19" border="0" align="absmiddle" alt="Next Section"></a>
</div></td></tr>
</table>
</body>
</html>
