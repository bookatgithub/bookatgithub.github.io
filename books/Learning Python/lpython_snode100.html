<HTML><HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<!--SafClassName="docSection1Title"--><!--SafTocEntry="9.4 Internet-Related Activities"-->
<LINK REL="stylesheet" href="FILES/sbolM.css">
</HEAD>
<BODY link="#354278" vlink="#000066" alink="#000080" topmargin="0">
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#CCCCCC">
<td><font color="black" size=1>I l<font color="#FF0000">@</font>ve RuBoard</td>
<td valign="top" class="v2" align="right"><a href="lpython_snode99.html"><img src="FILES/previous.gif" width="62" height="15" border="0" align="absmiddle" alt="Previous Section"></a> <a href="lpython_snode101.html"><img src="FILES/next.gif" width="41" height="15" border="0" align="absmiddle" alt="Next Section"></a></td></table>
<FONT>
<h3>9.4
Internet-Related Activities</h3>


<p>The Internet is a treasure trove of information, but its exponential
growth can make it hard to manage. Furthermore, most tools currently
available for "surfing the Web" are not programmable.
Many web-related tasks can be automated quite simply with the tools
in the standard Python distribution.</p>




<H4>9.4.1
Downloading a Web Page Programmatically</H4>


<P>If you're <A name="idx788"></A>
<A NAme="idx789"></a> <a NAME="idx790"></a>interested in finding out what the weather
in a given location is over a period of months, it's much
easier to set up an automated program to get the information and
collect it in a file than to have to remember to do it by hand.</p>



<p>Here is a program that finds the weather in a couple of cities and
states using the pages of the <i>weather.com</i> web
site:</p>



<pre class="monofont">import urllib, urlparse, string, time

def get_temperature(country, state, city):
    url = urlparse.urljoin('http://www.weather.com/weather/cities/',
                           string.lower(country)+'_' + \
                           string.lower(state) + '_' + \
                           string.replace(string.lower(city), ' ',
                                          '_') + '.html')
    data = urllib.urlopen(url).read()
    start = string.index(data, 'current temp: ') + len('current temp: ')
    stop = string.index(data, '&amp;deg;F', start-1)
    temp = int(data[start:stop])
    localtime = time.asctime(time.localtime(time.time()))
    print ("On %(localtime)s, the temperature in %(city)s, " +\
           "%(state)s %(country)s is %(temp)s F.") % vars()

get_temperature('FR', '', 'Paris')
get_temperature('US', 'RI', 'Providence')
get_temperature('US', 'CA', 'San Francisco')</pre>


<p>When run, it produces output like:</p>



<pRe ClasS="monofont">~/book:&gt; <b>python get_temperature.py</B>
On Wed Nov 25 16:22:25 1998, the temperature in Paris,  FR is 39 F.
On Wed Nov 25 16:22:30 1998, the temperature in Providence, RI US is 39 F.
On Wed Nov 25 16:22:35 1998, the temperature in San Francisco, CA US is 58 F.</PrE>


<P>The code in <i>get_temperature.py</I> suffers from one
flaw, which is that the logic of the URL creation and of the
temperature extraction is dependent on the specific
<a NAME="idx791"></a>HTML produced by the
web site you use. The day the site's graphic designer decides
that "current temp:" should be spelled with capitalized
words, this script won't work. This is a problem with
programmatic parsing of web pages that will go away only when more
structural formats (such as XML) are used to produce web
pages.<fonT SIZe="1"><sup><A HREf="#FOOTNOTE-6">[6]</a></suP></FONt>
</p>


<blockquote><font size="1">
<P cLass="footnote">
<SuP><A nAMe="FOOTNOTE-6">[6]</A></sUP>

<A Name="idx792"></a>XML (eXtensible
Markup Language) is a language for marking up structured text files
that emphasizes the structure of the document, not its graphical
nature. XML processing is an entirely different area of Python text
processing, with much ongoing work. See <A href="lpython_cnode114.html">Appendix A</a>,
for some pointers to discussion groups and software.</p>
</fONT></BlockQUOTe>









<h4>9.4.2
Checking the Validity of Links and Mirroring Web Sites: <i>webchecker.py</i> and Friends</h4>


<p>One of the big hassles of maintaining a<a name="idx793"></a> web
site is that as the number of links in the site increases, so does
the chance that some of the links will no longer be valid. Good
web-site maintenance therefore includes periodic checking for such
stale links. The standard Python distribution includes a tool that
does just this. It lives in the <i>Tools/webchecker</i>
directory and is called
<i>webchecker.py</i>
<a namE="idx794"></a>.</P>



<p>A companion program called <i>websucker.py</i> located
in the same directory uses similar logic to create a local copy of a
remote web site. Be careful when trying it out, because if
you're not careful, it will try to download the entire Web on
your machine! The same directory includes two programs called
<I>wsgui.py</i> and <I>webgui.py</I> that
are Tkinter-based frontends to <i>websucker</I> and
<I>webchecker</i>, respectively. We encourage you to
look at the source code for these programs to see how one can build
sophisticated web-management systems with Python's standard
toolset.</P>



<p>In the <I>Tools/Scripts</I> directory, you'll
find many other small to medium-sized scripts that might be of
interest, such as an equivalent of <I>websucker.py</I>
for FTP servers called <i>ftpmirror.py</i>.</p>







<h4>9.4.3
Checking Mail</H4>


<P>Electronic<A Name="idx795"></a> <A NAMe="idx796"></a> mail is probably the most important
medium on the Internet today; it's certainly the protocol with
which most information passes between individuals. Python includes
several libraries for processing mail. The one you'll need to
use depends on the kind of<a nAME="idx797"></A> mail server you're using. Modules
for interacting with POP3 servers (<tt class="monofont">poplib</tt>) and
IMAP servers (<tt class="monofont">imaplib</tt>) are included. If you need
to talk to a Microsoft Exchange server, you'll need some of the
tools in the <i>win32</i> distribution (see <A href="lpython_cnode123.html">Appendix B</a>, for pointers to the
<I>win32</i> extensions web page).</P>



<P>Here's a simple test of the <tT ClAsS="monofont">poplib</TT> module,
which is used to talk to a mail server running the POP protocol:</P>



<pre cLASS="monofont">&gt;&gt;&gt; <b>from poplib import *</b>
&gt;&gt;&gt; <b>server = POP3('mailserver.spam.org')</b>
&gt;&gt;&gt; <B>print server.getwelcome()</B>
+OK QUALCOMM Pop server derived from UCB (version 2.1.4-R3) at spam starting.
&gt;&gt;&gt; <B>server.user('da')</B>
'+OK Password required for da.'
&gt;&gt;&gt; <b>server.pass_('youllneverguess')</b>
'+OK da has 153 message(s) (458167 octets).'
&gt;&gt;&gt; <b>header, msg, octets = server.retr(152)</b># let's get the latest msgs
&gt;&gt;&gt; <B>import string</B>
&gt;&gt;&gt; <B>print string.join(msg[:3], '\n')</B>   # and look at the first three lines
Return-Path: &lt;jim@bigbad.com&gt;
Received: from gator.bigbad.com by mailserver.spam.org (4.1/SMI-4.1)
        id AA29605; Wed, 25 Nov 98 15:59:24 PST</pre>


<p>In a real application, you'd use a specialized module such as
<tt class="monofont">rfc822</tt> to parse the header lines, and perhaps the
<tt class="monofont">mimetools</Tt> and <Tt clAsS="monofont">mimify</Tt> modules to
get the data out of the message body (e.g., to process attached
files).</P>


</FoNt>
 
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#CCCCCC">
<td><font color="black" size=1>I l<font color="#FF0000">@</font>ve RuBoard</td>
<td valign="top" class="v2" align="right"><a href="lpython_snode99.html"><img src="FILES/previous.gif" width="62" height="15" border="0" align="absmiddle" alt="Previous Section"></a> <a href="lpython_snode101.html"><img src="FILES/next.gif" width="41" height="15" border="0" align="absmiddle" alt="Next Section"></a></td></table>
</BODY></HTML>