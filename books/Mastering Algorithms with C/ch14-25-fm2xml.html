<HTML><HEAD><TITLE>ch14-25-fm2xml</TITLE>
<META http-equiv=Content-Type content="text/html; charset=windows-1252"><LINK 
href="image/style.css" type=text/css rel=STYLESHEET><LINK 
href="image/docsafari.css" type=text/css rel=STYLESHEET>

<META content="MSHTML 6.00.2800.1141" name=GENERATOR></HEAD>
<BODY leftMargin=0 topMargin=0 marginheight="0" marginwidth="0"><A 
name=toppage></A><SPAN class=v2></SPAN>
<TABLE cellSpacing=0 cellPadding=0 width="100%" border=0>
  <TBODY>
  <TR vAlign=top>
    <TD align=middle>
      <TABLE width="95%">
        <TBODY>
        <TR>
          <TD class=v2 align=left>
            <HR SIZE=1>

            <TABLE cellSpacing=0 cellPadding=5 width="100%" border=0>
              <TBODY>
              <TR>
                <TD vAlign=top width=76 rowSpan=4><A 
                  ><IMG 
                  height=100 src="image/masteralgoc_xs.gif" width=76 
                  border=0></A></TD>
                <TD class=v2 vAlign=top><A class=t1 
                  >Mastering 
                  Algorithms with C</A><BR>By Kyle&nbsp;Loudon<BR>Slots : 
                1<BR></TD></TR>
              <TR>
                <TD class=v1><A class=v1 
                  href="table.html">Table 
                  of Contents</A></TD></TR>
              <TR>
                <TD></TD></TR>
              <TR>
                <TD class=t1 vAlign=bottom>Chapter 14.&nbsp; Data 
                Compression</TD></TR></TBODY></TABLE>
            <HR SIZE=1>
            <TABLE cellSpacing=0 cellPadding=2 width="100%" border=0>
              <TBODY>
              <TR>
              <TR>
                <TD class=v2 vAlign=top align=center>&nbsp; <A accessKey=2 
                  href="ch14-20-fm2xml.html"><IMG 
                  height=16 src="image/btn_prev.gif" width=56 
                  align=left border=0></A> &nbsp; <A  
href="table.html" align=center> Content </A>	<A accessKey=1                  href="ch14-30-fm2xml.html"><IMG 
                  height=16 src="image/btn_next.gif" width=41 
                  align=right border=0></A></TD></TR></TBODY></TABLE>
            <BR>
            <TABLE cellSpacing=0 cellPadding=0 width="100%" border=0>
              <TBODY>
              <TR>
                <TD vAlign=top><A name=ch14-25-fm2xml></A>
                  <H3 class=docSection1Title>14.4 Description of Huffman 
                  Coding</H3>
                  <P class=docText><A name=ch14-idx-213479-1></A><A 
                  name=ch14-idx-213479-2></A><A name=ch14-idx-213479-3></A>One 
                  of the oldest and most elegant forms of data compression is 
                  Huffman coding, an algorithm based on minimum redundancy 
                  coding. Minimum redundancy coding<A name=IXT-14-314634></A><A 
                  name=IXT-14-314635></A> suggests that if we know how often 
                  different symbols occur in a set of data, we can represent the 
                  symbols in a way that makes the data require less space. The 
                  idea is to encode symbols that occur more frequently with 
                  fewer bits than those that occur less frequently. It is 
                  important to realize that a symbol is not necessarily a 
                  character of text: a symbol can be any amount of data we 
                  choose, but it is often one byte's worth.</P><A 
                  name=ch14-26-fm2xml></A>
                  <H4 class=docSection2Title>14.4.1 Entropy and Minimum 
                  Redundancy</H4>
                  <P class=docText><A name=IXT-14-314636></A><A 
                  name=IXT-14-314637></A>To begin, let's revisit the concept of 
                  entropy introduced at the beginning of the chapter. Recall 
                  that every set of data has some informational content, which 
                  is called its entropy. The entropy of a set of data is the sum 
                  of the entropies of each of its symbols. The entropy <SPAN 
                  class=docEmphasis>S</SPAN> of a symbol <SPAN 
                  class=docEmphasis>z</SPAN> is defined as:</P>
                  <P class=docText><SPAN class=docEmphasis>S<SUB>z</SUB></SPAN> 
                  = -lg<SPAN class=docEmphasis>P<SUB>z</SUB></SPAN></P>
                  <P class=docText><A name=IXT-14-314638></A>where <SPAN 
                  class=docEmphasis>Pz</SPAN> is the probability of <SPAN 
                  class=docEmphasis>z</SPAN> being found in the data. If it is 
                  known exactly how many times <SPAN class=docEmphasis>z</SPAN> 
                  occurs, <SPAN class=docEmphasis>Pz</SPAN> is referred to as 
                  the <SPAN class=docEmphasis>frequency</SPAN> of <SPAN 
                  class=docEmphasis>z</SPAN>. As an example, if <SPAN 
                  class=docEmphasis>z</SPAN> occurs 8 times in 32 symbols, or 
                  one-fourth of the time, the entropy of <SPAN 
                  class=docEmphasis>z</SPAN> is:</P>
                  <P class=docText>-lg(1/4) = 2 bits</P>
                  <P class=docText>This means that using any more than two bits 
                  to represent <SPAN class=docEmphasis>z</SPAN> is more than we 
                  need. If we consider that normally we represent a symbol using 
                  eight bits (one byte), we see that compression here has the 
                  potential to improve the representation a great deal.</P>
                  <P class=docText><A class=docLink 
                  href="#ch14-36243">Table 
                  14.1</A> presents an example of calculating the entropy of 
                  some data containing 72 instances of five different symbols. 
                  To do this, we sum the entropies contributed by each symbol. 
                  Using "U" as an example, the total entropy for a symbol is 
                  computed as follows. Since "U" occurs 12 times out of the 72 
                  total, each instance of "U" has an entropy that is calculated 
                  as:</P>
                  <P class=docText>-lg(12/72) = 2.584963 bits</P>
                  <P class=docText>Consequently, because "U" occurs 12 times in 
                  the data, its contribution to the entropy of the data is 
                  calculated as:</P>
                  <P class=docText>(2.584963)(12) = 31.01955 bits</P>
                  <P class=docText>In order to calculate the overall entropy of 
                  the data, we sum the total entropies contributed by each 
                  symbol. To do this for the data in <A class=docLink 
                  href="#ch14-36243">Table 
                  14.1</A>, we have:</P>
                  <P class=docText>31.01955+36.000000+23.53799+33.94552+36.95994 
                  = 161.46300 bits</P>
                  <P class=docText>If using 8 bits to represent each symbol 
                  yields a data size of (72)(8) = 576 bits, we should be able to 
                  compress this data, in theory, by up to:</P>
                  <P class=docText>1-(161.463000/576) = 72.0%</P><A 
                  name=ch14-36243></A>
                  <P>
                  <TABLE cellSpacing=0 cellPadding=4 rules=all width="100%" 
                  border=1>
                    <CAPTION>
                    <H5 class=docTableTitle>Table 14.1. The Entropy of a Set of 
                    Data Containing 72 Instances of 5 Different Symbols 
                    </H5></CAPTION>
                    <COLGROUP span=4>
                    <THEAD>
                    <TR>
                      <TH class=docTableHeader>
                        <P class=docText>Symbol</P></TH>
                      <TH class=docTableHeader>
                        <P class=docText>Probability</P></TH>
                      <TH class=docTableHeader>
                        <P class=docText>Entropy of Each Instance</P></TH>
                      <TH class=docTableHeader>
                        <P class=docText>Total Entropy</P></TH></TR></THEAD>
                    <TBODY>
                    <TR>
                      <TD class=docTableCell>
                        <P class=docText>U</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>12/72</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>2.584963</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>31.01955</P></TD></TR>
                    <TR>
                      <TD class=docTableCell>
                        <P class=docText>V</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>18/72</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>2.000000</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>36.00000</P></TD></TR>
                    <TR>
                      <TD class=docTableCell>
                        <P class=docText>W</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>7/72</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>3.362570</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>23.53799</P></TD></TR>
                    <TR>
                      <TD class=docTableCell>
                        <P class=docText>X</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>15/72</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>2.263034</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>33.94552</P></TD></TR>
                    <TR>
                      <TD class=docTableCell>
                        <P class=docText>Y</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>20/72</P></TD>
                      <TD class=docTableCell>
                        <P class=docText>1.847997</P></TD>
                      <TD class=docTableCell>
                        <P 
                  class=docText>36.95994</P></TD></TR></COLGROUP></TBODY></TABLE></P><A 
                  name=ch14-27-fm2xml></A>
                  <H4 class=docSection2Title>14.4.2 Building a Huffman Tree</H4>
                  <P class=docText>Huffman coding presents a way to approximate 
                  the optimal representation of data based on its entropy. It 
                  works by building a data structure called a <A 
                  name=ch14-idx-1027926-1></A><A name=IXT-14-314639></A><SPAN 
                  class=docEmphasis>Huffman tree</SPAN>, which is a binary tree 
                  (see <A class=docLink 
                  href="ch09-1-fm2xml.html#ch09-1-fm2xml">Chapter 
                  9</A>) organized to generate <SPAN class=docEmphasis>Huffman 
                  codes</SPAN>. <A name=IXT-14-314640></A>Huffman codes are the 
                  codes assigned to symbols in the data to achieve compression. 
                  However, Huffman codes result in compression that only 
                  approximates the data's entropy because, as you may have 
                  noticed in <A class=docLink 
                  href="#ch14-36243">Table 
                  14.1</A>, the entropies of symbols often come out to be 
                  fractions of bits. Since the actual number of bits used in 
                  Huffman codes cannot be fractions in practice, some codes end 
                  up with slightly too many bits to be optimal.</P>
                  <P class=docText><A class=docLink 
                  href="#ch14-20227">Figure 
                  14.1</A> illustrates the process of building a Huffman tree 
                  from the data in <A class=docLink 
                  href="#ch14-36243">Table 
                  14.1</A>. Building a Huffman tree proceeds from its leaf nodes 
                  upward. To begin, we place each symbol and its frequency in 
                  its own tree (see <A class=docLink 
                  href="#ch14-20227">Figure 
                  14.1</A>, step 1). Next, we merge the two trees whose root 
                  nodes have the smallest frequencies and store the sum of the 
                  frequencies in the new tree's root (see <A class=docLink 
                  href="#ch14-20227">Figure 
                  14.1</A>, step 2). This process is then repeated until we end 
                  up with a single tree (see <A class=docLink 
                  href="#ch14-20227">Figure 
                  14.1</A>, step 5), which is the final Huffman tree. The root 
                  node of this tree contains the total number of symbols in the 
                  data, and its leaf nodes contain the original symbols and 
                  their frequencies. Because Huffman coding continually seeks 
                  out the two trees that appear to be the best to merge at any 
                  given time, it is a good example of a greedy algorithm (see <A 
                  class=docLink 
                  href="ch01-1-fm2xml.html#ch01-1-fm2xml">Chapter 
                  1</A>).</P>
                  <CENTER><A name=IXTR3-135></A>
                  <H5 class=docFigureTitle><A name=ch14-20227></A>Figure 14.1. 
                  Building a Huffman tree from the symbols and frequencies in <A 
                  class=docLink 
                  href="#ch14-36243">Table 
                  14.1</A></H5><IMG height=474 alt=figs/alc.1401.gif 
                  src="image/alc.1401.gif" width=502 border=0></CENTER><A 
                  name=ch14-28-fm2xml></A>
                  <H4 class=docSection2Title>14.4.3 Compressing and 
                  Uncompressing Data</H4>
                  <P class=docText><A name=ch14-idx-1026149-1></A>Building a 
                  Huffman tree is part of both compressing and uncompressing 
                  data. To compress data using a Huffman tree, given a specific 
                  symbol, we start at the root of the tree and trace a path to 
                  the symbol's leaf. As we descend along the path, whenever we 
                  move to the left, we append to the current code; whenever we 
                  move to the right, we append 1. Thus, in <A class=docLink 
                  href="#ch14-20227">Figure 
                  14.1</A>, step 6, to determine the Huffman code for "U" we 
                  move to the right (1), then to the left (10), and then to the 
                  right again (101). The Huffman codes for all of the symbols in 
                  the figure are:</P>
                  <P class=docText>U = 101, V = 01, W = 100, X = 00, Y = 11</P>
                  <P class=docText>To uncompress data using a Huffman tree, we 
                  read the compressed data bit by bit. Starting at the tree's 
                  root, whenever we encounter in the data, we move to the left 
                  in the tree; whenever we encounter 1, we move to the right. 
                  Once we reach a leaf node, we generate the symbol it contains, 
                  move back to the root of the tree, and repeat the process 
                  until we exhaust the compressed data. Uncompressing data in 
                  this manner is possible because Huffman codes are <SPAN 
                  class=docEmphasis>prefix free</SPAN>, which means that no code 
                  is a prefix of any other. This ensures that once we encounter 
                  a sequence of bits that matches a code, there is no ambiguity 
                  as to the symbol it represents. For example, notice that 01, 
                  the code for "V," is not a prefix of any of the other codes. 
                  Thus, as soon as we encounter 01 in the compressed data, we 
                  know that the code must represent "V."</P><A 
                  name=ch14-29-fm2xml></A>
                  <H4 class=docSection2Title>14.4.4 Effectiveness of Huffman 
                  Coding</H4>
                  <P class=docText><A name=IXT-14-314641></A>To determine the 
                  reduced size of data compressed using Huffman coding, we 
                  calculate the product of each symbol's frequency times the 
                  number of bits in its Huffman code, then add them together. 
                  Thus, to calculate the compressed size of the data presented 
                  in <A class=docLink 
                  href="#ch14-36243">Table 
                  14.1</A> and <A class=docLink 
                  href="#ch14-20227">Figure 
                  14.1</A>, we have:</P>
                  <P class=docText>(12)(3)+(18)(2)+(7)(3)+(15)(2)+(20)(2) = 163 
                  bits</P>
                  <P class=docText>Assuming that without compression each of the 
                  72 symbols would be represented with 8 bits, for a total data 
                  size of 576 bits, we end up with the following compression 
                  ratio:</P>
                  <P class=docText>1-(163/576)=71.7%</P>
                  <P class=docText>Once again, considering the fact that we 
                  cannot take into account fractional bits in Huffman coding, in 
                  many cases this value will not be quite as good as the data's 
                  entropy suggests, although in this case it is very close.</P>
                  <P class=docText>In general, Huffman coding is not the most 
                  effective form of compression, but it runs fast both when 
                  compressing and uncompressing data. Generally, the most 
                  time-consuming aspect of compressing data with Huffman coding 
                  is the need to scan the data twice: once to gather 
                  frequencies, and a second time actually to compress the data. 
                  Uncompressing the data is particularly efficient because 
                  decoding the sequence of bits for each symbol requires only a 
                  brief scan of the Huffman tree, which is bounded.<A 
                  name=IXTR3-136></A></P>                  <UL></UL></TD></TR></TBODY></TABLE>
            <HR SIZE=1>

            <TABLE cellSpacing=0 cellPadding=2 width="100%" border=0>
              <TBODY>
              <TR>
              <TR>
                <TD class=v2 vAlign=top align=right>&nbsp; <A accessKey=2 
                  href="ch14-20-fm2xml.html"><IMG 
                  height=16 src="image/btn_prev.gif" width=56 
                  align=absMiddle border=0></A> &nbsp; <A accessKey=1 
                  href="ch14-30-fm2xml.html"><IMG 
                  height=16 src="image/btn_next.gif" width=41 
                  align=absMiddle border=0></A></TD></TR></TBODY></TABLE>
            <TABLE cellSpacing=0 cellPadding=0 width="100%" border=0>
              <TBODY>
              <TR>
                <TD vAlign=top align=right><A class=v1 
                  href="#toppage">Top</A></TD></TR></TBODY></TABLE>
            </TD></TR></TBODY></TABLE></TD>
</BODY></HTML>
