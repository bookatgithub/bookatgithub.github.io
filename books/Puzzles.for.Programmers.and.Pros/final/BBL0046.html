<html>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<head>
<title>Overloaded Scheduling and Freezing Crystals</title>
<link rel="STYLESHEET" type="text/css" href="images/css1.css">
<link rel="STYLESHEET" type="text/css" href="images/css2.css">
</head>
<body>
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr><td><div STYLE="MARGIN-LEFT: 0.15in;">
<a href=BBL0045.html><img src="images/prev.gif" width="60" height="17" border="0" align="absmiddle" alt="Previous Page"></a>
<td align="right"><div STYLE="MARGIN-LEFT: 0.15in;">
<a href=BBL0047.html><img src="images/next.gif" width="60" height="17" border="0" align="absmiddle" alt="Next Page"></a>
</div></td></tr></table>
<br><div class="chapter">
<a name=""></a>
<div class="section">
<h2 class="first-section-title">
<a name="389"></a><a name="ch06lev1"></a>Overloaded Scheduling and Freezing Crystals</h2>
<a name="390"></a><a name="IDX-"></a>
<p class="para">When first faced with a problem, try pure greed (look for moves that lower the cost). If that doesn&rsquo;t work or if it reaches a dead end, then you can try case analysis as we did for Sudoko. If there are too many cases, then try dynamic programming. If all these fail, it is time for heuristic search techniques along with some bound on the optimum.</p>
<p class="para">A heuristic technique is one that is not guaranteed to work perfectly, but often (you hope) works well. Greed is, of course, a great heuristic technique but can be brittle, as we saw in the solution to the &ldquo;Finding a Schedule that Works&rdquo; puzzle.</p>
<p class="para">The principle of exploratory heuristic techniques is that they take anti-greedy moves sometimes. They do this in order to explore the search space. If each problem configuration could be encoded as some scalar value (horizontal axis of <a class="internaljump" href="#ch0">Figure 2-23</a>), then the squiggly line in <a class="internaljump" href="#ch0">Figure 2-23</a> represents the cost of each configuration.</p>
<div class="figure">
<a name="391"></a><a name="ch0"></a><span class="figuremediaobject"><a href="images/fig178_01_0.jpg" NAME="IMG_65" target="_parent"><img alt="Image from book" id="IMG_65" src="images/fig178_01.jpg" height="173" width="350" title="Click To expand" border="0"></a></span>
<br style="line-height: 1">
<span class="figure-title"><span class="figure-titlelabel">Figure 2-23: </span>The horizontal axis represents the search space. The vertical represents the cost. A greedy approach, given a location in the search space, will perform moves that decrease cost. So the arrows always start from a higher cost and go to a lower one.</span>
</div>
<p class="para">A greedy approach will make moves that decrease cost only. An exploratory heuristic technique will sometimes make moves to more costly states in the hopes of leaving a local minimum (see <a class="internaljump" href="#ch0">Figure 2-24</a>). If this reminds you of high school chemistry notions of activation energy, you have the right intuition.</p>
<a name="392"></a><a name="IDX-"></a>
<div class="figure">
<a name="393"></a><a name="ch0"></a><span class="figuremediaobject"><a href="images/fig179_01_0.jpg" NAME="IMG_66" target="_parent"><img alt="Image from book" id="IMG_66" src="images/fig179_01.jpg" height="173" width="350" title="Click To expand" border="0"></a></span>
<br style="line-height: 1">
<span class="figure-title"><span class="figure-titlelabel">Figure 2-24: </span>In simulated annealing and other heuristic search techniques, some moves increase the cost. You can see that with the rightmost arrow which goes from a lower cost valley to a higher cost hill. The idea is to escape local cost minima.</span>
</div>
<p class="para">But we don&rsquo;t want to go any which way all the time. Let&rsquo;s first look at animal analogies - flies vs. bees. If trying to leave the interior of a house, a fly, starting from a closed window, may bump against the window a few times, but will eventually fly away from that window and find an open one. Bees keep bumping up against the glass. Flies are using anti-greedy moves to escape the local optimum of the windowpane. Bees insist on trying to push through the pane.<sup>[<a name="ch06foot" href="#ftn.ch06foot">2</a>]</sup> On the other hand, flies never build anything as complex as a beehive. To me, this suggests that when you are faced with a technical problem, you must alternate between fly behavior and bee behavior. At some point you must stop exploring and start doing, but if you hit a roadblock, then you must start to explore some more.</p>
<p class="para">The main exploratory search techniques are genetic (also known as evolutionary) algorithms, tabu searching, integer programming, and simulated annealing. A book that explains these techniques both engagingly and academically is <I xmlns:crossref="http://www.jclark.com/xt/java/com.books24x7.xsl.Crossref" class="citetitle">How to Solve It: Modern Heuristics</I> by Zvigniew Michalewicz and David B. Fogel (Springer, 2004).</p>
<p class="para">Everybody seems to have a favorite. Mine is simulated annealing, because it requires fewer parameters than genetic algorithms and has a wider range of applicability than integer programming. I should note, however, that the other techniques work really well sometimes. If a problem is heavily numeric, for example, linear programming approximations to the integer programming problem can sometimes work well and avoid numerical inaccuracies. Some people make great use of genetic algorithms. I consider it a character flaw that I can&rsquo;t ever tune the parameters correctly.</p>
<a name="395"></a><a name="IDX-"></a>
<p class="para">Simulated annealing conforms to my intuition about the way the world works. I first heard about the technique in a lecture by its main inventor Kirkpatrick in 1983. He described how he had thought of the idea when he had to solve a traveling salesman-like problem - how to minimize the lengths of wires (I think power wires) around a circuit board. The problem was NP-complete but the company (IBM) needed a solution. Kirkpatrick was a solid-state physicist, so he suggested a technique inspired by the cooling of crystals.</p>
<p class="para">If you take liquid silicon and freeze it quickly it will not form a crystal, which is the lowest energy state, but rather an amorphous (higher energy) blob. If you freeze it slowly (&ldquo;anneal it&rdquo;), then it will form a nice crystal. The slow cooling allows it to find an optimal state. At high temperatures atoms move rapidly, allowing them to escape local energy minima to fly off to far places. At lower temperatures, the atoms rearrange themselves only slightly.</p>
<p class="para">Kirkpatrick reasoned that the same thing could be done with algorithms. Given a wire route at some stage of the algorithm, a move would replace, say, a random quadruple in the path, say A to B to C to D, by A to C to B to D in one of two cases:</p>
<ul class="itemizedlist">
<li class="first-listitem">
<p class="first-para"> The change resulted in a shorter path.</p>
</li>
<li class="listitem">
<p class="first-para"> The change resulted in a longer path, but when you threw some pseudo-random dice, they came out in a certain improbable configuration.</p>
</li>
</ul>
<p class="para">Thus, step 2 is anti-greedy but occurs only with a certain probability. The key insight in Kirkpatrick&rsquo;s proposal was that this probability of taking an anti-greedy move would depend on both the amount by which the path became longer, and on the amount of time the algorithm had run. The probability would start out quite high (potentially near 1) but then gradually become lower as the algorithm proceeded. Towards the end, the algorithm almost never does anti-greedy moves. In annealing terms, the temperature decreases over the course of the algorithm as does the probability of taking anti-greedy moves. Here is the pseudo-code.</p>
<div class="informalexample">
<pre class="programlisting">
take an initial route r1
 initialize temperature T to a high value
 loop until T is low enough
     or you have found an answer close to optimal
   consider a randomly chosen possible change
           yielding a route r2
   if r2 is lower cost than r1
     then replace r1 by r2
     else replace r1 by r2 with probability
        e<sup>((cost(r1)-cost(r2))/T)</sup>
   end if
   decrease the temperature T
 end loop</pre>
</div>
<p class="para">Note that the exponent is always negative for an anti-greedy move, so the probability is between 0 and 1. The larger the temperature T, the closer the exponent is to 0, so the higher the probability. Also, the larger the increase in cost the more negative, so the lower the probability. Thus, anti-greed tends to happen earlier and for small increases in cost.</p>
<a name="396"></a><a name="IDX-"></a>
<p class="para">Now, Kirkpatrick observed that the technique was in no way dependent on the specifics of wire layout, as I hope you can see from the pseudo-code. Further, the only parameters are the initial temperature, the speed at which the temperature decreases, and an encoding of the cost.</p>
<p class="para">The elegance of this approach immediately appealed to me, but the feeling was not universal. One professor asked pointedly, &ldquo;What have you proved?&rdquo; &ldquo;Nothing,&rdquo; Kirkpatrick replied with that impatience that physicists manifest when faced with questions from pesky mathematicians. &ldquo;Under certain conditions one can show that if you freeze infinitely slowly you will get a crystal, but what this means in terms of an algorithm is very uncertain.&rdquo;</p>
<p class="para">Their exchange illustrates a cultural divide between the heurist and the algorithmist. The heurist has an NP-hard problem to solve, so uses a heuristic that seems at least plausible. The algorithmist wants a time guarantee. Both have their place, but since NP-completeness has been around since the 1970s and no algorithm seems to be coming over the horizon, it&rsquo;s possible that we&rsquo;ll be stuck with heuristics for quite some time for many problems.</p>
<p class="para">Heuristic search techniques offer no guarantee, however. The search space could have just one sweet spot protected by a high-cost slope as in <a class="internaljump" href="#ch0">Figure 2-25</a>. This would make it hard even for simulated annealing to find the optimum.</p>
<div class="figure">
<a name="397"></a><a name="ch0"></a><span class="figuremediaobject"><a href="images/fig181_01_0.jpg" NAME="IMG_67" target="_parent"><img alt="Image from book" id="IMG_67" src="images/fig181_01.jpg" height="173" width="350" title="Click To expand" border="0"></a></span>
<br style="line-height: 1">
<span class="figure-title"><span class="figure-titlelabel">Figure 2-25: </span>A search space for which simulated annealing might have a difficult time finding a solution. The optimum is at a very specific part of the search space, so a move is unlikely to find it.</span>
</div>
<p class="para">I invite you now to apply simulated annealing to the overloaded scheduling problem. Here is how it goes. You have a bunch of tasks, each of which has a deadline, an amount of work, and a positive value. You receive all the value of the task if you finish by the deadline and nothing if you finish after the deadline. You have more work to do than you could possibly finish.</p>
<ul class="simple-list">
<li class="first-listitem">
<p class="first-para">Task T1 takes 3 days with deadline on day 19 and value 17</p>
</li>
<li class="listitem">
<p class="first-para">Task T2 takes 4 days with deadline on day 23 and value 14</p>
</li>
<li class="listitem">
<p class="first-para">Task T3 takes 6 days with deadline on day 51 and value 10</p>
</li>
<li class="listitem">
<p class="first-para">Task T4 takes 3 days with deadline on day 30 and value 7</p>
</li>
<li class="listitem">
<p class="first-para">Task T5 takes 7 days with deadline on day 38 and value 13</p>
<a name="398"></a><a name="IDX-"></a>
</li>
<li class="listitem">
<p class="first-para">Task T6 takes 6 days with deadline on day 36 and value 11</p>
</li>
<li class="listitem">
<p class="first-para">Task T7 takes 7 days with deadline on day 45 and value 18</p>
</li>
<li class="listitem">
<p class="first-para">Task T8 takes 3 days with deadline on day 16 and value 10</p>
</li>
<li class="listitem">
<p class="first-para">Task T9 takes 5 days with deadline on day 22 and value 13</p>
</li>
<li class="listitem">
<p class="first-para">Task T10 takes 2 days with deadline on day 13 and value 16</p>
</li>
<li class="listitem">
<p class="first-para">Task T11 takes 8 days with deadline on day 12 and value 6</p>
</li>
<li class="listitem">
<p class="first-para">Task T12 takes 1 day with deadline on day 31 and value 15</p>
</li>
<li class="listitem">
<p class="first-para">Task T13 takes 5 days with deadline on day 17 and value 13</p>
</li>
<li class="listitem">
<p class="first-para">Task T14 takes 2 days with deadline on day 2 and value 13</p>
</li>
<li class="listitem">
<p class="first-para">Task T15 takes 7 days with deadline on day 30 and value 11</p>
</li>
<li class="listitem">
<p class="first-para">Task T16 takes 5 days with deadline on day 11 and value 18</p>
</li>
<li class="listitem">
<p class="first-para">Task T17 takes 4 days with deadline on day 4 and value 10</p>
</li>
<li class="listitem">
<p class="first-para">Task T18 takes 5 days with deadline on day 27 and value 15</p>
</li>
<li class="listitem">
<p class="first-para">Task T19 takes 4 days with deadline on day 6 and value 15</p>
</li>
</ul>
<ol class="orderedlist">
<li class="first-listitem">
<p class="first-para"> How much value can you obtain in total from these tasks?</p>
</li>
</ol>
<p class="para">Now, before attempting to answer this using simulated annealing (or any technique of your choice), first try to establish an upper bound on the value. A simple upper bound is the sum of all values. But such an upper bound might be excessively high. So, let&rsquo;s consider another method. Define the value density of a task to be the value of the task divided by the number of days it takes. For example, T18 has a value density of 15/5 = 3. Now, take the highest deadline (51 in this case). Order the tasks by descending order of their value densities.</p>
<div class="informalexample">
<pre class="programlisting">
T12: 15
T10: 8
T14: 6.5
T1: 5.666667
T19: 3.75
T16: 3.6
T2: 3.5
T8: 3.333333
T18: 3
T9: 2.6
T13: 2.6
T7: 2.571429
T17: 2.5
T4: 2.333333
T5: 1.857143
T6: 1.833333
T3: 1.666667
T15: 1.571429
T11: 0.75</pre>
</div>
<a name="399"></a><a name="IDX-"></a>
<p class="para">To compute the upper bound, note their computation time and see how many of these tasks starting from the one with highest value density can complete by day 51. If there is any time left over, then use the next task&rsquo;s value density and multiply it by the number of days left.</p>
<p class="para">In this case, we can complete all tasks down to task T17 in 50 days using the value density ordering. The upper bound, therefore, is the total value for tasks T12, T10, T14, T1, T19, T16, T2, T8, T18, T9, T13, T7, T17, which is 187 plus the value density of T4 times the one day remaining (2.3). The total, therefore, is 189.3. Now this upper bound is generous in that we have loosely assumed that all these tasks have deadline 51. But it may still be useful.</p>
<p class="para">Also, the value density heuristic may yield a good starting configuration for simulated annealing. Okay, enough hints. Try it for yourself.</p>
<a name="400"></a><a name="IDX-"></a>
<div class="section">
<h3 class="sect3-title">
<a name="401"></a><a name="ch06lev2"></a>Solution to Overloaded Scheduling and Freezing Crystals</h3>
<ol class="orderedlist">
<li class="first-listitem">
<p class="first-para">
<b class="bold"> How much value can you obtain in total from these tasks?</b>
</p>
</li>
</ol>
<p class="last-para">We&rsquo;ll follow the heuristic of preferring tasks with the greatest value density (value/computation time), suggesting a schedule of T14 T19 T16 T10 T8 T1 T2 T15 T12 T5 T7 T3. These give values of 13, 15, 18, 16, 10, 17, 14, 11, 15, 13, 18, and 10 respectively for a total of 170. That is quite close to the upper bound of 183. See if you can anneal to something better.</p>
</div>
</div>
<div class="footnotes">
<div class="footnote">
<p>
<a name="394"></a><sup>[<a name="ftn.ch06foot" href="#ch06foot">2</a>]</sup>One can speculate that the reason people who make the most radical discoveries in science are young is that they act like flies. They are less committed to the methods that have worked in the past. Older people, like bees, make greedy moves hoping that experience will lead to an optimum result. To keep the edge, older folk should look around more.</p>
</div>
</div>
</div>
</div>
</DIV>
<br>
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr><td><div STYLE="MARGIN-LEFT: 0.15in;">
<a href=BBL0045.html><img src="images/prev.gif" width="60" height="17" border="0" align="absmiddle" alt="Previous Page"></a>
<td align="right"><div STYLE="MARGIN-LEFT: 0.15in;">
<a href=BBL0047.html><img src="images/next.gif" width="60" height="17" border="0" align="absmiddle" alt="Next Page"></a>
</div></td></tr></table>
</body></html>