<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>D.1 The Benchmark Environment</title>
<link rel="STYLESHEET" type="text/css" href="images/style.css">
<link rel="STYLESHEET" type="text/css" href="images/docsafari.css">
</head>
<body >
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#e6e6e6">
<tr style="background-image: url(images/tile_back.gif);">
<td class="v2" align="left" width="30%">
<a href="squid-APP-D.html"><img src="images/previous.gif" width="70" height="19" border="0" align="absmiddle" alt="Previous Section"></a>
</td>
<td class="v2" align="center" width="40%">
<a href="main.html" style="color:white;text-decoration:none;text-underline:none">&nbsp;&lt;&nbsp;Day Day Up&nbsp;&gt;&nbsp;</a>
</td>
<td class="v2" align="right" width="30%">
<a href="squid-APP-D-SECT-2.html"><img src="images/next.gif" width="70" height="19" border="0" align="absmiddle" alt="Next Section"></a>
</td>
</tr>
</table>
<br>
<table width="100%" border="0" cellspacing="0" cellpadding="0"><tr><td valign="top"><A NAME="squid-APP-D-SECT-1"></A>
<H3 class="docSection1Title">D.1 The Benchmark Environment</H3>

<P class="docText">The primary purpose of these benchmarks is to provide a number of
measurements that allow you to compare different Squid configurations
and features. In order to produce comparable results,
I've taken care to minimize any differences between
systems being tested.</P>

<A NAME="squid-APP-D-SECT-1.1"></A>
<H4 class="docSection2Title">D.1.1 Hardware for Squid</H4>

<P class="docText">I used five identical
<A NAME="squid-APP-D-ITERM-2409"></A><A NAME="squid-APP-D-ITERM-2410"></A><A NAME="squid-APP-D-ITERM-2411"></A>computer systems—one
for each of the following operating systems: FreeBSD, Linux, NetBSD,
OpenBSD, and Solaris. The boxes are IBM Netfinity servers with one
500-MHz PIII CPU, 1 GB of RAM, an Intel fast-Ethernet NIC, and three
8-GB disk SCSI drives. I realize that these aren't
particularly powerful machines by today's standards,
but they are good enough for these tests. Anyway, it is more
important that they be identical than powerful.</P>

<P class="docText">The requirement to use identical hardware means that I
can't generate comparable results for other hardware
platforms, such as Sun, Digital/Compaq/HP, and others.</P>


<A NAME="squid-APP-D-SECT-1.2"></A>
<H4 class="docSection2Title">D.1.2 Squid Version and Configuration</H4>

<P class="docText">Except for the <span class="docEmphasis">coss</span> tests, all
<A NAME="squid-APP-D-ITERM-2412"></A><A NAME="squid-APP-D-ITERM-2413"></A><A NAME="squid-APP-D-ITERM-2414"></A><A NAME="squid-APP-D-ITERM-2415"></A><A NAME="squid-APP-D-ITERM-2416"></A>results are from Squid Version
2.5.STABLE2. The <span class="docEmphasis">coss</span> results are from a patched
version of 2.5.STABLE3. Those patches have been committed to the
source tree for inclusion into 2.5.STABLE4.</P>

<P class="docText">Unless otherwise specified, I used only the
<I>—enable-storeio</I> option when running
<I>./configure</I> before compiling Squid. For example:</P>

<PRE>% ./configure --enable-storeio=diskd,ufs,null,coss</PRE>

<P class="docText">In all cases, Squid is configured to use 7500 MB of each 8.2-GB disk.
This is a total cache size of 21.5 GB. Additionally,
<I>access.log</I> and <I>store.log</I>
have been disabled in the configuration file. Here is a sample
<I>squid.conf</I> file:</P>

<PRE>visible_hostname linux-squid.bench.tst
acl All src 0/0
http_access allow All

cache_dir aufs /cache0 7500 16 256
cache_dir aufs /cache1 7500 16 256
cache_dir aufs /cache2 7500 16 256

cache_effective_user nobody
cache_effective_group nobody
cache_access_log /dev/null
cache_store_log none
logfile_rotate 0</PRE>


<A NAME="squid-APP-D-SECT-1.3"></A>
<H4 class="docSection2Title">D.1.3 Web Polygraph Workload</H4>

<P class="docText">All the tests in this appendix <A NAME="squid-APP-D-ITERM-2417"></A><A NAME="squid-APP-D-ITERM-2418"></A>use the same Polygraph workload
file.<sup class="docFootnote"><A class="docLink" HREF="#squid-APP-D-FNOTE-1">[1]</A></sup> Meeting this requirement was, perhaps, the hardest part
of running these tests. Normally, the desired throughput is a
configuration parameter in a Polygraph workload. However, because the
sustainable throughput is different for each configuration, my
colleague Alex Rousskov and I developed a workload that can be used
for all tests.<sup class="docFootnote"><A class="docLink" HREF="#squid-APP-D-FNOTE-2">[2]</A></sup> We call this the "peak
finder" workload because it finds the peak
throughput for a device under test.</P><blockquote><p class="docFootnote"><sup><A NAME="squid-APP-D-FNOTE-1">[1]</A></sup> Except for the number-of-spindles tests, in
which the cache size depends on the number of disks in use.</p></blockquote><blockquote><p class="docFootnote"><sup><A NAME="squid-APP-D-FNOTE-2">[2]</A></sup> You can download this workload at
<A class="docLink" target="_blank" HREF="http://squidbook.org/extras/pf2-pm4.pg.txt">http://squidbook.org/extras/pf2-pm4.pg.txt</A>.</p></blockquote>

<P class="docText">The name "peak finder" is somewhat
misleading because, at least in Squid's case,
sustainable throughput decreases over time. The workload is designed
to periodically adjust the offered load (throughput) subject to
response time requirements. If the measured response time is below a
given threshold, Polygraph increases the load. If response time is
above the threshold, it decreases the load. Thus, at any point in
time during the test, we know the maximum throughput that still
satisfies the response time requirements.</P>

<P class="docText">In order to reach a steady-state condition, the test runs until the
cache has been filled twice. Polygraph knows the total cache size
(21.5 GB) and keeps track of the amount of fill traffic pulled into
the cache. These are responses that are cachable but not cache hits.
The test duration, then, depends on the sustainable throughput. When
the throughput is low, the test takes longer to complete. Some of
these tests took more than 10 days to run.</P>



<ul></ul></td></tr></table>
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#e6e6e6">
<tr style="background-image: url(images/tile_back.gif);">
<td class="v2" align="left" width="30%">
<a href="squid-APP-D.html"><img src="images/previous.gif" width="70" height="19" border="0" align="absmiddle" alt="Previous Section"></a>
</td>
<td class="v2" align="center" width="40%">
<a href="main.html" style="color:white;text-decoration:none;text-underline:none">&nbsp;&lt;&nbsp;Day Day Up&nbsp;&gt;&nbsp;</a>
</td>
<td class="v2" align="right" width="30%">
<a href="squid-APP-D-SECT-2.html"><img src="images/next.gif" width="70" height="19" border="0" align="absmiddle" alt="Next Section"></a>
</td>
</tr>
</table>
</body>
</html>
