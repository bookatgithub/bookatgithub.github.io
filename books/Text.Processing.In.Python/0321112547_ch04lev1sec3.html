<html><head>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<!--SafClassName="docSection1Title"--><!--SafTocEntry="4.3 Parser Libraries for Python"-->
<link rel="STYLESHEET" type="text/css" href="FILES/style.css">
<link rel="STYLESHEET" type="text/css" href="FILES/docsafari.css">
<style type="text/css">	.tt1    {font-size: 10pt;}</style>
</head>
<body>
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<td class="tt1"><a href="NFO/lib.html">[ Team LiB ]</a></td><td valign="top" class="tt1" align="right">
	<a href="0321112547_ch04lev1sec2.html"><img src="FILES/btn_prev.gif" width="62" height="15" border="0" align="absmiddle" alt="Previous Section"></a>
	<a href="0321112547_ch05.html"><img src="FILES/btn_next.gif" width="41" height="15" border="0" align="absmiddle" alt="Next Section"></a>
</td></table>
<br>
<table width="100%" border="0" cellspacing="0" cellpadding="0"><tr><td valign="top"><A NAME="ch04lev1sec3"></A><H3 class="docSection1Title">4.3 Parser Libraries for Python</H3>
<A NAME="ch04lev2sec14"></A><H4 class="docSection2Title">4.3.1 Specialized Parsers in the Standard Library</H4>
<P class="docText">Python comes standard with a number of modules that perform specialized parsing tasks. A variety of custom formats are in sufficiently widespread use that it is convenient to have standard library support for them. Aside from those listed in this chapter, <A class="docLink" HREF="0321112547_ch05.html#ch05">Chapter 5</A> discusses the <span class="docEmphasis"><TT>email</TT></span> and <span class="docEmphasis"><TT>xml</TT></span> packages, and the modules <span class="docEmphasis"><TT>mailbox</TT>, <TT>HTMLParser</TT></span>, and <span class="docEmphasis"><TT>urlparse</TT></span>, each of which performs parsing of sorts. A number of additional modules listed in <A class="docLink" HREF="0321112547_ch01.html#ch01">Chapter 1</A>, which handle and process audio and image formats, in a broad sense could be considered parsing tools. However, these media formats are better considered as byte streams and structures than as token streams of the sort parsers handle (the distinction is fine, though).</P>
<P class="docText">The specialized tools discussed under this section are presented only in summary. Consult the <span class="docEmphasis">Python Library Reference</span> for detailed documentation of their various APIs and features. It is worth knowing what is available, but for space reasons, this book does not document usage specifics of these few modules.</P>
<A NAME="ch04lev4sec1"></A><H5 class="docSection3Title"> ConfigParser</H5>
<P class="docText">Parse and modify Windows-style configuration files.</P>
<pre>
&gt;&gt;&gt; import ConfigParser
&gt;&gt;&gt; config = ConfigParser.ConfigParser()
&gt;&gt;&gt; config.read(['test.ini','nonesuch.ini'])
&gt;&gt;&gt; config.sections()
['userlevel', 'colorscheme']
&gt;&gt;&gt; config.get('userlevel','login')
'2'
&gt;&gt;&gt; config.set('userlevel','login',5)
&gt;&gt;&gt; config.write(sys.stdout)
[userlevel]
login = 5
title = 1

[colorscheme]
background = red
foreground = blue
</pre>
<A NAME="ch04lev4sec2"></A><H5 class="docSection3Title"> difflib<br>.../Tools/scripts/ndiff.py</H5>
<P class="docText">The module <span class="docEmphasis"><TT>difflib</TT></span>, introduced in Python 2.1, contains a variety of functions and classes to help you determine the difference and similarity of pairs of sequences. The API of <span class="docEmphasis"><TT>difflib</TT></span> is flexible enough to work with sequences of all kinds, but the typical usage is in comparing sequences of lines or sequences of characters.</P>
<P class="docText">Word similarity is useful for determining likely misspellings and typos and/or edit changes required between strings. The function <span class="docEmphasis"><TT>difflib.get_close_matches()</TT></span> is a useful way to perform "fuzzy matching" of a string against patterns. The required similarity is configurable.</P>
<pre>
&gt;&gt;&gt; users = ['j.smith', 't.smith', 'p.smyth', 'a.simpson']
&gt;&gt;&gt; maxhits = 10
&gt;&gt;&gt; login = 'a.smith'
&gt;&gt;&gt; difflib.get_close_matches(login, users, maxhits)
['t.smith', 'j.smith', 'p.smyth']
&gt;&gt;&gt; difflib.get_close_matches(login, users, maxhits, cutoff=.75)
['t.smith', 'j.smith']
&gt;&gt;&gt; difflib.get_close_matches(login, users, maxhits, cutoff=.4)
['t.smith', 'j.smith', 'p.smyth', 'a.simpson']
</pre>
<P class="docText">Line matching is similar to the behavior of the Unix <TT>diff</TT> (or <TT>ndiff)</TT> and <TT>patch</TT> utilities. The latter utility is able to take a source and a difference, and produce the second compared line-list (file). The functions <span class="docEmphasis"><TT>difflib.ndiff()</TT></span> and <span class="docEmphasis"><TT>difflib.restore()</TT></span> implement these capabilities. Much of the time, however, the bundled <TT>ndiff.py</TT> tool performs the comparisons you are interested in (and the "patches" with an <TT>-r#</TT> option).</P>
<pre>
%. ./ndiff.py chap4.txt chap4.txt~ | grep   '^[+-]'
-:  chap4.txt
+:  chap4.txt~
+      against patterns.
-     against patterns.  The required similarity is configurable.
-
-     &gt;&gt;&gt; users = ['j.smith', 't.smith', 'p.smyth', 'a.simpson']
-     &gt;&gt;&gt; maxhits = 10
-     &gt;&gt;&gt; login = 'a.smith'
</pre>
<P class="docText">There are a few more capabilities in the <span class="docEmphasis"><TT>difflib</TT></span> module, and considerable customization is possible.</P>
<A NAME="ch04lev4sec3"></A><H5 class="docSection3Title"> formatter</H5>
<P class="docText">Transform an abstract sequence of formatting events into a sequence of callbacks to "writer" objects. Writer objects, in turn, produce concrete outputs based on these callbacks. Several parent formatter and writer classes are contained in the module.</P>
<P class="docText">In a way, <span class="docEmphasis"><TT>formatter</TT></span> is an "anti-parser"—that is, while a parser transforms a series of tokens into program events, <span class="docEmphasis"><TT>formatter</TT></span> transforms a series of program events into output tokens.</P>
<P class="docText">The purpose of the <span class="docEmphasis"><TT>formatter</TT></span> module is to structure creation of streams such as word processor file formats. The module <span class="docEmphasis"><TT>htmllib</TT></span> utilizes the <span class="docEmphasis"><TT>formatter</TT></span> module. The particular API details provide calls related to features like fonts, margins, and so on.</P>
<P class="docText">For highly structured output of prose-oriented documents, the <span class="docEmphasis"><TT>formatter</TT></span> module is useful, albeit requiring learning a fairly complicated API. At the minimal level, you may use the classes included to create simple tools. For example, the following utility is approximately equivalent to <TT>lynx -dump:</TT></P>
<H5 class="docExampleTitle"><A NAME="ch04list13"></A> urldump.py</H5>

<PRE>
#!/usr/bin/env python
import sys
from urllib import urlopen
from htmllib import HTMLParser
from formatter import AbstractFormatter, DumbWriter
if len(sys.argv) &gt; 1:
    fpin = urlopen(sys.argv[1])
    parser = HTMLParser(AbstractFormatter(DumbWriter()))
    parser.feed(fpin.read())
    print '-----------------------------------------------'
    print fpin.geturl()
    print fpin.info()
else:
    print "No specified URL"
</PRE>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
htmllib <span class="docEmphasis">285;</span> urllib <span class="docEmphasis">388;</span></p>
<A NAME="ch04lev4sec4"></A><H5 class="docSection3Title"> htmllib</H5>
<P class="docText">Parse and process HTML files, using the services of <span class="docEmphasis"><TT>sgmllib</TT></span>. In contrast to the <span class="docEmphasis"><TT>HTMLParser</TT></span> module, <span class="docEmphasis"><TT>htmllib</TT></span> relies on the user constructing a suitable "formatter" object to accept callbacks from HTML events, usually utilizing the <span class="docEmphasis"><TT>formatter</TT></span> module. A formatter, in turn, uses a "writer" (also usually based on the <span class="docEmphasis"><TT>formatter</TT></span> module). In my opinion, there are enough layers of indirection in the <span class="docEmphasis"><TT>htmllib</TT></span> API to make <span class="docEmphasis"><TT>HTMLParser</TT></span> preferable for almost all tasks.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
HTMLParser <span class="docEmphasis">384</span><SUB>;</SUB> formatter <span class="docEmphasis">284</span><SUB>;</SUB> sgmllib <span class="docEmphasis">285;</span></p>
<A NAME="ch04lev4sec5"></A><H5 class="docSection3Title"> multifile</H5>
<P class="docText">The class <span class="docEmphasis"><TT>multifile.MultiFile</TT></span> allows you to treat a text file composed of multiple delimited parts as if it were several files, each with their own FILE methods: <TT>.read()</TT>, <TT>.readline()</TT>, <TT>.readlines()</TT>, <TT>.seek()</TT>, and <TT>.tell()</TT> methods. In iterator fashion, advancing to the next virtual file is performed with the method <span class="docEmphasis"><TT>multifile.MultiFile.next()</TT></span>.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
fileinput <span class="docEmphasis">61;</span> mailbox <span class="docEmphasis">372;</span> email.Parser <span class="docEmphasis">363;</span> string.split() <span class="docEmphasis">142;</span> file <span class="docEmphasis">15;</span></p>
<A NAME="ch04lev4sec6"></A><H5 class="docSection3Title"> parser<br>symbol<br>token<br>tokenize</H5>
<P class="docText">Interface to Python's internal parser and tokenizer. Although parsing Python source code is arguably a text processing task, the complexities of parsing Python are too specialized for this book.</P>
<A NAME="ch04lev4sec7"></A><H5 class="docSection3Title"> robotparser</H5>
<P class="docText">Examine a <TT>robots.txt</TT> access control file. This file is used by Web servers to indicate the desired behavior of automatic indexers and Web crawlers—all the popular search engines honor these requests.</P>
<A NAME="ch04lev4sec8"></A><H5 class="docSection3Title"> sgmllib</H5>
<P class="docText">A partial parser for SGML. Standard Generalized Markup Language (SGML) is an enormously complex document standard; in its full generality, SGML cannot be considered a <span class="docEmphasis">format,</span> but rather a grammar for describing concrete formats. HTML is one particular SGML dialect, and XML is (almost) a simplified subset of SGML.</P>
<P class="docText">Although it might be nice to have a Python library that handled generic SGML, <span class="docEmphasis"><TT>sgmllib</TT></span> is not such a thing. Instead, <span class="docEmphasis"><TT>sgmllib</TT></span> implements just enough SGML parsing to support HTML parsing with <span class="docEmphasis"><TT>htmllib</TT></span>. You might be able to coax parsing an XML library out of <span class="docEmphasis"><TT>sgmllib</TT></span>, with some work, but Python's standard XML tools are far more refined for this purpose.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
htmllib <span class="docEmphasis">285;</span> xml.sax <span class="docEmphasis">405;</span></p>
<A NAME="ch04lev4sec9"></A><H5 class="docSection3Title"> shlex</H5>
<P class="docText">A lexical analyzer class for simple Unix shell-like syntaxes. This capability is primarily useful to implement small command language within Python applications.</P>
<A NAME="ch04lev4sec10"></A><H5 class="docSection3Title"> tabnanny</H5>
<P class="docText">This module is generally used as a command-line script rather than imported into other applications. The module/script <span class="docEmphasis"><TT>tabnanny</TT></span> checks Python source code files for mixed use of tabs and spaces within the same block. Behind the scenes, the Python source is fully tokenized, but normal usage consists of something like:</P>
<pre>
% /sw/lib/python2.2/tabnanny.py SCRIPTS/
SCRIPTS/cmdline.py 165 '\treturn 1\r\n'
'SCRIPTS/HTMLParser_stack.py': Token Error: ('EOF in
                                multi-line string', (3, 7))
SCRIPTS/outputters.py 18 '\tself.writer=writer\r\n'
SCRIPTS/txt2bookU.py 148 '\ttry:\n'
</pre>
<P class="docText">The tool is single purpose, but that purpose addresses a common pitfall in Python programming.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
tokenize <span class="docEmphasis">285;</span></p>
<A NAME="ch04lev2sec15"></A><H4 class="docSection2Title">4.3.2 Low-Level State Machine Parsing</H4>
<P><A NAME="ch04sb01"></A><TABLE CELLSPACING="0" WIDTH="90%" BORDER="1"><TR><TD>
<P class="docText"><span class="docEmphStrong">mx.TextTools</span> &#8226; <span class="docEmphStrong">Fast Text Manipulation Tools</span></P></TD></TR></TABLE></P>
<P class="docText">Marc-Andre Lemburg's <span class="docEmphasis"><TT>mx.TextTools</TT></span> is a remarkable tool that is a bit difficult to grasp the gestalt of. <span class="docEmphasis"><TT>mx.TextTools</TT></span> can be blazingly fast and extremely powerful. But at the same time, as difficult as it might be to "get" the mindset of <span class="docEmphasis"><TT>mx.TextTools</TT></span>, it is still more difficult to get an application written with it working just right. Once it is working, an application that utilizes <span class="docEmphasis"><TT>mx.TextTools</TT></span> can process a larger class of text structures than can regular expressions, while simultaneously operating much faster. But debugging an <span class="docEmphasis"><TT>mx.TextTools</TT></span> "tag table" can make you wish you were merely debugging a cryptic regular expression!</P>
<P class="docText">In recent versions, <span class="docEmphasis"><TT>mx.TextTools</TT></span> has come in a larger package with eGenix.com's several other "mx Extensions for Python." Most of the other subpackages add highly efficient C implementations of datatypes not found in a base Python system.</P>
<P class="docText"><span class="docEmphasis"><TT>mx.TextTools</TT></span> stands somewhere between a state machine and a full-fledged parser. In fact, the module <span class="docEmphasis"><TT>SimpleParse</TT></span>, discussed below, is an EBNF parser library that is built on top of <span class="docEmphasis"><TT>mx.TextTools</TT></span>. As a state machine, <span class="docEmphasis"><TT>mx.TextTools</TT></span> feels like a lower-level tool than the <span class="docEmphasis"><TT>statemachine</TT></span> module presented in the prior section. And yet, <span class="docEmphasis"><TT>mx.TextTools</TT></span> is simultaneously very close to a high-level parser. This is how Lemburg characterizes it in the documentation accompanying <span class="docEmphasis"><TT>mx.TextTools</TT></span>:</P>
<blockquote>
<p class="docText">mxTextTools is an extension package for Python that provides several useful functions and types that implement high-performance text manipulation and searching algorithms in addition to a very flexible and extendable state machine, the Tagging Engine, that allows scanning and processing text based on low-level byte-code "programs" written using Python tuples. It gives you access to the speed of C without the need to do any compile and link steps every time you change the parsing description.</p>
<p class="docText">Applications include parsing structured text, finding and extracting text (either exact or using translation tables) and recombining strings to form new text.</p></blockquote>
<P class="docText">The Python standard library has a good set of text processing tools. The basic tools are powerful, flexible, and easy to work with. But Python's basic text processing is <span class="docEmphasis">not</span> particularly fast. Mind you, for most problems, Python by itself is as fast as you need. But for a certain class of problems, being able to choose <span class="docEmphasis"><TT>mx.TextTools</TT></span> is invaluable.</P>
<P class="docText">The unusual structure of <span class="docEmphasis"><TT>mx.TextTools</TT></span> applications warrants some discussion of concrete usage. After a few sample applications are presented, a listing of <span class="docEmphasis"><TT>mx.TextTools</TT></span> constants, commands, modifiers, and functions is given.</P>
<A NAME="ch04lev3sec1"></A><H5 class="docSection3Title"> BENCHMARKS</H5>
<P class="docText">A familiar computer-industry paraphrase of Mark Twain (who repeats Benjamin Disraeli) dictates that there are "Lies, Damn Lies, and Benchmarks." I will not argue with that and certainly do not want readers to put too great an import on the timings suggested. Nonetheless, in exploring <span class="docEmphasis"><TT>mx.TextTools</TT></span>, I wanted to get some sense of just how fast it is. So here is a rough idea.</P>
<P class="docText">The second example below presents part of a reworked version of the state machine-based <TT>Txt2Html</TT> application reproduced in <A class="docLink" HREF="0321112547_app04.html#app04">Appendix D</A>. The most time-consuming aspect of <TT>Txt2Html</TT> is the regular expression replacements performed in the function <TT>Typography()</TT> for smart ASCII inline markup of words and phrases.</P>
<P class="docText">In order to get a timeable test case, I concatenated 110 copies of an article I wrote to get a file a bit over 2MB, and about 41k lines and 300k words. My test processes an entire input as one text block, first using an <span class="docEmphasis"><TT>mx.TextTools</TT></span> version of <TT>Typography()</TT>, then using the <span class="docEmphasis"><TT>re</TT></span> version.</P>
<P class="docText">Processing time of the same test file went from about 34 seconds to about 12 seconds on one slowish Linux test machine (running Python 1.5.2). In other words, <span class="docEmphasis"><TT>mx.TextTools</TT></span> gave me about a 3x speedup over what I get with the <span class="docEmphasis"><TT>re</TT></span> module. This speedup is probably typical, but particular applications might gain significantly more or less from use of <span class="docEmphasis"><TT>mx.TextTools</TT></span>. Moreover, 34 seconds is a long time in an interactive application, but is not very long at all for a batch process done once a day, or once a week.</P>
<A NAME="ch04lev5sec2"></A><H5 class="docSection4Title"> Example: Buyer/Order Report Parsing</H5>
<P class="docText">Recall (or refer to) the sample report presented in the previous section "<A class="docLink" HREF="0321112547_ch04lev1sec2.html#ch04lev1sec2">An Introduction to State Machines</A>." A report contained a mixture of header material, buyer orders, and comments. The state machine we used looked at each successive line of the file and decided based on context whether the new line indicated a new state should start. It would be possible to write almost the same algorithm utilizing <span class="docEmphasis"><TT>mx.TextTools</TT></span> only to speed up the decisions, but that is not what we will do.</P>
<P class="docText">A more representative use of <span class="docEmphasis"><TT>mx.TextTools</TT></span> is to produce a concrete parse tree of the interesting components of the report document. In principle, you should be able to create a "grammar" that describes every valid "buyer report" document, but in practice using a mixed procedural/grammar approach is much easier, and more maintainable—at least for the test report.</P>
<P class="docText">An <span class="docEmphasis"><TT>mx.TextTools</TT></span> tag table is a miniature state machine that either matches or fails to match a portion of a string. Matching, in this context, means that a "success" end state is reached, while nonmatching means that a "failure" end state is reached. Falling off the end of the tag table is a success state. Each individual state in a tag table tries to match some smaller construct by reading from the "read-head" and moving the read-head correspondingly. On either success or failure, program flow jumps to an indicated target state (which might be a success or failure state for the tag table as a whole). Of course, the jump target for success is often different from the jump target for failure—but there are only these two possible choices for jump targets, unlike the <span class="docEmphasis"><TT>statemachine</TT></span> module's indefinite number.</P>
<P class="docText">Notably, one of the types of states you can include in a tag table is another tag table. That one state can "externally" look like a simple match attempt, but internally it might involve complex subpatterns and machine flow in order to determine if the state is a match or nonmatch. Much as in an EBNF grammar, you can build nested constructs for recognition of complex patterns. States can also have special behavior, such as function callbacks—but in general, an <span class="docEmphasis"><TT>mx.TextTools</TT></span> tag table state is simply a binary match/nonmatch switch.</P>
<P class="docText">Let us look at an <span class="docEmphasis"><TT>mx.TextTools</TT></span> parsing application for "buyer reports" and then examine how it works:</P>
<H5 class="docExampleTitle"><A NAME="ch04list14"></A> buyer_report.py</H5>

<PRE>
from mx.TextTools import *

word_set = set(alphanumeric+white+'-')
quant_set = set(number+'kKmM')

item   = ( (None, AllInSet, newline_set, +1),                # 1
           (None, AllInSet, white_set, +1),                  # 2
           ('Prod', AllInSet, a2z_set, Fail),                # 3
           (None, AllInSet, white_set, Fail),                # 4
           ('Quant', AllInSet, quant_set, Fail),             # 5
           (None, WordEnd, '\n', -5) )                       # 6

buyers = ( ('Order', Table,                                  # 1
                  ( (None, WordEnd, '\n&gt;&gt; ', Fail),          # 1.1
                    ('Buyer', AllInSet, word_set, Fail),     # 1.2
                    ('Item', Table, item, MatchOk, +0) ),    # 1.3
                  Fail, +0), )

comments = ( ('Comment', Table,                              # 1
                  ( (None, Word, '\n*', Fail),               # 1.1
                    (None, WordEnd, '*\n', Fail),            # 1.2
                    (None, Skip, -1) ),                      # 1.3
                  +1, +2),
             (None, Skip, +1),                               # 2
             (None, EOF, Here, -2) )                         # 3

def unclaimed_ranges(tagtuple):
    starts = [0] + [tup[2] for tup in tagtuple [1] ]
    stops = [tup[1] for tup in tagtuple[1]] + [tagtuple[2]]
    return zip(starts, stops)

def report2data(s):
    comtuple = tag(s, comments)
    taglist = comtuple[1]
    for beg,end in unclaimed_ranges(comtuple):
        taglist.extend(tag(s, buyers, beg, end)[1])
    taglist.sort(cmp)
    return taglist

if __name__=='__main__':
    import sys, pprint
    pprint.pprint(report2data(sys.stdin.read()))
</PRE>
<P class="docText">Several tag tables are defined in <span class="docEmphasis"><TT>buyer_report:</TT></span> <TT>item</TT>, <TT>buyers</TT>, and <TT>comments</TT>. State machines such as those in each tag table are general matching engines that can be used to identify patterns; after working with <span class="docEmphasis"><TT>mx.TextTools</TT></span> for a while, you might accumulate a library of useful tag tables. As mentioned above, states in tag tables can reference other tag tables, either by name or inline. For example, <TT>buyers</TT> contains an inline tag table, while this inline tag table utilizes the tag table named <TT>item</TT>.</P>
<P class="docText">Let us take a look, step by step, at what the <TT>buyers</TT> tag table does. In order to <span class="docEmphasis">do</span> anything, a tag table needs to be passed as an argument to the <TT>mx.TextTools.tag()</TT> function, along with a string to match against. That is done in the <TT>report2data()</TT> function in the example. But in general, <TT>buyers</TT>—or any tag table—contains a list of states, each containing branch offsets. In the example, all such states are numbered in comments. <TT>buyers</TT> in particular contains just one state, which contains a subtable with three states.</P>
<A NAME="ch04lev5sec3"></A><H5 class="docSection4Title"> Tag table state in <TT>buyers</TT></H5>
<span style="font-weight:bold"><OL class="docList" TYPE="1"><LI><span style="font-weight:normal"><P class="docList">Try to match the subtable. If the match succeeds, add the name <TT>Order</TT> to the taglist of matches. If the match fails, do not add anything. If the match succeeds, jump back into the one state (i.e., +0). In effect, <TT>buyers</TT> loops as long as it succeeds, advancing the read-head on each such match.</P></span></LI></OL></span>
<A NAME="ch04lev5sec4"></A><H5 class="docSection4Title"> Subtable states in <TT>buyers</TT></H5>
<span style="font-weight:bold"><OL class="docList" TYPE="1"><LI><span style="font-weight:normal"><P class="docList">Try to find the end of the "word" \<TT>n&gt;&gt;</TT> in the string. That is, look for two greater-than symbols at the beginning of a line. If successful, move the read-head just past the point that first matched. If this state match fails, jump to <TT>Fail</TT>—that is, the (sub)table as a whole fails to match. No jump target is given for a successful match, so the default jump of +1 is taken. Since <TT>None</TT> is the tag object, do not add anything to the taglist upon a state match.</P></span></LI><LI><span style="font-weight:normal"><P class="docList">Try to find some <TT>word_set</TT> characters. This set of characters is defined in <span class="docEmphasis"><TT>buyer_report</TT></span>; various other sets are defined in <span class="docEmphasis"><TT>mx.TextTools</TT></span> itself. If the match succeeds, add the name <TT>Buyer</TT> to the taglist of matches. As many contiguous characters in the set as possible are matched. The match is considered a failure if there is not at least one such character. If this state match fails, jump to <TT>Fail</TT>, as in state (1).</P></span></LI><LI><span style="font-weight:normal"><P class="docList">Try to match the <TT>item</TT> tag table. If the match succeeds, add the name <TT>Item</TT> to the taglist of matches. What gets added, moreover, includes anything added within the <TT>item</TT> tag table. If the match fails, jump to <TT>MatchOk</TT>—that is, the (sub)table as a whole matches. If the match succeeds, jump +0—that is, keep looking for another <TT>Item</TT> to add to the taglist.</P></span></LI></OL></span>
<P class="docText">What <span class="docEmphasis"><TT>buyer_report</TT></span> actually does is to first identify any comments, then to scan what is left in between comments for buyer orders. This approach proved easier to understand. Moreover, the design of <span class="docEmphasis"><TT>mx.TextTools</TT></span> allows us to do this with no real inefficiency. Tagging a string does not involve actually pulling out the slices that match patterns, but simply identifying numerically the offset ranges where they occur. This approach is much "cheaper" than performing repeated slices, or otherwise creating new strings.</P>
<P class="docText">The following is important to notice: As of version 2.1.0, the documentation of the <TT>mx.TextTools.tag()</TT> function that accompanies <span class="docEmphasis"><TT>mx.TextTools</TT></span> does not match its behavior! If the optional third and fourth arguments are passed to <TT>tag()</TT> they must indicate the start and end offsets within a larger string to scan, <span class="docEmphasis">not</span> the starting offset and length. Hopefully, later versions will fix the discrepancy (either approach would be fine, but could cause some breakage in existing code).</P>
<P class="docText">What <span class="docEmphasis"><TT>buyer_report</TT></span> produces is a data structure, not final output. This data structure looks something like:</P>
<H5 class="docExampleTitle"><A NAME="ch04list15"></A> buyer_report.py data structure</H5>

<PRE>
$ python ex_mx.py &lt; recs.tmp
[('Order', 0,  638,
  [('Buyer', 547, 562, None),
   ('Item', 562, 583,
    [('Prod', 566, 573, None), ('Quant', 579, 582, None)]),
   ('Item', 583, 602,
     [('Prod', 585, 593, None), ('Quant', 597, 601, None)]),
   ('Item', 602, 621,
     [('Prod', 604, 611, None), ('Quant', 616, 620, None)]),
   ('Item', 621, 638,
     [('Prod', 623, 632, None), ('Quant', 635, 637, None)])]),
 ('Comment', 638, 763, []),
 ('Order', 763, 805,
  [('Buyer', 768, 776, None),
   ('Item', 776, 792,
    [('Prod', 778, 785, None), ('Quant', 788, 791, None)]),
   ('Item', 792, 805,
    [('Prod', 792, 800, None), ('Quant', 802, 804, None)])]),
 ('Order', 805, 893,
  [('Buyer', 809, 829, None),
   ('Item', 829, 852,
    [('Prod', 833, 840, None), ('Quant', 848, 851, None)]),
   ('Item', 852, 871,
    [('Prod', 855, 863, None), ('Quant', 869, 870, None)]),
   ('Item', 871, 893,
    [('Prod', 874, 879, None), ('Quant', 888, 892, None)])]),
 ('Comment', 893, 952, []),
 ('Comment', 952, 1025, []),
 ('Comment', 1026, 1049, []),
 ('Order', 1049, 1109,
  [('Buyer', 1054, 1069, None),
   ('Item',1069, 1109,
    [('Prod', 1070, 1077, None), ('Quant', 1083, 1086, None)])])]
</PRE>
<P class="docText">While this is "just" a new data structure, it is quite easy to deal with compared to raw textual reports. For example, here is a brief function that will create well-formed XML out of any taglist. You could even arrange for it to be valid XML by designing tag tables to match DTDs (see <A class="docLink" HREF="0321112547_ch05.html#ch05">Chapter 5</A> for details about XML, DTDs, etc.):</P>
<pre>
def taglist2xml(s, taglist, root):
    print '&lt;%s&gt;' % root
    for tt in taglist:
        if tt[3] :
            taglist2xml(s, tt[3], tt[0])
        else:
            print '&lt;%s&gt;%s&lt;/%s&gt;' % (tt[0], s[tt[1]:tt[2]], tt[0])
    print '&lt;/%s&gt;' % root
</pre>
<A NAME="ch04lev5sec5"></A><H5 class="docSection4Title"> Example: Marking up smart ASCII</H5>
<P class="docText">The "smart ASCII" format uses email-like conventions to lightly mark features like word emphasis, source code, and URL links. This format—with <IMG BORDER="0" WIDTH="40" HEIGHT="17" src="FILES/latex.gif" ALT="graphics/latex.gif"> as an intermediate format—was used to produce the book you hold (which was written using a variety of plaintext editors). By obeying just a few conventions (that are almost the same as you would use on Usenet or in email), a writer can write without much clutter, but still convert to production-ready markup.</P>
<P class="docText">The <TT>Txt2Html</TT> utility uses a block-level state machine, combined with a collection of inline-level regular expressions, to identify and modify markup patterns in smart ASCII texts. Even though Python's regular expression engine is moderately slow, converting a five-page article takes only a couple seconds. In practice, <TT>Txt2Html</TT> is more than adequate for my own 20 kilobyte documents. However, it is easy to imagine a not-so-different situation where you were converting multimegabyte documents and/or delivering such dynamically converted content on a high-volume Web site. In such a case, Python's string operations, and especially regular expressions, would simply be too slow.</P>
<P class="docText"><span class="docEmphasis"><TT>mx.TextTools</TT></span> can do everything regular expressions can, plus some things regular expressions cannot. In particular, a taglist can contain recursive references to matched patterns, which regular expressions cannot. The utility <TT>mxTypography.py</TT> utilizes several <span class="docEmphasis"><TT>mx.TextTools</TT></span> capabilities the prior example did not use. Rather than create a nested data structure, <TT>mxTypography.py</TT> utilizes a number of callback functions, each responding to a particular match event. As well, <TT>mxTypography.py</TT> adds some important debugging techniques. Something similar to these techniques is almost required for tag tables that are likely to be updated over time (or simply to aid the initial development). Overall, this looks like a robust application should.</P>
<H5 class="docExampleTitle"><A NAME="ch04list16"></A> mx.TextTools version of Typography()</H5>

<PRE>
from mx.TextTools import *
import string, sys

#-- List of all words with  markup, head position, loop count
ws, head_pos, loops = [], None, 0

#-- Define "emitter" callbacks for each output format
def emit_misc(tl,txt,l,r,s):
    ws.append(txt[l:r])
def emit_func(tl,txt,l,r,s):
    ws.append('&lt;code&gt;'+txt[l+1:r-1]+'&lt;/code&gt;')
def emit_modl(tl,txt,l,r,s):
    ws.append('&lt;em&gt;&lt;code&gt;'+txt[l+1:r-1]+'&lt;/code&gt;&lt;/em&gt;')
def emit_emph(tl,txt,l,r,s):
    ws.append('&lt;em&gt;'+txt[l+1:r-1]+'&lt;/em&gt;')
def emit_strg(tl,txt,l,r,s):
    ws.append('&lt;strong&gt;'+txt[l+1:r-1]+'&lt;/strong&gt;')
def emit_titl(tl,txt,l,r,s):
    ws.append('&lt;cite&gt;'+txt[l+1:r-1]+'&lt;/cite&gt;')
def jump_count(tl,txt,l,r,s):
    global head_pos, loops
    loops = loops+1
    if head_pos is None: head_pos = r
    elif head_pos == r:
        raise "InfiniteLoopError", \
              txt[l-20:l]+'{'+txt[l]+'}'+txt[l+1:r+15]
    else: head_pos = r

#-- What can appear inside, and what can be, markups?
punct_set = set("'!@#$%^&amp;*()_-+=|\{}[]:;'&lt;&gt;,.?/"+'"')
markable = alphanumeric+whitespace+"'!@#$%^&amp;()+= |\{}:;&lt;&gt;,.?/"+'"'
markable_func = set(markable+"*-_[]")
markable_modl = set(markable+"*-_'")
markable_emph = set(markable+"*_'[]")
markable_strg = set(markable+"-_'[]")
markable_titl = set(markable+"*-'[]")
markup_set    = set("-*'[]_")

#-- What can precede and follow markup phrases?
darkins = '(/"'
leadins = whitespace+darkins      # might add from "-*'[]_"
darkouts = '/.),:;?!"'
darkout_set = set(darkouts)
leadouts = whitespace+darkouts    # for non-conflicting markup
leadout_set = set(leadouts)

#-- What can appear inside plain words?
word_set = set(alphanumeric+'{}/@#$%^&amp;-_+= |\&gt;&lt;'+darkouts)
wordinit_set = set(alphanumeric+"$#+\&lt;.&amp;{"+darkins)

#-- Define the word patterns (global so as to do it only at import)
# Special markup
def markup_struct(lmark, rmark, callback, markables, x_post="-"):
    struct = \
      ( callback, Table+CallTag,
        ( (None, Is, lmark),                 # Starts with left marker
          (None, AllInSet, markables),       # Stuff marked
          (None, Is, rmark),                 # Ends with right marker
          (None, IsInSet, leadout_set,+2,+1),# EITHR: postfix w/ leadout
          (None, Skip, -1,+1, MatchOk),      # ..give back trailng ldout
          (None, IsIn, x_post, MatchFail),   # OR: special case postfix
          (None, Skip, -1,+1, MatchOk)       # ..give back trailing char
        )
      )
    return struct
funcs   = markup_struct("'", "'", emit_func, markable_func)
modules = markup_struct("[", "]", emit_modl, markable_modl)
emphs   = markup_struct("-", "-", emit_emph, markable_emph, x_post="")
strongs = markup_struct("*", "*", emit_strg, markable_strg)
titles  = markup_struct("_", "_", emit_titl, markable_titl)

# All the stuff not specially marked
plain_words = \
 ( ws, Table+AppendMatch,           # AppendMatch only -slightly
   ( (None, IsInSet,                # faster than emit_misc callback
        wordinit_set, MatchFail),   # Must start with word-initial
     (None, Is, "'",+1),            # May have apostrophe next
     (None, AllInSet, word_set,+1), # May have more word-internal
     (None, Is, "'", +2),           # May have trailing apostrophe
     (None, IsIn, "st",+1),         # May have [ts] after apostrophe
     (None, IsInSet,
        darkout_set,+1, MatchOk),   # Postfixed with dark lead-out
     (None, IsInSet,
        whitespace_set, MatchFail), # Give back trailing whitespace
     (None, Skip, -1)
   ) )
# Catch some special cases
bullet_point = \
 ( ws, Table+AppendMatch,
   ( (None, Word+CallTag, "* "),       # Asterisk bullet is a word
   ) )
horiz_rule = \
 ( None, Table,
   ( (None, Word, "-"*50),             # 50 dashes in a row
     (None, AllIn, "-"),               # More dashes
   ) )
into_mark = \
 ( ws, Table+AppendMatch,             # Special case where dark leadin
   ( (None, IsInSet, set(darkins)),   #   is followed by markup char
     (None, IsInSet, markup_set),
     (None, Skip, -1)                 # Give back the markup char
   ) )
stray_punct = \
 ( ws, Table+AppendMatch,              # Pickup any cases where multiple
   ( (None, IsInSet, punct_set),       # punctuation character occur
     (None, AllInSet, punct_set),      # alone (followed by whitespace)
     (None, IsInSet, whitespace_set),
     (None, Skip, -1)                  # Give back the whitespace
   ) )
leadout_eater = (ws, AllInSet+AppendMatch, leadout_set)

#-- Tag all the (possibly marked-up) words
tag_words = \
 ( bullet_point+(+1,),
   horiz_rule + (+1,),
   into_mark  + (+1,),
   stray_punct+ (+1,),
   emphs   + (+1,),
   funcs   + (+1,),
   strongs + (+1,),
   modules + (+1,),
   titles  + (+1,),
   into_mark+(+1,),
   plain_words +(+1,),             # Since file is mstly plain wrds, can
   leadout_eater+(+1,-1),          # shortcut by tight looping (w/ esc)
   (jump_count, Skip+CallTag, 0),  # Check for infinite loop
   (None, EOF, Here, -13)          # Check for EOF
 )
def Typography(txt):
    global ws
    ws = []    # clear the list before we proceed
    tag(txt, tag_words, 0, len(txt), ws)
    return string.join(ws, '')

if __name__ == '__main__':
    print Typography(open(sys.argv[1]).read())
</PRE>
<P class="docText"><TT>mxTypographify.py</TT> reads through a string and determines if the next bit of text matches one of the markup patterns in <TT>tag_words</TT>. Or rather, it better match some pattern or the application just will not know what action to take for the next bit of text. Whenever a named subtable matches, a callback function is called, which leads to a properly annotated string being appended to the global list <TT>ws</TT>. In the end, all such appended strings are concatenated.</P>
<P class="docText">Several of the patterns given are mostly fallback conditions. For example, the <TT>stray_punct</TT> tag table detects the condition where the next bit of text is some punctuation symbols standing alone without abutting any words. In most cases, you don't <span class="docEmphasis">want</span> smart ASCII to contain such a pattern, but <span class="docEmphasis"><TT>mxTypographify</TT></span> has to do <span class="docEmphasis">something</span> with them if they are encountered.</P>
<P class="docText">Making sure that every subsequence is matched by some subtable or another is tricky. Here are a few examples of matches and failures for the <TT>stray_punct</TT> subtable. Everything that does not match this subtable needs to match some other subtable instead:</P>
<pre>
-- spam      # matches "--"
&amp; spam       # fails at "AllInSet" since '&amp;' advanced head
#@$ %% spam  # matches "#@$"
**spam       # fails (whitespace isn't encountered before 's')
</pre>
<P class="docText">After each success, the read-head is at the space right before the next word "spam" or "%%". After a failure, the read-head remains where it started out (at the beginning of the line).</P>
<P class="docText">Like <TT>stray_punct</TT>, <TT>emphs</TT>, <TT>funcs</TT>, <TT>strongs</TT>, <TT>plain_words</TT>, et cetera contain tag tables. Each entry in <TT>tag_words</TT> has its appropriate callback functions (all "emitters" of various names, because they "emit" the match, along with surrounding markup if needed). Most lines each have a "+1" appended to their tuple; what this does is specify where to jump in case of a match failure. That is, even if these patterns fail to match, we continue on—with the read-head in the same position—to try matching against the other patterns.</P>
<P class="docText">After the basic word patterns each attempt a match, we get to the "leadout eater" line. For <TT>mxTypography.py</TT>, a "leadout" is the opposite of a "leadin." That is, the latter are things that might precede a word pattern, and the former are things that might follow a word pattern. The <TT>leadout_set</TT> includes whitespace characters, but it also includes things like a comma, period, and question mark, which might end a word. The "leadout eater" uses a callback function, too. As designed, it preserves exactly the whitespace the input has. However, it would be easy to normalize whitespace here by emitting something other than the actual match (e.g., a single space always).</P>
<P class="docText">The <TT>jump_count</TT> is extremely important; we will come back to it momentarily. For now, it is enough to say that we <span class="docEmphasis">hope</span> the line never does anything.</P>
<P class="docText">The <TT>EOF</TT> line is our flow control, in a way. The call made by this line is to <TT>None</TT>, which is to say that nothing is actually <span class="docEmphasis">done</span> with any match. The command <TT>EOF</TT> is the important thing (<TT>Here</TT> is just a filler value that occupies the tuple position). It succeeds if the read-head is past the end of the read buffer. On success, the whole tag table <TT>tag_words</TT> succeeds, and having succeeded, processing stops. <TT>EOF</TT> failure is more interesting. Assuming we haven't reached the end of our string, we jump -13 states (to <TT>bullet_point</TT>). From there, the whole process starts over, hopefully with the read-head advanced to the next word. By looping back to the start of the list of tuples, we continue eating successive word patterns until the read buffer is exhausted (calling callbacks along the way).</P>
<P class="docText">The <TT>tag()</TT> call simply launches processing of the tag table we pass to it (against the read buffer contained in <TT>txt</TT>). In our case, we do not care about the return value of <TT>tag()</TT> since everything is handled in callbacks. However, in cases where the tag table does not loop itself, the returned tuple can be used to determine if there is reason to call <TT>tag()</TT> again with a tail of the read buffer.</P>
<A NAME="ch04lev3sec2"></A><H5 class="docSection3Title"> DEBUGGING A TAG TABLE</H5>
<P class="docText">Describing it is easy, but I spent a large number of hours finding the exact collection of tag tables that would match every pattern I was interested in without mismatching any pattern as something it wasn't. While smart ASCII markup seems pretty simple, there are actually quite a few complications (e.g., markup characters being used in nonmarkup contexts, or markup characters and other punctuation appearing in various sequences). Any structured document format that is complicated enough to warrant using <span class="docEmphasis"><TT>mx.TextTools</TT></span> instead of <span class="docEmphasis"><TT>string</TT></span> is likely to have similar complications.</P>
<P class="docText">Without question, the worst thing that can go wrong in a looping state pattern like the one above is that <span class="docEmphasis">none</span> of the listed states match from the current read-head position. If that happens, your program winds up in a tight infinite loop (entirely inside the extension module, so you cannot get at it with Python code directly). I wound up forcing a manual kill of the process <span class="docEmphasis">countless</span> times during my first brush at <span class="docEmphasis"><TT>mx.TextTools</TT></span> development.</P>
<P class="docText">Fortunately, there is a solution to the infinite loop problem. This is to use a callback like <TT>jump_count</TT>.</P>
<H5 class="docExampleTitle"><A NAME="ch04list17"></A> mxTypography.py infinite loop catcher</H5>

<PRE>
def jump_count(taglist,txt,l,r,subtag):
    global head_pos
    if head_pos is None: head_pos = r
    elif head_pos == r:
        raise "InfiniteLoopError", \
              txt[1-20:1]+'{'+txt[1]+'}'+txt[l+1:r+15]
    else: head_pos = r
</PRE>
<P class="docText">The basic purpose of <TT>jump_count</TT> is simple: We want to catch the situation where our tag table has been run through multiple times without matching anything. The simplest way to do this is to check whether the last read-head position is the same as the current. If it is, more loops cannot get anywhere, since we have reached the exact same state twice, and the same thing is fated to happen forever. <TT>mxTypography.py</TT> simply raises an error to stop the program (and reports a little bit of buffer context to see what is going on).</P>
<P class="docText">It is also possible to move the read-head manually and try again from a different starting position. To manipulate the read head in this fashion, you could use the <TT>Call</TT> command in tag table items. But a better approach is to create a nonlooping tag table that is called repeatedly from a Python loop. This Python loop can look at a returned tuple and use adjusted offsets in the next call if no match occurred. Either way, since much more time is spent in Python this way than with the loop tag table approach, less speed would be gained from <span class="docEmphasis"><TT>mx.TextTools</TT></span>.</P>
<P class="docText">Not as bad as an infinite loop, but still undesirable, is having patterns within a tag table match when they are not supposed to or not match when they are suppose to (but something else has to match, or we would have an infinite loop issue). Using callbacks everywhere makes examining this situation much easier. During development, I frequently create temporary changes to my <TT>emit_*</TT> callbacks to print or log when certain emitters get called. By looking at output from these temporary <TT>print</TT> statements, most times you can tell where the problem lies.</P>
<A NAME="ch04lev3sec3"></A><H5 class="docSection3Title"> CONSTANTS</H5>
<P class="docText">The <span class="docEmphasis"><TT>mx.TextTools</TT></span> module contains constants for a number of frequently used collections of characters. Many of these character classes are the same as ones in the <span class="docEmphasis"><TT>string</TT></span> module. Each of these constants also has a <TT>set</TT> version predefined; a set is an efficient representation of a character class that may be used in tag tables and other <span class="docEmphasis"><TT>mx.TextTools</TT></span> functions. You may also obtain a character set from a (custom) character class using the <span class="docEmphasis"><TT>mx.TextTools.set()</TT></span> function:</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import a2z, set
&gt;&gt;&gt; varname_chars = a2z + '_'
&gt;&gt;&gt; varname_set = set(varname_chars)
</pre>
<A NAME="ch04lev4sec11"></A><H5 class="docSection4Title"> mx.TextTools.a2z<br>mx.TextTools.a2z_set</H5>
<P class="docText">English lowercase letters ("abcdefghijklmnopqrstuvwxyz").</P>
<A NAME="ch04lev4sec12"></A><H5 class="docSection4Title"> mx.TextTools.A2Z<br>mx.TextTools.A2Z_set</H5>
<P class="docText">English uppercase letters ("ABCDEFGHIJKLMNOPQRSTUVWXYZ").</P>
<A NAME="ch04lev4sec13"></A><H5 class="docSection4Title"> mx.TextTools.umlaute<br>mx.TextTools.umlaute_set</H5>
<P class="docText">Extra German lowercase hi-bit characters.</P>
<A NAME="ch04lev4sec14"></A><H5 class="docSection4Title"> mx.TextTools.Umlaute<br>mx.TextTools.Umlaute_set</H5>
<P class="docText">Extra German uppercase hi-bit characters.</P>
<A NAME="ch04lev4sec15"></A><H5 class="docSection4Title"> mx.TextTools.alpha<br>mx.TextTools.alpha_set</H5>
<P class="docText">English letters (A2Z + a2z).</P>
<A NAME="ch04lev4sec16"></A><H5 class="docSection4Title"> mx.TextTools.german_alpha<br>mx.TextTools.german_alpha_set</H5>
<P class="docText">German letters (A2Z + a2z + umlaute + Umlaute).</P>
<A NAME="ch04lev4sec17"></A><H5 class="docSection4Title"> mx.TextTools.number<br>mx.TextTools.number_set</H5>
<P class="docText">The decimal numerals ("0123456789").</P>
<A NAME="ch04lev4sec18"></A><H5 class="docSection4Title"> mx.TextTools.alphanumeric<br>mx.TextTools.alphanumeric_set</H5>
<P class="docText">English numbers and letters (alpha + number).</P>
<A NAME="ch04lev4sec19"></A><H5 class="docSection4Title"> mx.TextTools.white<br>mx.TextTools.white_set</H5>
<P class="docText">Spaces and tabs (" \t\v"). This is more restricted than <span class="docEmphasis"><TT>string.whitespace</TT></span>.</P>
<A NAME="ch04lev4sec20"></A><H5 class="docSection4Title"> mx.TextTools.newline<br>mx.TextTools.newline_set</H5>
<P class="docText">Line break characters for various platforms ("\n\r").</P>
<A NAME="ch04lev4sec21"></A><H5 class="docSection4Title"> mx.TextTools.formfeed<br>mx.TextTools.formfeed_set</H5>
<P class="docText">Formfeed character ("\f").</P>
<A NAME="ch04lev4sec22"></A><H5 class="docSection4Title"> mx.TextTools.whitespace<br>mx.TextTools.whitespace_set</H5>
<P class="docText">Same as <span class="docEmphasis"><TT>string.whitespace</TT></span> <TT>(white+newline+formfeed)</TT>.</P>
<A NAME="ch04lev4sec23"></A><H5 class="docSection4Title"> mx.TextTools.any<br>mx.TextTools.any_set</H5>
<P class="docText">All characters (0x00-0xFF).</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.digits <span class="docEmphasis">130</span>; string.hexdigits <span class="docEmphasis">130</span>; string.octdigits <span class="docEmphasis">130</span>; string.lowercase <span class="docEmphasis">131</span>; string.uppercase <span class="docEmphasis">131</span>; string.letters <span class="docEmphasis">131</span>; string.punctuation <span class="docEmphasis">131</span>; string.whitespace <span class="docEmphasis">131</span>; string.printable <span class="docEmphasis">132</span>;</p>
<A NAME="ch04lev3sec4"></A><H5 class="docSection3Title"> COMMANDS</H5>
<P class="docText">Programming in <span class="docEmphasis"><TT>mx.TextTools</TT></span> amounts mostly to correctly configuring tag tables. Utilizing a tag table requires just one call to the <span class="docEmphasis"><TT>mx.TextTools.tag()</TT></span>, but inside a tag table is a kind of mini-language—something close to a specialized Assembly language, in many ways.</P>
<P class="docText">Each tuple within a tag table contains several elements, of the form:</P>
<pre>
(tagobj, command[+modifiers], argument
         [,jump_no_match=MatchFail [,jump_match=+l]])
</pre>
<P class="docText">The "tag object" may be <TT>None</TT>, a callable object, or a string. If <TT>tagobj</TT> is <TT>None</TT>, the indicated pattern may match, but nothing is added to a taglist data structure if so, nor is a callback invoked. If a callable object (usually a function) is given, it acts as a callback for a match. If a string is used, it is used to name a part of the taglist data structure returned by a call to <span class="docEmphasis"><TT>mx.TextTools.tag()</TT></span>.</P>
<P class="docText">A command indicates a type of pattern to match, and a modifier can change the behavior that occurs in case of such a match. Some commands succeed or fail unconditionally, but allow you to specify behaviors to take if they are reached. An argument is required, but the specific values that are allowed and how they are interpreted depends on the command used.</P>
<P class="docText">Two jump conditions may optionally be specified. If no values are given, <TT>jump_no_match</TT> defaults to <TT>MatchFail</TT>—that is, unless otherwise specified, failing to match a tuple in a tag table causes the tag table as a whole to fail. If a value <span class="docEmphasis">is</span> given, <TT>jump_no_match</TT> branches to a tuple the specified number of states forward or backward. For clarity, an explicit leading "+" is used in forward branches. Branches backward will begin with a minus sign. For example:</P>
<pre>
# Branch forward one state if next character -is not- an X
# ... branch backward three states if it is an X
tupX = (None, Is, 'X', +1, -3)
# assume all the tups are defined somewhere...
tagtable = (tupA, tupB, tupV, tupW, tupX, tupY, tupZ)
</pre>
<P class="docText">If no value is given for <TT>jump_match</TT>, branching is one state forward in the case of a match.</P>
<P class="docText">Version 2.1.0 of <span class="docEmphasis"><TT>mx.TextTools</TT></span> adds named jump targets, which are often easier to read (and maintain) than numeric offsets. An example is given in the <span class="docEmphasis"><TT>mx.TextTools</TT></span> documentation:</P>
<pre>
tag_table = ('start',
             ('lowercase',AllIn,a2z,+1,'skip'),
             ('upper',AllIn,A2Z,'skip'),
             'skip',
             (None,AllIn,white+newline,+1),
             (None,AllNotIn,alpha+white+newline,+1),
             (None,EOF,Here,'start') )
</pre>
<P class="docText">It is easy to see that if you were to add or remove a tuple, it is less error prone to retain a jump to, for example, <TT>skip</TT> than to change every necessary <TT>+2</TT> to a <TT>+3</TT> or the like.</P>
<A NAME="ch04lev3sec5"></A><H5 class="docSection3Title"> UNCONDITIONAL COMMANDS</H5>
<A NAME="ch04lev4sec24"></A><H5 class="docSection4Title"> mx.TextTools.Fail<br>mx.TextTools.Jump</H5>
<P class="docText">Nonmatch at this tuple. Used mostly for documentary purposes in a tag table, usually with the <TT>Here</TT> or <TT>To</TT> placeholder. The tag tables below are equivalent:</P>
<pre>
table1 = ( ('foo', Is, 'X', MatchFail, MatchOk), )
table2 = ( ('foo', Is, 'X', +1, +2),
           ('Not_X', Fail, Here) )
</pre>
<P class="docText">The <TT>Fail</TT> command may be preferred if several other states branch to the same failure, or if the condition needs to be documented explicitly.</P>
<P class="docText"><TT>Jump</TT> is equivalent to <TT>Fail</TT>, but it is often better self-documenting to use one rather than the other; for example:</P>
<pre>
tup1 = (None, Fail, Here, +3)
tup2 = (None, Jump, To, +3)
</pre>
<A NAME="ch04lev4sec25"></A><H5 class="docSection4Title"> mx.TextTools.Skip<br>mx.TextTools.Move</H5>
<P class="docText">Match at this tuple, and change the read-head position. <TT>Skip</TT> moves the read-head by a relative amount, <TT>Move</TT> to an absolute offset (within the slice the tag table is operating on). For example:</P>
<pre>
# read-head forward 20 chars, jump to next state
tup1 = (None, Skip, 20)
# read-head to position 10, and jump back 4 states
tup2 = (None, Move, 10, 0, -4)
</pre>
<P class="docText">Negative offsets are allowed, as in Python list indexing.</P>
<A NAME="ch04lev3sec6"></A><H5 class="docSection3Title"> MATCHING PARTICULAR CHARACTERS</H5>
<A NAME="ch04lev4sec26"></A><H5 class="docSection4Title"> mx.TextTools.AllIn<br>mx.TextTools.AllInSet<br>mx.TextTools.AllInCharSet</H5>
<P class="docText">Match all characters up to the first that is not included in <TT>argument</TT>. <TT>AllIn</TT> uses a character string while <TT>AllInSet</TT> uses a set as <TT>argument</TT>. For version 2.1.0, you may also use <TT>AllInCharSet</TT> to match <TT>CharSet</TT> objects. In general, the set or CharSet form will be faster and is preferable. The following are functionally the same:</P>
<pre>
tup1 = ('xyz', AllIn, 'XYZxyz')
tup2 = ('xyz', AllInSet, set('XYZxyz')
tup3 = ('xyz', AllInSet, CharSet('XYZxyz'))
</pre>
<P class="docText">At least one character must match for the tuple to match.</P>
<A NAME="ch04lev4sec27"></A><H5 class="docSection4Title"> mx.TextTools.AIINotIn</H5>
<P class="docText">Match all characters up to the first that <span class="docEmphasis">is</span> included in <TT>argument</TT>. As of version 2.1.0, <span class="docEmphasis"><TT>mx.TextTools</TT></span> does not include an <TT>AllNotInSet</TT> command. However, the following tuples are functionally the same (the second usually faster):</P>
<pre>
from mx.TextTools import AllNotIn, AllInSet, invset
tup1 = ('xyz', AllNotIn, 'XYZxyz')
tup2 = ('xyz', AllInSet, invset('xyzXYZ'))
</pre>
<P class="docText">At least one character must match for the tuple to match.</P>
<A NAME="ch04lev4sec28"></A><H5 class="docSection4Title"> mx.TextTools.ls</H5>
<P class="docText">Match specified character. For example:</P>
<pre>
tup = ('X', Is, 'X')
</pre>
<A NAME="ch04lev4sec29"></A><H5 class="docSection4Title"> mx.TextTools.IsNot</H5>
<P class="docText">Match any one character except the specified character.</P>
<pre>
tup = ('X', IsNot, 'X')
</pre>
<A NAME="ch04lev4sec30"></A><H5 class="docSection4Title"> mx.TextTools.IsIn<br>mx.TextToo1s.IsInSet<br>mx.TextTools.IsInCharSet</H5>
<P class="docText">Match exactly one character if it is in <TT>argument</TT>. <TT>IsIn</TT> uses a character string while <TT>IsInSet</TT> use a set as <TT>argument</TT>. For version 2.1.0, you may also use <TT>IsInCharSet</TT> to match <TT>CharSet</TT> objects. In general, the set or CharSet form will be faster and is preferable. The following are functionally the same:</P>
<pre>
tup1 = ('xyz', IsIn, 'XYZxyz')
tup2 = ('xyz', IsInSet, set('XYZxyz')
tup3 = ('xyz', IsInSet, CharSet('XYZxyz')
</pre>
<A NAME="ch04lev4sec31"></A><H5 class="docSection4Title"> mx.TextTools.IsNotIn</H5>
<P class="docText">Match exactly one character if it is <span class="docEmphasis">not</span> in <TT>argument</TT>. As of version 2.1.0, <span class="docEmphasis"><TT>mx.TextTools</TT></span> does not include an <TT>'AllNotInSet</TT> command. However, the following tuples are functionally the same (the second usually faster):</P>
<pre>
from mx.TextTools import IsNotIn, IsInSet, invset
tup1 = ('xyz', IsNotIn, 'XYZxyz')
tup2 = ('xyz', IsInSet, invset('xyzXYZ'))
</pre>
<A NAME="ch04lev3sec7"></A><H5 class="docSection3Title"> MATCHING SEQUENCES</H5>
<A NAME="ch04lev4sec32"></A><H5 class="docSection4Title"> mx.TextTools.Word</H5>
<P class="docText">Match a word at the current read-head position. For example:</P>
<pre>
tup = ('spam', Word, 'spam')
</pre>
<A NAME="ch04lev4sec33"></A><H5 class="docSection4Title"> mx.TextTools.WordStart<br>mx.TextTools.sWordStart<br>mx.TextTools.WordEnd<br>mx.TextTools.sWordEnd</H5>
<P class="docText">Search for a word, and match up to the point of the match. Searches performed in this manner are extremely fast, and this is one of the most powerful elements of tag tables. The commands <TT>sWordStart</TT> and <TT>sWordEnd</TT> use "search objects" rather than plaintexts (and are significantly faster).</P>
<P class="docText"><TT>WordStart</TT> and <TT>sWordStart</TT> leave the read-head immediately prior to the matched word, if a match succeeds. <TT>WordEnd</TT> and <TT>sWordEnd</TT> leave the read-head immediately after the matched word. On failure, the read-head is not moved for any of these commands.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import *
&gt;&gt;&gt; s = 'spam and eggs taste good'
&gt;&gt;&gt; tab1 = ( ('toeggs', WordStart, 'eggs'), )
&gt;&gt;&gt; tag(s, tab1)
(1, [('toeggs', 0, 9, None)], 9)
&gt;&gt;&gt; s[0:9]
'spam and '
&gt;&gt;&gt; tab2 = ( ('pasteggs', sWordEnd, BMS('eggs')), )
&gt;&gt;&gt; tag(s, tab2)
(1, [('pasteggs', 0, 13, None)], 13)
&gt;&gt;&gt; s[0:13]
'spam and eggs'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.BMS() <span class="docEmphasis">307</span>; mx.TextTools.sFindWord <span class="docEmphasis">303</span>;</p>
<A NAME="ch04lev4sec34"></A><H5 class="docSection4Title"> mx.TextTools.sFindWord</H5>
<P class="docText">Search for a word, and match only that word. Any characters leading up to the match are ignored. This command accepts a search object as an argument. In case of a match, the read-head is positioned immediately after the matched word.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import *
&gt;&gt;&gt; s = 'spam and eggs taste good'
&gt;&gt;&gt; tab3 = ( ('justeggs', sFindWord, BMS('eggs')), )
&gt;&gt;&gt; tag(s, tab3)
(1, [('justeggs', 9, 13, None)], 13)
&gt;&gt;&gt; s[9:13]
'eggs'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.sWordEnd <span class="docEmphasis">302</span>;</p>
<A NAME="ch04lev4sec35"></A><H5 class="docSection4Title"> mx.TextTools.EOF</H5>
<P class="docText">Match if the read-head is past the end of the string slice. Normally used with placeholder argument <TT>Here</TT>, for example:</P>
<pre>
tup = (None, EOF, Here)
</pre>
<A NAME="ch04lev3sec8"></A><H5 class="docSection3Title"> COMPOUND MATCHES</H5>
<A NAME="ch04lev4sec36"></A><H5 class="docSection4Title"> mx.TextTools.Table<br>mx.TextTools.SubTable</H5>
<P class="docText">Match if the table given as <TT>argument</TT> matches at the current read-head position. The difference between the <TT>Table</TT> and the <TT>SubTable</TT> commands is in where matches get inserted. When the <TT>Table</TT> command is used, any matches in the indicated table are nested in the data structure associated with the tuple. When <TT>SubTable</TT> is used, matches are written into the current level taglist. For example:</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import *
&gt;&gt;&gt; from pprint import pprint
&gt;&gt;&gt; caps = ('Caps', AllIn, A2Z)
&gt;&gt;&gt; lower = ('Lower', AllIn, a2z)
&gt;&gt;&gt; words = ( ('Word', Table, (caps, lower)),
...           (None, AllIn, whitespace, MatchFail, -1) )
&gt;&gt;&gt; from pprint import pprint
&gt;&gt;&gt; pprint(tag(s, words))
(0,
 [('Word', 0, 4, [('Caps', 0, 1, None), ('Lower', 1, 4, None)]),
  ('Word', 5, 19, [('Caps', 5, 6, None), ('Lower', 6, 19, None)]),
  ('Word', 20, 29, [('Caps', 20, 24, None), ('Lower', 24, 29, None)]),
  ('Word', 30, 35, [('Caps', 30, 32, None), ('Lower', 32, 35, None)])
 ],
 35)
&gt;&gt;&gt; flatwords = ( (None, SubTable, (caps, lower)),
...               (None, AllIn, whitespace, MatchFail, -1) )
&gt;&gt;&gt; pprint (tag(s, flatwords))
(0,
 [('Caps', 0, 1, None),
  ('Lower', 1, 4, None),
  ('Caps', 5, 6, None),
  ('Lower', 6, 19, None),
  ('Caps', 20, 24, None),
  ('Lower', 24, 29, None),
  ('Caps', 30, 32, None),
  ('Lower', 32, 35, None)],
 35)
</pre>
<P class="docText">For either command, if a match occurs, the read-head is moved to immediately after the match.</P>
<P class="docText">The special constant <TT>ThisTable</TT> can be used instead of a tag table to call the current table recursively.</P>
<A NAME="ch04lev4sec37"></A><H5 class="docSection4Title"> mx.TextTools.TableInList<br>mx.TextTools.SubTableInList</H5>
<P class="docText">Similar to <TT>Table</TT> and <TT>SubTable</TT> except that the <TT>argument</TT> is a tuple of the form <TT>(list_of_tables, index)</TT>. The advantage (and the danger) of this is that a list is mutable and may have tables added after the tuple defined—in particular, the containing tag table may be added to <TT>list_of_tables</TT> to allow recursion. Note, however, that the special value <TT>ThisTable</TT> can be used with the <TT>Table</TT> or <TT>SubTable</TT> commands and is usually more clear.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.Table <span class="docEmphasis">304</span>; mx.TextTools.SubTable <span class="docEmphasis">304</span>;</p>
<A NAME="ch04lev4sec38"></A><H5 class="docSection4Title"> mx.TextTools.Call</H5>
<P class="docText">Match on any computable basis. Essentially, when the <TT>Call</TT> command is used, control over parsing/matching is turned over to Python rather than staying in the <span class="docEmphasis"><TT>mx.TextTools</TT></span> engine. The function that is called must accept arguments <TT>s</TT>, <TT>pos</TT>, and <TT>end</TT>—where <TT>s</TT> is the underlying string, <TT>pos</TT> is the current read-head position, and <TT>end</TT> is ending of the slice being processed. The called function must return an integer for the new read-head position; if the return is different from <TT>pos</TT>, the match is a success.</P>
<P class="docText">As an example, suppose you want to match at a certain point only if the next N characters make up a dictionary word. Perhaps an efficient stemmed data structure is used to represent the dictionary word list. You might check dictionary membership with a tuple like:</P>
<pre>
tup = ('DictWord', Call, inDict)
</pre>
<P class="docText">Since the function <TT>inDict</TT> is written in Python, it will generally not operate as quickly as does an <span class="docEmphasis"><TT>mx.TextTools</TT></span> pattern tuple.</P>
<A NAME="ch04lev4sec39"></A><H5 class="docSection4Title"> mx.TextTools.CallArg</H5>
<P class="docText">Same as <TT>Call</TT>, except <TT>CallArg</TT> allows passing additional arguments. For example, suppose the dictionary example given in the discussion of <TT>Call</TT> also allows you to specify language and maximum word length for a match:</P>
<pre>
tup = ('DictWord', Call, (inDict,['English',10]))
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.Call <span class="docEmphasis">305</span>;</p>
<A NAME="ch04lev3sec9"></A><H5 class="docSection3Title"> MODIFIERS</H5>
<A NAME="ch04lev4sec40"></A><H5 class="docSection4Title"> mx.TextTools.CallTag</H5>
<P class="docText">Instead of appending <TT>(tagobj, l, r, subtags)</TT> to the taglist upon a successful match, call the function indicated as the tag object (which must be a function rather than <TT>None</TT> or a string). The function called must accept the arguments <TT>taglist</TT>, <TT>s</TT>, <TT>start</TT>, <TT>end</TT>, and <TT>subtags</TT>—where <TT>taglist</TT> is the present taglist, <TT>s</TT> is the underlying string, <TT>start</TT> and <TT>end</TT> are the slice indices of the match, and <TT>subtags</TT> is the nested taglist. The function called <span class="docEmphasis">may</span>, but need not, append to or modify <TT>taglist</TT> or <TT>subtags</TT> as part of its action. For example, a code parsing application might include:</P>
<pre>
&gt;&gt;&gt; def todo_flag(taglist, s, start, end, subtags):
...     sys.stderr.write("Fix issue at offset %d\n" % start)
...
&gt;&gt;&gt; tup = (todo_flag, Word+CallTag, 'XXX')
&gt;&gt;&gt; tag('XXX more stuff', (tup,))
Fix issue at offset 0
(1, [], 3)
</pre>
<A NAME="ch04lev4sec41"></A><H5 class="docSection4Title"> mx.TextTools.AppendMatch</H5>
<P class="docText">Instead of appending <TT>(tagobj,start,end,subtags)</TT> to the taglist upon successful matching, append the match found as string. The produced taglist is "flattened" and cannot be used in the same manner as "normal" taglist data structures. The flat data structure is often useful for joining or for list processing styles.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import *
&gt;&gt;&gt; words = (('Word', AllIn+AppendMatch, alpha),
...          (None, AllIn, whitespace, MatchFail, -1))
&gt;&gt;&gt; tag('this and that', words)
(0, ['this', 'and', 'that'], 13)
&gt;&gt;&gt; join(tag('this and that', words)[1], '-')
'this-and-that'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.split() <span class="docEmphasis">142</span>;</p>
<A NAME="ch04lev4sec42"></A><H5 class="docSection4Title"> mx.TextTools.AppendToTagobj</H5>
<P class="docText">Instead of appending <TT>(tagobj,start,end,subtags)</TT> to the taglist upon successful matching, call the <TT>.append()</TT> method of the tag object. The <TT>tag object</TT> must be a list (or a descendent of <TT>list</TT> in Python 2.2+).</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import *
&gt;&gt;&gt; ws = []
&gt;&gt;&gt; words = ((ws, AllIn+AppendToTagobj, alpha),
...          (None, AllIn, whitespace, MatchFail, -1))
&gt;&gt;&gt; tag('this and that', words)
(0, [], 13)
&gt;&gt;&gt; ws
[(None, 0, 4, None), (None, 5, 8, None), (None, 9, 13, None)]
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.CallTag <span class="docEmphasis">305</span>;</p>
<A NAME="ch04lev4sec43"></A><H5 class="docSection4Title"> mx.TextTools.AppendTagobj</H5>
<P class="docText">Instead of appending <TT>(tagobj,start,end,subtags)</TT> to the taglist upon successful matching, append the tag object. The produced taglist is usually nonstandard and cannot be used in the same manner as "normal" taglist data structures. A flat data structure is often useful for joining or for list processing styles.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import *
&gt;&gt;&gt; words = (('word', AllIn+AppendTagobj, alpha),
...          (None, AllIn, whitespace, MatchFail, -1))
&gt;&gt;&gt; tag('this and that', words)
(0, ['word', 'word', 'word'], 13)
</pre>
<A NAME="ch04lev4sec44"></A><H5 class="docSection4Title"> mx.TextTools.LookAhead</H5>
<P class="docText">If this modifier is used, the read-head position is not changed when a match occurs. As the name suggests, this modifier allows you to create patterns similar to regular expression lookaheads.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import *
&gt;&gt;&gt; from pprint import pprint
&gt;&gt;&gt; xwords = ((None, IsIn+LookAhead, 'Xx', +2),
...           ('xword', AllIn, alpha, MatchFail, +2),
...           ('other', AllIn, alpha),
...           (None, AllIn, whitespace, MatchFail, -3))
&gt;&gt;&gt; pprint(tag('Xylophone trumpet xray camera', xwords))
(0,
 [('xword', 0, 9, None),
  ('other', 10, 17, None),
  ('xword', 18, 22, None),
  ('other', 23, 29, None)],
 29)
</pre>
<A NAME="ch04lev3sec10"></A><H5 class="docSection3Title"> CLASSES</H5>
<A NAME="ch04lev4sec45"></A><H5 class="docSection4Title"> mx.TextTools.BMS(word [,translate])<br>mx.TextTools.FS(word [,translate])<br>mx.TextTools.TextSearch(word [,translate [,algorithm=BOYERMOORE]])</H5>
<P class="docText">Create a search object for the string <TT>word</TT>. This is similar in concept to a compiled regular expression. A search object has several methods to locate its encoded string within another string. The <TT>BMS</TT> name is short for "Boyer-Moore," which is a particular search algorithm. The name <TT>FS</TT> is reserved for accessing the "Fast Search" algorithm in future versions, but currently both classes use Boyer-Moore. For <span class="docEmphasis"><TT>mx.TextTools</TT></span> 2.1.0+, you are urged to use the <TT>.TextSearch()</TT> constructor.</P>
<P class="docText">If a <TT>translate</TT> argument is given, the searched string is translated during the search. This is equivalent to transforming the string with <span class="docEmphasis"><TT>string.translate()</TT></span> prior to searching it.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.translate() <span class="docEmphasis">145</span>;</p>
<A NAME="ch04lev4sec46"></A><H5 class="docSection4Title"> mx.TextTools.CharSet(definition)</H5>
<P class="docText">Version 2.1.0 of <span class="docEmphasis"><TT>mx.TextTools</TT></span> adds the Unicode-compatible <TT>CharSet</TT> object. <TT>CharSet</TT> objects may be initialized to support character ranges, as in regular expressions; for example, <TT>definition="a-mXYZ"</TT>. In most respects, <TT>CharSet</TT> objects are similar to older sets.</P>
<A NAME="ch04lev3sec11"></A><H5 class="docSection3Title"> METHODS AND ATTRIBUTES</H5>
<A NAME="ch04lev4sec47"></A><H5 class="docSection4Title"> mx.TextTools.BMS.search(s [,start [,end]])<br>mx.TextTools.FS.search(s [,start [,end]])<br>mx.TextTools.TextSearch.search(s [,start [,end]])</H5>
<P class="docText">Locate as a slice the first match of the search object against <TT>s</TT>. If optional arguments <TT>start</TT> and <TT>end</TT> are used, only the slice <TT>s[start:end]</TT> is considered. Note: As of version 2.1.0, the documentation that accompanies <span class="docEmphasis"><TT>mx.TextTools</TT></span> inaccurately describes the <TT>end</TT> parameter of search object methods as indicating the length of the slice rather than its ending offset.</P>
<A NAME="ch04lev4sec48"></A><H5 class="docSection4Title"> mx.TextTools.BMS.find(s, [,start [,end]])<br>mx.TextTools.FS.find(s, [,start [,end]])<br>mx.TextTools.TextSearch.search(s [,start [,end]])</H5>
<P class="docText">Similar to <span class="docEmphasis"><TT>mx.TextTools.BMS.search()</TT></span>, except return only the starting position of the match. The behavior is similar to that of <span class="docEmphasis"><TT>string.find()</TT></span>.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.find() <span class="docEmphasis">135</span>; mx.TextTools.find() <span class="docEmphasis">312</span>;</p>
<A NAME="ch04lev4sec49"></A><H5 class="docSection4Title"> mx.TextTools.BMS.findall(s [,start [,end]])<br>mx.TextTools.FS.findall(s [,start [,end]])<br>mx.TextTools.TextSearch.search(s [,start [,end]])</H5>
<P class="docText">Locate as slices <span class="docEmphasis">every</span> match of the search object against <TT>s</TT>. If the optional arguments <TT>start</TT> and <TT>end</TT> are used, only the slice <TT>s[start:end]</TT> is considered.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import BMS, any, upper
&gt;&gt;&gt; foosrch = BMS('FOO', upper(any))
&gt;&gt;&gt; foosrch.search('foo and bar and FOO and BAR')
(0, 3)
&gt;&gt;&gt; foosrch.find('foo and bar and FOO and BAR')
0
&gt;&gt;&gt; foosrch.findall('foo and bar and FOO and BAR')
[(0, 3), (16, 19)]
&gt;&gt;&gt; foosrch.search('foo and bar and FOO and BAR', 10, 20)
(16, 19)
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
re.findall <span class="docEmphasis">245</span>; mx.TextTools.findall() <span class="docEmphasis">312</span>;</p>
<A NAME="ch04lev4sec50"></A><H5 class="docSection4Title"> mx.TextTools.BMS.match<br>mx.TextTools.FS.match<br>mx.TextTools.TextSearch.match</H5>
<P class="docText">The string that the search object will look for in the search text (read-only).</P>
<A NAME="ch04lev4sec51"></A><H5 class="docSection4Title"> mx.TextTools.BMS.translate<br>mx.TextTools.FS.translate<br>mx.TextTools.TextSearch.match</H5>
<P class="docText">The translation string used by the object, or <TT>None</TT> if no <TT>translate</TT> string was specified.</P>
<A NAME="ch04lev4sec52"></A><H5 class="docSection4Title"> mx.TextTools.CharSet.contains(c)</H5>
<P class="docText">Return a true value if character <TT>c</TT> is in the <TT>CharSet</TT>.</P>
<A NAME="ch04lev4sec53"></A><H5 class="docSection4Title"> mx.TextTools.CharSet.search(s [,direction [,start=0 [,stop=len(s)]]])</H5>
<P class="docText">Return the position of the first <TT>CharSet</TT> character that occurs in <TT>s[start:end]</TT>. Return <TT>None</TT> if there is no match. You may specify a negative <TT>direction</TT> to search backwards.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
re.search() <span class="docEmphasis">249</span>;</p>
<A NAME="ch04lev4sec54"></A><H5 class="docSection4Title"> mx.TextTools.CharSet.match(s [,direction [,start=0 [,stop=len(s)]]])</H5>
<P class="docText">Return the length of the longest contiguous match of the <TT>CharSet</TT> object against substrings of <TT>s[start:end]</TT>.</P>
<A NAME="ch04lev4sec55"></A><H5 class="docSection4Title"> mx.TextTools.CharSet.split(s [,start=0 [,stop=len(text)]])</H5>
<P class="docText">Return a list of substrings of <TT>s[start:end]</TT> divided by occurrences of characters in the <TT>CharSet</TT>.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
re.search() <span class="docEmphasis">249</span>;</p>
<A NAME="ch04lev4sec56"></A><H5 class="docSection4Title"> mx.TextTools.CharSet.splitx(s [,start=0 [,stop=len(text)]])</H5>
<P class="docText">Like <span class="docEmphasis"><TT>mx.TextTools.CharSet.split()</TT></span> except retain characters from <TT>CharSet</TT> in interspersed list elements.</P>
<A NAME="ch04lev4sec57"></A><H5 class="docSection4Title"> mx.TextTools.CharSet.strip(s [,where=0 [,start=0 [,stop=len(s)]]])</H5>
<P class="docText">Strip all characters in <TT>s[start:stop]</TT> appearing in the character set.</P>
<A NAME="ch04lev3sec12"></A><H5 class="docSection3Title"> FUNCTIONS</H5>
<P class="docText">Many of the functions in <span class="docEmphasis"><TT>mx.TextTools</TT></span> are used by the tagging engine. A number of others are higher-level utility functions that do not require custom development of tag tables. The latter are listed under a separate heading and generally resemble faster versions of functions in the <span class="docEmphasis"><TT>string</TT></span> module.</P>
<A NAME="ch04lev4sec58"></A><H5 class="docSection4Title"> mx.TextTools.cmp(t1, t2)</H5>
<P class="docText">Compare two valid taglist tuples on their slice positions. Taglists generated with multiple passes of <span class="docEmphasis"><TT>mx.TextTools.tag()</TT></span>, or combined by other means, may not have tuples sorted in string order. This custom comparison function is coded in C and is very fast.</P>
<pre>
&gt;&gt;&gt; import mx.TextTools
&gt;&gt;&gt; from pprint import pprint
&gt;&gt;&gt; t1 = [('other', 10, 17, None),
...       ('other', 23, 29, None),
...       ('xword', 0, 9, None),
...       ('xword', 18, 22, None)]
&gt;&gt;&gt; t1.sort(mx.TextTools.cmp)
&gt;&gt;&gt; pprint(tl)
[('xword', 0, 9, None),
 ('other', 10, 17, None),
 ('xword', 18, 22, None),
 ('other', 23, 29, None)]
</pre>
<A NAME="ch04lev4sec59"></A><H5 class="docSection4Title"> mx.TextTools.invset(s)</H5>
<P class="docText">Identical to <TT>mx.TextTools.set(s, 0)</TT>.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.set() <span class="docEmphasis">310</span>;</p>
<A NAME="ch04lev4sec60"></A><H5 class="docSection4Title"> mx.TextTools.set(s [,includechars=1])</H5>
<P class="docText">Return a bit-position encoded character set. Bit-position encoding makes tag table commands like <TT>InSet</TT> and <TT>AllInSet</TT> operate more quickly than their character-string equivalents (e.g, <TT>In</TT>, <TT>AllIn)</TT>.</P>
<P class="docText">If <TT>includechars</TT> is set to 0, invert the character set.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.invset() <span class="docEmphasis">310</span>;</p>
<A NAME="ch04lev4sec61"></A><H5 class="docSection4Title"> mx.TextTools.tag(s, table [,start [,end [,taglist]]])</H5>
<P class="docText">Apply a tag table to a string. The return value is a tuple of the form <TT>(success, taglist, next)</TT>. <TT>success</TT> is a binary value indicating whether the table matched. <TT>next</TT> is the read-head position after the match attempt. Even on a nonmatch of the table, the read-head might have been advanced to some degree by member tuples matching. The <TT>taglist</TT> return value contains the data structure generated by application. Modifiers and commands within the tag table can alter the composition of <TT>taglist</TT>; but in the normal case, <TT>taglist</TT> is composed of zero or more tuples of the form <TT>(tagname, start, end, subtaglist)</TT>.</P>
<P class="docText">Assuming a "normal" taglist is created, <TT>tagname</TT> is a string value that was given as a tag object in a tuple within the tag table. <TT>start</TT> and <TT>end</TT> the slice ends of that particular match. <TT>subtaglist</TT> is either <TT>None</TT> or a taglist for a subtable match.</P>
<P class="docText">If <TT>start</TT> or <TT>end</TT> are given as arguments to <span class="docEmphasis"><TT>mx.TextTools.tag()</TT></span> , application is restricted to the slice <TT>s[start:end]</TT> (or <TT>s[start:]</TT> if only <TT>start</TT> is used). If a <TT>taglist</TT> argument is passed, that list object is used instead of a new list. This allows extending a previously generated taglist, for example. If <TT>None</TT> is passed as <TT>taglist</TT>, no taglist is generated.</P>
<P class="docText">See the application examples and command illustrations for a number of concrete uses of <span class="docEmphasis"><TT>mx.TextTools.tag()</TT></span>.</P>
<A NAME="ch04lev3sec13"></A><H5 class="docSection3Title"> UTILITY FUNCTIONS</H5>
<A NAME="ch04lev4sec62"></A><H5 class="docSection4Title"> mx.TextTools.charsplit(s, char, [start [,end]])</H5>
<P class="docText">Return a list split around each <TT>char</TT>. Similar to <span class="docEmphasis"><TT>string.split()</TT></span>, but faster. If the optional arguments <TT>start</TT> and <TT>end</TT> are used, only the slice <TT>s[start:end]</TT> is operated on.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.split() <span class="docEmphasis">142</span>; mx.TextTools.setsplit() <span class="docEmphasis">314</span>;</p>
<A NAME="ch04lev4sec63"></A><H5 class="docSection4Title"> mx.TextTools.collapse(s, sep=' ')</H5>
<P class="docText">Return a string with normalized whitespace. This is equivalent to <TT>string.join(string.split (s),sep)</TT>, but faster.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import collapse
&gt;&gt;&gt; collapse('this and that','-')
'this-and-that'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.join() <span class="docEmphasis">137</span>; string.split() <span class="docEmphasis">142</span>;</p>
<A NAME="ch04lev4sec64"></A><H5 class="docSection4Title"> mx.TextTools.countlines(s)</H5>
<P class="docText">Returns the number of lines in s in a platform-portable way. Lines may end with CR (Mac-style), LF (Unix-style), or CRLF (DOS-style), including a mixture of these.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
FILE.readlines() <span class="docEmphasis">17</span>; mx.TextTools.splitlines() <span class="docEmphasis">315</span>;</p>
<A NAME="ch04lev4sec65"></A><H5 class="docSection4Title"> mx.TextTools.find(s, search_obj, [start, [,end]])</H5>
<P class="docText">Return the position of the first match of <TT>search_obj</TT> against <TT>s</TT>. If the optional arguments <TT>start</TT> and <TT>end</TT> are used, only the slice <TT>s[start:end]</TT> is considered. This function is identical to the search object method of the same name; the syntax is just slightly different. The following are synonyms:</P>
<pre>
from mx.TextTools import BMS, find
s = 'some string with a pattern in it'
pos1 = find(s, BMS('pat'))
pos2 = BMS('pat').find(s)
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.find() <span class="docEmphasis">135</span>; mx.TextTools.BMS.find() <span class="docEmphasis">308</span>;</p>
<A NAME="ch04lev4sec66"></A><H5 class="docSection4Title"> mx.TextTools.findall(s, search_obj [,start [,end]])</H5>
<P class="docText">Return as slices <span class="docEmphasis">every</span> match of <TT>search_obj</TT> against <TT>s</TT>. If the optional arguments <TT>start</TT> and <TT>end</TT> are used, only the slice <TT>s[start:end]</TT> is considered. This function is identical to the search object method of the same name; the syntax is just slightly different. The following are synonyms:</P>
<pre>
from mx.TextTools import BMS, findall
s = 'some string with a pattern in it'
pos1 = findall(s, BMS('pat'))
pos2 = BMSCpat').findall(s)
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.find() <span class="docEmphasis">312</span>; mx.TextTools.BMS.findall() <span class="docEmphasis">308</span>;</p>
<A NAME="ch04lev4sec67"></A><H5 class="docSection4Title"> mx.TextTools.hex2str(hexstr)</H5>
<P class="docText">Returns a string based on the hex-encoded string <TT>hexstr</TT>.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import hex2str, str2hex
&gt;&gt;&gt; str2hex('abc')
'616263'
&gt;&gt;&gt; hex2str('616263')
'abc'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.str2hex() <span class="docEmphasis">315</span>;</p>
<A NAME="ch04lev4sec68"></A><H5 class="docSection4Title"> mx.TextTools.is_whitespace(s [,start [,end]])</H5>
<P class="docText">Returns a Boolean value indicating whether <TT>s[start:end]</TT> contains only whitespace characters. <TT>start</TT> and <TT>end</TT> are optional, and will default to <TT>0</TT> and <TT>len(s)</TT>, respectively.</P>
<A NAME="ch04lev4sec69"></A><H5 class="docSection4Title"> mx.TextTools.isascii(s)</H5>
<P class="docText">Returns a Boolean value indicating whether s contains only ASCII characters.</P>
<A NAME="ch04lev4sec70"></A><H5 class="docSection4Title"> mx.TextTools.join(joinlist [,sep="" [,start [,end]]])</H5>
<P class="docText">Return a string composed of slices from other strings. <TT>joinlist</TT> is a sequence of tuples of the form <TT>(s, start, end, ...)</TT> each indicating the source string and offsets for the utilized slice. Negative offsets do not behave like Python slice offsets and should not be used. If a <TT>joinlist</TT> item tuple contains extra entries, they are ignored, but are permissible.</P>
<P class="docText">If the optional argument <TT>sep</TT> is specified, a delimiter between each joined slice is added. If <TT>start</TT> and <TT>end</TT> are specified, only <TT>joinlist[start:end]</TT> is utilized in the joining.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import join
&gt;&gt;&gt; s = 'Spam and eggs for breakfast'
&gt;&gt;&gt; t = 'This and that for lunch'
&gt;&gt;&gt; j1 = [(s, 0, 4), (s, 9, 13), (t, 0, 4), (t, 9, 13)]
&gt;&gt;&gt; join(j1, '/', 1, 4)
'/eggs/This/that'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.join() <span class="docEmphasis">137</span>;</p>
<A NAME="ch04lev4sec71"></A><H5 class="docSection4Title"> mx.TextTools.lower(s)</H5>
<P class="docText">Return a string with any uppercase letters converted to lowercase. Functionally identical to <TT>string.lower()</TT> , but much faster.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.lower() <span class="docEmphasis">138</span>; mx.TextTools.upper() <span class="docEmphasis">316</span>;</p>
<A NAME="ch04lev4sec72"></A><H5 class="docSection4Title"> mx.TextTools.prefix(s, prefixes [,start [,stop [,translate]]])</H5>
<P class="docText">Return the first prefix in the tuple <TT>prefixes</TT> that matches the end of <TT>s</TT>. If <TT>start</TT> and <TT>end</TT> are specified, only operate on the slice <TT>s[start:end]</TT>. Return <TT>None</TT> if no prefix matches.</P>
<P class="docText">If a <TT>translate</TT> argument is given, the searched string is translated during the search. This is equivalent to transforming the string with <TT>string.translate()</TT> prior to searching it.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import prefix
&gt;&gt;&gt; prefix('spam and eggs', ('spam','and','eggs'))
'spam'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.suffix() <span class="docEmphasis">316</span>;</p>
<A NAME="ch04lev4sec73"></A><H5 class="docSection4Title"> mx.TextTools.multireplace(s ,replacements [,start [,stop]])</H5>
<P class="docText">Replace multiple nonoverlapping slices in s with string values. <TT>replacements</TT> must be list of tuples of the form <TT>(new, left, right)</TT>. Indexing is always relative to s, even if an earlier replacement changes the length of the result. If <TT>start</TT> and <TT>end</TT> are specified, only operate on the slice <TT>s[start:end]</TT>.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import findall, multireplace
&gt;&gt;&gt; s = 'spam, bacon, sausage, and spam'
&gt;&gt;&gt; repls = [('X',l,r) for l,r in findall(s, 'spam')]
&gt;&gt;&gt; multireplace(s, repls)
'X, bacon, sausage, and X'
&gt;&gt;&gt; repls
[('X', 0, 4), ('X', 26, 30)]
</pre>
<A NAME="ch04lev4sec74"></A><H5 class="docSection4Title"> mx.TextTools.replace(s, old, new [,start [,stop]])</H5>
<P class="docText">Return a string where the pattern matched by search object <TT>old</TT> is replaced by string <TT>new</TT>. If <TT>start</TT> and <TT>end</TT> are specified, only operate on the slice <TT>s[start:end]</TT>. This function is much faster than <span class="docEmphasis"><TT>string.replace()</TT> ,</span> since a search object is used in the search aspect.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import replace, BMS
&gt;&gt;&gt; s = 'spam, bacon, sausage, and spam'
&gt;&gt;&gt; spam = BMS('spam')
&gt;&gt;&gt; replace(s, spam, 'eggs')
'eggs, bacon, sausage, and eggs'
&gt;&gt;&gt; replace(s, spam, 'eggs', 5)
' bacon, sausage, and eggs'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.replace() <span class="docEmphasis">139</span>; mx.TextTools.BMS <span class="docEmphasis">307</span>;</p>
<A NAME="ch04lev4sec75"></A><H5 class="docSection4Title"> mx.TextTools.setfind(s, set [,start [,end]])</H5>
<P class="docText">Find the first occurence of any character in <TT>set</TT>. If <TT>start</TT> is specified, look only in <TT>s[start:]</TT>; if <TT>end</TT> is specified, look only in <TT>s[start:end]</TT>. The argument <TT>set</TT> must be a set.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import *
&gt;&gt;&gt; s = 'spam and eggs'
&gt;&gt;&gt; vowel = set('aeiou')
&gt;&gt;&gt; setfind(s, vowel)
2
&gt;&gt;&gt; setfind(s, vowel, 7, 10)
9
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.set() <span class="docEmphasis">310</span>;</p>
<A NAME="ch04lev4sec76"></A><H5 class="docSection4Title"> mx.TextTools.setsplit(s, set [,start [,stop]])</H5>
<P class="docText">Split <TT>s</TT> into substrings divided at any characters in <TT>set</TT>. If <TT>start</TT> is specified, create a list of substrings of <TT>s[start:]</TT>; if <TT>end</TT> is specified, use <TT>s[start:end]</TT>. The argument <TT>set</TT> must be a set.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.split() <span class="docEmphasis">142</span>; mx.TextTools.set() <span class="docEmphasis">310</span>; mx.TextTools.setsplitx() <span class="docEmphasis">315</span>;</p>
<A NAME="ch04lev4sec77"></A><H5 class="docSection4Title"> mx.TextTools.setsplitx(text,set[,start =0, stop =len(text)])</H5>
<P class="docText">Split <TT>s</TT> into substrings divided at any characters in <TT>set</TT>. Include the split characters in the returned list. Adjacent characters in <TT>set</TT> are returned in the same list element. If <TT>start</TT> is specified, create a list of substrings of <TT>s[start:]; if end</TT> is specified, use <TT>s[start:end]</TT>. The argument <TT>set</TT> must be a set.</P>
<pre>
&gt;&gt;&gt; s = 'do you like spam'
&gt;&gt;&gt; setsplit(s, vowel)
['d', ' y', ' l', 'k', ' sp', 'm']
&gt;&gt;&gt; setsplitx(s, vowel)
['d', 'o', ' y', 'ou', ' l', 'i', 'k', 'e', ' sp', 'a', 'm']
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.split() <span class="docEmphasis">142</span>; mx.TextTools.set() <span class="docEmphasis">310</span>; mx.TextTools.setsplit() <span class="docEmphasis">314</span>;</p>
<A NAME="ch04lev4sec78"></A><H5 class="docSection4Title"> mx.TextTools.splitat(s, char, [n=1 [,start [end]]])</H5>
<P class="docText">Return a 2-element tuple that divides s around the n'th occurence of <TT>char</TT>. If <TT>start</TT> and <TT>end</TT> are specified, only operate on the slice <TT>s[start:end]</TT>.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import splitat
&gt;&gt;&gt; s = 'spam, bacon, sausage, and spam'
&gt;&gt;&gt; splitat(s, 'a', 3)
('spam, bacon, s', 'usage, and spam')
&gt;&gt;&gt; splitat(s, 'a', 3, 5, 20)
(' bacon, saus', 'ge')
</pre>
<A NAME="ch04lev4sec79"></A><H5 class="docSection4Title"> mx.TextTools.splitlines(s)</H5>
<P class="docText">Return a list of lines in <TT>s</TT>. Line-ending combinations for Mac, PC, and Unix platforms are recognized in any combination, which makes this function more portable than is <TT>string.split(s,"\n")</TT> or <span class="docEmphasis"><TT>FILE.readlines()</TT></span>.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.split() <span class="docEmphasis">142</span>; FILE.readlines() <span class="docEmphasis">17</span>; mx.TextTools.setsplit() <span class="docEmphasis">314</span>; mx.TextTools.countlines() <span class="docEmphasis">311</span>;</p>
<A NAME="ch04lev4sec80"></A><H5 class="docSection4Title"> mx.TextTools.splitwords(s)</H5>
<P class="docText">Return a list of whitespace-separated words in <TT>s</TT>. Equivalent to <TT>string.split(s)</TT>.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.split() <span class="docEmphasis">142</span>;</p>
<A NAME="ch04lev4sec81"></A><H5 class="docSection4Title"> mx.TextTools.str2hex(s)</H5>
<P class="docText">Returns a hexadecimal representation of a string. For Python 2.0+, this is equivalent to <TT>s.encode("hex")</TT>.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
"".encode() <span class="docEmphasis">188</span>; mx.TextTools.hex2str() <span class="docEmphasis">312</span>;</p>
<A NAME="ch04lev4sec82"></A><H5 class="docSection4Title"> mx.TextTools.suffix(s, suffixes [,start [,stop [,translate]]])</H5>
<P class="docText">Return the first suffix in the tuple <TT>suffixes</TT> that matches the end of <TT>s</TT>. If <TT>start</TT> and <TT>end</TT> are specified, only operate on the slice <TT>s[start:end]</TT>. Return <TT>None</TT> if no suffix matches.</P>
<P class="docText">If a <TT>translate</TT> argument is given, the searched string is translated during the search. This is equivalent to transforming the string with <span class="docEmphasis"><TT>string.translate()</TT></span> prior to searching it.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import suffix
&gt;&gt;&gt; suffix('spam and eggs', ('spam','and','eggs'))
'eggs'
</pre>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
mx.TextTools.prefix() <span class="docEmphasis">313</span>;</p>
<A NAME="ch04lev4sec83"></A><H5 class="docSection4Title"> mx.TextTools.upper(s)</H5>
<P class="docText">Return a string with any lowercase letters converted to uppercase. Functionally identical to <span class="docEmphasis"><TT>string.upper()</TT></span>, but much faster.</P>
<p class="docText"><span class="docEmphRoman">S<span class="docEmphSmaller">EE</span> A<span class="docEmphSmaller">LSO</span></span>: 
string.upper() <span class="docEmphasis">146</span>; mx.TextTools.lower() <span class="docEmphasis">313</span>;</p>
<A NAME="ch04lev2sec16"></A><H4 class="docSection2Title">4.3.3 High-Level EBNF Parsing</H4>
<P><A NAME="ch04sb02"></A><TABLE CELLSPACING="0" WIDTH="90%" BORDER="1"><TR><TD>
<P class="docText"><span class="docEmphStrong">SimpleParse &#8226; A Parser Generator for mx.TextTools</span></P></TD></TR></TABLE></P>
<P class="docText"><span class="docEmphasis"><TT>SimpleParse</TT></span> is an interesting tool. To use this module, you need to have the <span class="docEmphasis"><TT>mx.TextTools</TT></span> module installed. While there is nothing you can do with <span class="docEmphasis"><TT>SimpleParse</TT></span> that cannot be done with <span class="docEmphasis"><TT>mx.TextTools</TT></span> by itself, <span class="docEmphasis"><TT>SimpleParse</TT></span> is often much easier to work with. There exist other modules to provide higher-level APIs for <span class="docEmphasis"><TT>mx.TextTools</TT></span>; I find <span class="docEmphasis"><TT>SimpleParse</TT></span> to be the most useful of these, and the only one that this book will present. The examples in this section were written against <span class="docEmphasis"><TT>SimpleParse</TT></span> version 1.0, but the documentation is updated to include new features of 2.0. Version 2.0 is fully backward compatible with existing <span class="docEmphasis"><TT>SimpleParse</TT></span> code.</P>
<P class="docText"><span class="docEmphasis"><TT>SimpleParse</TT></span> substitutes an EBNF-style grammar for the low-level state matching language of <span class="docEmphasis"><TT>mx.TextTools</TT></span> tag tables. Or more accurately, <span class="docEmphasis"><TT>SimpleParse</TT></span> is a tool for generating tag tables based on friendlier and higher-level EBNF grammars. In principle, <span class="docEmphasis"><TT>SimpleParse</TT></span> lets you access and modify tag tables before passing them to <span class="docEmphasis"><TT>mx.TextTools.tag()</TT></span>. But in practice, you usually want to stick wholly with <span class="docEmphasis"><TT>SimpleParse</TT></span>'s EBNF variant when your processing is amenable to a grammatical description of the text format.</P>
<P class="docText">An application based on <span class="docEmphasis"><TT>SimpleParse</TT></span> has two main aspects. The first aspect is the grammar that defines the structure of a processed text. The second aspect is the traversal and use of a generated <span class="docEmphasis"><TT>mx.TextTools</TT></span> taglist. <span class="docEmphasis"><TT>SimpleParse</TT></span> 2.0 adds facilities for the traversal aspect, but taglists present a data structure that is quite easy to work with in any case. The tree-walking tools in <span class="docEmphasis"><TT>SimpleParse</TT></span> 2.0 are not covered here, but the examples given in the discussion of <span class="docEmphasis"><TT>mx.TextTools</TT></span> illustrate such traversal.</P>
<A NAME="ch04lev5sec6"></A><H5 class="docSection3Title"> Example: Marking up smart ASCII (Redux)</H5>
<P class="docText">Elsewhere in this book, applications to process the smart ASCII format are also presented. <A class="docLink" HREF="0321112547_app04.html#app04">Appendix D</A> lists the <TT>Txt2Html</TT> utility, which uses a combination of a state machine for parsing paragraphs and regular expressions for identifying inline markup. A functionally similar example was given in the discussion of <span class="docEmphasis"><TT>mx.TextTools</TT></span>, where a complex and compound tag table was developed to recognize inline markup elements. Using <span class="docEmphasis"><TT>SimpleParse</TT></span> and an EBNF grammar is yet another way to perform the same sort of processing. Comparing the several styles will highlight a number of advantages that <span class="docEmphasis"><TT>SimpleParse</TT></span> has—its grammars are clear and concise, and applications built around it can be extremely fast.</P>
<P class="docText">The application <TT>simpleTypography.py</TT> is quite simple; most of the work of programming it lies in creating a grammar to describe smart ASCII. EBNF grammars are almost self-explanatory to read, but designing one <span class="docEmphasis">does</span> require a bit of thought and testing:</P>
<H5 class="docExampleTitle"><A NAME="ch04list18"></A> typography.def</H5>

<PRE>
para           := (plain / markup)+
plain          := (word / whitespace / punctuation)+
&lt;whitespace&gt;   := [ \t\r\n]+
&lt;alphanums&gt;    := [a-zA-Z0-9]+
&lt;word&gt;         := alphanums, (wordpunct, alphanums)*, contraction?
&lt;wordpunct&gt;    := [-_]
&lt;contraction&gt;  := "'", ('am'/'clock'/'d'/'ll'/'m'/'re'/'s'/'t'/'ve')
markup         := emph / strong / module / code / title
emph           := '-', plain, '-'
strong         := '*', plain, '*'
module         := '[', plain, ']'
code           := "'", plain, "'"
title          := '_', plain, '_'
&lt;punctuation&gt;  := (safepunct / mdash)
&lt;mdash&gt;        := '--'
&lt;safepunct&gt;    := [!@#$%^&amp;()+=|\{}:;&lt;&gt;,.?/"]
</PRE>
<P class="docText">This grammar is almost exactly the way you would describe the smart ASCII language verbally, which is a nice sort of clarity. A paragraph consist of some plaintext and some marked-up text. Plaintext consists of some collection of words, whitespace, and punctuation. Marked-up text might be emphasized, or strongly emphasized, or module names, and so on. Strongly emphasized text is surrounded by asterisks. And so on. A couple of features like just what a "word" really is, or just what a contraction can end with, take a bit of thought, but the syntax of EBNF doesn't get in the way.</P>
<P class="docText">Notice that some declarations have their left side surrounded in angle brackets. Those productions will not be written to the taglist—this is the same as using <TT>None</TT> as a <TT>tagobj</TT> in an <span class="docEmphasis"><TT>mx.Texttools</TT></span> tag table. Of course, if a production is not written to the taglist, then its children cannot be, either. By omitting some productions from the resultant taglist, a simpler data structure is produced (with only those elements that interest us).</P>
<P class="docText">In contrast to the grammar above, the same sort of rules can be described even more tersely using regular expressions. This is what the <TT>Txt2Html</TT> version of the smart ASCII markup program does. But this terseness is much harder to write and harder still to tweak later. The <span class="docEmphasis"><TT>re</TT></span> code below expresses largely (but not precisely) the same set of rules:</P>
<H5 class="docExampleTitle"><A NAME="ch04list19"></A> Python regexes for smart ASCII markup</H5>

<PRE>
# [module] names
re_mods =   r"""([\(\s'/"&gt;]|^)\[(.*?)\]([&lt;\s\.\),:;'"?!/-])"""
# *strongly emphasize* words
re_strong = r"""([\(\s'/"]|^)\*(.*?)\*([\s\.\),:;'"?!/-])"""
# -emphasize- words
re_emph =   r"""([\(\s'/"]|^)-(.*?)-([\s\.\),:;'"?!/])"""
# _Book Title_ citations
re_title =  r"""([\(\s'/"]|^)_(.*?)_([\s\.\),:;'"?!/-])"""
# 'Function()' names
re_funcs =  r"""([\(\s/"]|^)'(.*?)'([\s\.\),:;"?!/-])"""
</PRE>
<P class="docText">If you discover or invent some slightly new variant of the language, it is <span class="docEmphasis">a lot</span> easier to play with the EBNF grammar than with those regular expressions. Moreover, using <span class="docEmphasis"><TT>SimpleParse</TT></span>—and therefore <span class="docEmphasis"><TT>mx.TextTools</TT></span>—will generally be even faster in performing the manipulations of the patterns.</P>
<A NAME="ch04lev3sec14"></A><H5 class="docSection3Title"> GENERATING AND USING A TAGLIST</H5>
<P class="docText">For <TT>simpleTypography.py</TT>, I put the actual grammar in a separate file. For most purposes, this is a good organization to use. Changing the grammar is usually a different sort of task than changing the application logic, and the files reflect this. But the grammar is just read as a string, so in principle you could include it in the main application (or even dynamically generate it in some way).</P>
<P class="docText">Let us look at the entire—compact—tagging application:</P>
<H5 class="docExampleTitle"><A NAME="ch04list20"></A> simpleTypography.py</H5>

<PRE>
from sys import stdin, stdout, stderr
from simpleparse import generator
from mx.TextTools import TextTools
from typo_html import codes
from pprint import pprint

src = stdin.read()
decl = open('typography.def').read()
parser = generator.buildParser(decl).parserbyname('para')
taglist = TextTools.tag(src, parser)
pprint(taglist, stderr)

for tag, beg, end, parts in taglist[1]:
    if tag == 'plain':
        stdout.write(src[beg:end])
    elif tag == 'markup':
        markup = parts[0]
        mtag, mbeg, mend = markup[:3]
        start, stop = codes.get(mtag, ('&lt;!-- unknown --&gt;',
                                       '&lt;!-- /unknown --&gt;'))
        stdout.write(start  +  src[mbeg+1:mend-1] + stop)
    else:
        raise TypeError, "Top level tagging should be plain/markup"
</PRE>
<P class="docText">With version 2.0 of <span class="docEmphasis"><TT>SimpleParse</TT></span>, you may use a somewhat more convenient API to create a taglist:</P>
<pre>
from simpleparse.parser import Parser
parser = Parser(open('typography.def').read(), 'para')
taglist = parser.parse(src)
</pre>
<P class="docText">Here is what it does. First read in the grammar and create an <span class="docEmphasis"><TT>mx.TextTools</TT></span> parser from the grammar. The generated parser is similar to the tag table that is found in the hand-written <TT>mxTypography.py</TT> module discussed earlier (but without the human-friendly comments and structure). Next, apply the tag table/parser to the input source to create a taglist. Finally, loop through the taglist, and emit some new marked-up text. The loop could, of course, do anything else desired with each production encountered.</P>
<P class="docText">For the particular grammar used for smart ASCII, everything in the source text is expected to fall into either a "plain" production or a "markup" production. Therefore, it suffices to loop across a single level in the taglist (except when we look exactly one level lower for the specific markup production, such as "title"). But a more free-form grammar—such as occurs for most programming languages—could easily recursively descend into the taglist and look for production names at every level. For example, if the grammar were to allow nested markup codes, this recursive style would probably be used. Readers might enjoy the exercise of figuring out how to adjust the grammar (hint: Remember that productions are allowed to be mutually recursive).</P>
<P class="docText">The particular markup codes that go to the output live in yet another file for organizational, not essential, reasons. A little trick of using a dictionary as a <TT>switch</TT> statement is used here (although the <TT>otherwise</TT> case remains too narrow in the example). The idea behind this organization is that we might in the future want to create multiple "output format" files for, say, HTML, DocBook, <IMG BORDER="0" WIDTH="40" HEIGHT="17" src="FILES/latex.gif" ALT="graphics/latex.gif">, or others. The particular markup file used for the example just looks like:</P>
<H5 class="docExampleTitle"><A NAME="ch04list21"></A> typo_html.py</H5>

<PRE>
codes = \
{ 'emph'   : ('&lt;em&gt;', '&lt;/em&gt;'),
  'strong' : ('&lt;strong&gt;', '&lt;/strong&gt;'),
  'module' : ('&lt;em&gt;&lt;code&gt;', '&lt;/code&gt;&lt;/em&gt;'),
  'code'   : ('&lt;code&gt;', '&lt;/code&gt;'),
  'title'  : ('&lt;cite&gt;', '&lt;/cite&gt;'),
}
</PRE>
<P class="docText">Extending this to other output formats is straightforward.</P>
<A NAME="ch04lev3sec15"></A><H5 class="docSection3Title"> THE TAGLIST AND THE OUTPUT</H5>
<P class="docText">The <span class="docEmphasis">tag table</span> generated from the grammar in <TT>typography.def</TT> is surprisingly complicated and includes numerous recursions. Only the exceptionally brave of heart will want to attempt manual—let alone automated—modification of tag tables created by <span class="docEmphasis"><TT>SimpleParse</TT></span>. Fortunately, an average user need not even look at these tags, but simply <span class="docEmphasis">use</span> them, as is done with <TT>parser</TT> in <TT>simpleTypography.py</TT>.</P>
<P class="docText">The <span class="docEmphasis">taglist</span> produced by applying a grammar, in contrast, can be remarkably simple. Here is a run of <TT>simpleTypography.py</TT> against a small input file:</P>
<pre>
% python simpleTypography.py &lt; p.txt &gt; p.html
(1,
 [('plain', 0, 15, []),
  ('markup', 15, 27, [('emph', 15, 27, [('plain', 16, 26, [])])]),
  ('plain', 27, 42, []),
  ('markup', 42, 51, [('module', 42, 51, [('plain', 43, 50, [])])]),
  ('plain', 51, 55, []),
  ('markup', 55, 70, [('code', 55, 70, [('plain', 56, 69, [])])]),
  ('plain', 70, 90, []),
  ('markup', 90, 96, [('strong', 90, 96, [('plain', 91, 95, [])])]),
  ('plain', 96, 132, []),
  ('markup', 132, 145, [('title', 132, 145, [('plain',133,144,[])])]),
  ('plain', 145, 174, [])],
 174)
</pre>
<P class="docText">Most productions that were satisfied are not written into the taglist, because they are not needed for the application. You can control this aspect simply by defining productions with or without angle braces on the left side of their declaration. The output looks like you would expect:</P>
<pre>
% cat p.txt
Some words are -in italics-, others
name [modules] or 'command lines'.
Still others are *bold* -- that's how
it goes. Maybe some _book titles_.
And some in-fixed dashes.
% cat p.html
Some words are &lt;em&gt;in italics&lt;/em&gt;, others
name &lt;em&gt;&lt;code&gt;modules&lt;/code&gt;&lt;/em&gt; or &lt;code&gt;command lines&lt;/code&gt;.
Still others are &lt;strong&gt;bold&lt;/strong&gt; -- that's how
it goes. Maybe some &lt;cite&gt;book titles&lt;/cite&gt;.
And some in-fixed dashes.
</pre>
<p class="docText"><IMG BORDER="0" WIDTH="500" HEIGHT="17" src="FILES/common.gif" ALT="graphics/common.gif"></p>
<A NAME="ch04lev3sec16"></A><H5 class="docSection3Title"> GRAMMAR</H5>
<P class="docText">The language of <span class="docEmphasis"><TT>SimpleParse</TT></span> grammars is itself defined using a <span class="docEmphasis"><TT>SimpleParse</TT></span> EBNF-style grammar. In principle, you could refine the language <span class="docEmphasis"><TT>SimpleParse</TT></span> uses by changing the variable <TT>declaration</TT> in <TT>bootstrap.py</TT>, or <TT>simpleparsegrammar.py</TT> in recent versions. For example, extended regular expressions, W3C XML Schemas, and some EBNF variants allow integer occurrence quantification. To specify that three to seven <TT>foo</TT> tokens occur, you could use the following declaration in <span class="docEmphasis"><TT>SimpleParse</TT></span>:</P>
<pre>
foos := foo, foo, foo, foo?, foo?, foo?, foo?
</pre>
<P class="docText">Hypothetically, it might be more elegant to write something like:</P>
<pre>
foos := foo{3,7}
</pre>
<P class="docText">In practice, only someone developing a custom/enhanced parsing module would have any reason to fiddle quite so deeply; "normal" programmers should use the particular EBNF variant defined by default. Nonetheless, taking a look at <TT>simpleparse/bootstrap.py</TT> can be illustrative in understanding the module.</P>
<A NAME="ch04lev3sec17"></A><H5 class="docSection3Title"> DECLARATION PATTERNS</H5>
<P class="docText">A <span class="docEmphasis"><TT>SimpleParse</TT></span> grammar consists of a set of one or more declarations. Each declaration generally occurs on a line by itself; within a line, horizontal whitespace may be used as desired to improve readability. A common strategy is to align the right sides of declarations, but any other use of internal whitespace is acceptable. A declaration contains a term, followed by the assignment symbol ":=", followed by a definition. An end-of-line comment may be added to a declaration, following an unquoted "#" (just as in Python).</P>
<P class="docText">In contrast to most imperative-style programming, the declarations within a grammar may occur in any order. When a parser generator's .<TT>parserbyname()</TT> method is called, the "top level" of the grammar is given as an argument. The documented API for <span class="docEmphasis"><TT>SimpleParse</TT></span> uses a call of the form:</P>
<pre>
from simpleparse import generator
parser = generator.buildParser(decl).parserbyname('toplevel')
from mx.TextTools import TextTools
taglist = TextTools.tag(src, parser)
</pre>
<P class="docText">Under <span class="docEmphasis"><TT>SimpleParse</TT></span> 2.0, you may simplify this to:</P>
<pre>
from simpleparse.parser import Parser
parser = Parser(decl,'toplevel')
taglist = parser.parse(src)
</pre>
<P class="docText">A left side term may be surrounded by angle brackets <TT>("&lt;"</TT>, <TT>"&gt;")</TT> to prevent that production from being written into a taglist produced by <span class="docEmphasis"><TT>mx.TextTools.tag()</TT></span>. This is called an "unreported" production. Other than in relation to the final taglist, an unreported production acts just like a reported one. Either type of term may be used on the right sides of other productions in the same manner (without angle brackets when occurring on the right side).</P>
<P class="docText">In <span class="docEmphasis"><TT>SimpleParse</TT></span> 2.0 you may also use reversed angle brackets to report the children of a production, but not the production itself. As with the standard angle brackets, the production functions normally in matching inputs; it differs only in produced taglist. For example:</P>
<pre>
PRODUCTIONS            TAGLIST
-------------------------------------------------------
a   := (b,c)           ('a', l, r, [
b   := (d,e)               ('b', l, r, [...]),
c   := (f,g)               ('c', l, r, [...]) ] )
-------------------------------------------------------
a   := (b,c)           ('a', l, r, [
&lt;b&gt; := (d,e)               # no b, and no children
c   := (f,g)               ('c', l, r, [...]) ] )
-------------------------------------------------------
# Only in 2.0+         ('a', l, r, [
a   := (b,c)               # no b, but raise children
&gt;b&lt; := (d,e)               ('d', l, r, [...]),
c   := (f,g)               ('e', l, r, [...]),
                           ('c', l, r, [...]) ] )
-------------------------------------------------------
</pre>
<P class="docText">The remainder of the documentation of the <span class="docEmphasis"><TT>SimpleParse</TT></span> module covers elements that may occur on the right sides of declarations. In addition to the elements listed, a term from another production may occur anywhere any element may. Terms may thus stand in mutually recursive relations to one another.</P>
<A NAME="ch04lev3sec18"></A><H5 class="docSection3Title"> LITERALS</H5>
<A NAME="ch04lev4sec84"></A><H5 class="docSection4Title"> Literal string</H5>
<P class="docText">A string enclosed in single quotes matches the exact string quoted. Python escaping may be used for the characters \<TT>a</TT>, \<TT>b</TT>, \<TT>f</TT>, \<TT>n</TT>, \<TT>r</TT>, \<TT>t</TT>, and \<TT>v</TT>, and octal escapes of one to three digits may used. To include a literal backslash, it should be escaped as \\.</P>
<pre>
foo := "bar"
</pre>
<A NAME="ch04lev4sec85"></A><H5 class="docSection4Title"> Character class: "[", "]"</H5>
<P class="docText">Specify a set of characters that may occur at a position. The list of allowable characters may be enumerated with no delimiter. A range of characters may be indicated with a dash ("-"). Multiple ranges are allowed within a class.</P>
<P class="docText">To include a "]" character in a character class, make it the first character. Similarly, a literal "-" character must be either the first (after the optional "]" character) or the last character.</P>
<pre>
varchar := [a-zA-Z_0-9]
</pre>
<A NAME="ch04lev3sec19"></A><H5 class="docSection3Title"> QUANTIFIERS</H5>
<A NAME="ch04lev4sec86"></A><H5 class="docSection4Title"> Universal quantifier: "*"</H5>
<P class="docText">Match zero or more occurrences of the preceding expression. Quantification has a higher precedence than alternation or sequencing; grouping may be used to clarify quantification scope as well.</P>
<pre>
any_Xs     := "X"*
any_digits := [0-9]*
</pre>
<A NAME="ch04lev4sec87"></A><H5 class="docSection4Title"> Existential quantifier: "+"</H5>
<P class="docText">Match one or more occurrences of the preceding expression. Quantification has a higher precedence than alternation or sequencing; grouping may be used to clarify quantification scope as well.</P>
<pre>
some_Xs     := "X"+
some_digits := [0-9]+
</pre>
<A NAME="ch04lev4sec88"></A><H5 class="docSection4Title"> Potentiality quantifier: "?"</H5>
<P class="docText">Match at most one occurrence of the preceding expression. Quantification has a higher precedence than alternation or sequencing; grouping may be used to clarify quantification scope as well.</P>
<pre>
maybe_Xs     := "X"?
maybe_digits := [0-9]?
</pre>
<A NAME="ch04lev4sec89"></A><H5 class="docSection4Title"> Lookahead quantifier: "?"</H5>
<P class="docText">In <span class="docEmphasis"><TT>SimpleParse</TT></span> 2.0+, you may place a question mark <span class="docEmphasis">before</span> a pattern to assert that it occurs, but should not actually claim the pattern. As with regular expressions, you can create either positive or negative lookahead assertions.</P>
<pre>
next_is_Xs         := ?"X"
next_is_not_digits := ?-[0-9]
</pre>
<A NAME="ch04lev4sec90"></A><H5 class="docSection4Title"> Error on Failure: "!"</H5>
<P class="docText">In <span class="docEmphasis"><TT>SimpleParse</TT></span> 2.0+, you may cause a descriptive exception to be raised when a production does not match, rather than merely stopping parsing at that point.</P>
<pre>
require_Xs   := "X"!
require_code := ([A-Z]+, [0-9])!
contraction := "'", ('clock'/'d'/'ll'/'m'/'re'/'s'/'t'/'ve')!
</pre>
<P class="docText">For example, modifying the <TT>contraction</TT> production from the prior discussion could require that every apostrophe is followed by an ending. Since this doesn't hold, you might see an exception like:</P>
<pre>
% python typo2.py &lt; p.txt
Traceback (most recent call last):
[...]
simpleparse.error.ParserSyntaxError:  ParserSyntaxError:
Failed parsing production "contraction" @pos 84 (~line 1:29).
Expected syntax: ('clock'/'d'/'ll'/'m'/'re'/'s'/'t'/'ve')
Got text: 'command lines'. Still others are *bold*
</pre>
<A NAME="ch04lev3sec20"></A><H5 class="docSection3Title"> STRUCTURES</H5>
<A NAME="ch04lev4sec91"></A><H5 class="docSection4Title"> Alternation operator: "/"</H5>
<P class="docText">Match the first pattern possible from several alternatives. This operator allows any of a list of patterns to match. Some EBNF-style parsers will match the <span class="docEmphasis">longest</span> possible pattern, but <span class="docEmphasis"><TT>SimpleParse</TT></span> more simply matches the <span class="docEmphasis">first</span> possible pattern. For example:</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import tag
&gt;&gt;&gt; from simpleparse import generator
&gt;&gt;&gt; decl = '''
... short := "foo", " "*
... long  := "foobar", " "*
... sl    := (short / long)*
... ls    := (long / short)*
... '''
&gt;&gt;&gt; parser = generator.buildParser(decl).parserbyname('sl')
&gt;&gt;&gt; tag('foo foobar foo bar', parser) [1]
[('short', 0, 4, []), ('short', 4, 7, [])]
&gt;&gt;&gt; parser = generator.buildParser(decl).parserbyname('ls')
&gt;&gt;&gt; tag('foo foobar foo bar', parser) [1]
[('short', 0, 4, []), ('long', 4, 11, []), ('short', 11, 15, [])]
</pre>
<A NAME="ch04lev4sec92"></A><H5 class="docSection4Title"> Sequence operator: ","</H5>
<P class="docText">Match the first pattern followed by the second pattern (followed by the third pattern, if present, ...). Whenever a definition needs several elements in a specific order, the comma sequence operator is used.</P>
<pre>
term := someterm, [0-9]*, "X"+, (otherterm, stillother)?
</pre>
<A NAME="ch04lev4sec93"></A><H5 class="docSection4Title"> Negation operator: "-"</H5>
<P class="docText">Match anything that the next pattern <span class="docEmphasis">does not</span> match. The pattern negated can be either a simple term or a compound expression.</P>
<pre>
nonletters   := -[a-zA-Z]
nonfoo       := -foo
notfoobarbaz := -(foo, bar, baz)
</pre>
<P class="docText">An expression modified by the negation operator is very similar conceptually to a regular expression with a negative lookahead assertion. For example:</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import tag
&gt;&gt;&gt; from simpleparse import generator
&gt;&gt;&gt; decl = '''not_initfoo : = [ \t]*, -"foo", [a-zA-Z ]+'''
&gt;&gt;&gt; p = generator.buildParser(decl).parserbyname('not_initfoo')
&gt;&gt;&gt; tag(' foobar and baz', p)      # no match
(0, [], 0)
&gt;&gt;&gt; tag(' bar, foo and baz', p)    # match on part
(1, [], 5)
&gt;&gt;&gt; tag(' bar foo and baz', p)     # match on all
(1, [], 17)
</pre>
<A NAME="ch04lev4sec94"></A><H5 class="docSection4Title"> Grouping operators: "(", ")"</H5>
<P class="docText">Parentheses surrounding any pattern turn that pattern into an expression (possibly within a larger expression). Quantifiers and operators refer to the immediately adjacent expression, if one is defined, otherwise to the adjacent literal string, character class, or term.</P>
<pre>
&gt;&gt;&gt; from mx.TextTools import tag
&gt;&gt;&gt; from simpleparse import generator
&gt;&gt;&gt; decl = '''
... foo      := "foo"
... bar      := "bar"
... foo_bars := foo, bar+
... foobars  := (foo, bar)+
... '''
&gt;&gt;&gt; p1 = generator.buildParser(decl).parserbyname('foobars')
&gt;&gt;&gt; p2 = generator.buildParser(decl).parserbyname('foo_bars')
&gt;&gt;&gt; tag('foobarfoobar', p1)
(1, [('foo', 0, 3, []), ('bar', 3, 6, []),
     ('foo', 6, 9, []), ('bar', 9, 12, [])], 12)
&gt;&gt;&gt; tag('foobarfoobar', p2)
(1, [('foo', 0, 3, []), ('bar', 3, 6, [])], 6)
&gt;&gt;&gt; tag('foobarbarbar', p1)
(1, [('foo', 0, 3, []), ('bar', 3, 6, [])], 6)
&gt;&gt;&gt; tag('foobarbarbar', p2)
(1, [('foo', 0, 3, []), ('bar', 3, 6, []),
     ('bar', 6, 9, []), ('bar', 9, 12, [])], 12)
</pre>
<A NAME="ch04lev3sec21"></A><H5 class="docSection3Title"> USEFUL PRODUCTIONS</H5>
<P class="docText">In version 2.0+, <span class="docEmphasis"><TT>SimpleParse</TT></span> includes a number of useful productions that may be included in your grammars. See the examples and documentation that accompany <span class="docEmphasis"><TT>SimpleParse</TT></span> for details on the many included productions and their usage.</P>
<P class="docText">The included productions, at the time of this writing, fall into the categories below:</P>
<A NAME="ch04lev4sec95"></A><H5 class="docSection4Title"> simpleparse.common.calendar_names</H5>
<P class="docText">Locale-specific names of months, and days of the week, including abbreviated forms.</P>
<A NAME="ch04lev4sec96"></A><H5 class="docSection4Title"> simpleparse.common.chartypes</H5>
<P class="docText">Locale-specific categories of characters, such as digits, uppercase, octdigits, punctuation, locale_decimal_point, and so on.</P>
<A NAME="ch04lev4sec97"></A><H5 class="docSection4Title"> simpleparse.common.comments</H5>
<P class="docText">Productions to match comments in a variety of programming languages, such as hash <TT>(#)</TT> end-of-line comments (Python, Bash, Perl, etc.); C paired comments <TT>(/* comment */);</TT> and others.</P>
<A NAME="ch04lev4sec98"></A><H5 class="docSection4Title"> simpleparse.common.iso_date</H5>
<P class="docText">Productions for strictly conformant ISO date and time formats.</P>
<A NAME="ch04lev4sec99"></A><H5 class="docSection4Title"> simpleparse.common.iso_date_loose</H5>
<P class="docText">Productions for ISO date and time formats with some leeway as to common variants in formatting.</P>
<A NAME="ch04lev4sec100"></A><H5 class="docSection4Title"> simpleparse.common.numbers</H5>
<P class="docText">Productions for common numeric formats, such as integers, floats, hex numbers, binary numbers, and so on.</P>
<A NAME="ch04lev4sec101"></A><H5 class="docSection4Title"> simpleparse.common.phonetics</H5>
<P class="docText">Productions to match phonetically spelled words. Currently, the US military style of "alpha, bravo, charlie, ..." spelling is the only style supported (with some leeway in word spellings).</P>
<A NAME="ch04lev4sec102"></A><H5 class="docSection4Title"> simpleparse.common.strings</H5>
<P class="docText">Productions to match quoted strings as used in various programming languages.</P>
<A NAME="ch04lev4sec103"></A><H5 class="docSection4Title"> simpleparse.common.timezone_names</H5>
<P class="docText">Productions to match descriptions of timezones, as you might find in email headers or other data/time fields.</P>
<A NAME="ch04lev3sec22"></A><H5 class="docSection3Title"> GOTCHAS</H5>
<P class="docText">There are a couple of problems that can easily arise in constructed <span class="docEmphasis"><TT>SimpleParse</TT></span> grammars. If you are having problems in your application, keep a careful eye out for these issues:</P>
<span style="font-weight:bold"><OL class="docList" TYPE="1"><LI><span style="font-weight:normal"><P class="docList">Bad recursion. You might fairly naturally construct a pattern of the form:</P><pre>
a := b, a?
</pre><P class="docList">Unfortunately, if a long string of <TT>b</TT> rules are matched, the repeated recognition can either exceed the C-stack's recursion limit, or consume inordinate amounts of memory to construct nested tuples. Use an alternate pattern like:</P><pre>
a := b+
</pre><P class="docList">This will grab all the <TT>b</TT> productions in one tuple instead (you could separately parse out each <TT>b</TT> if necessary).</P></span></LI><LI><span style="font-weight:normal"><P class="docList">Quantified potentiality. That is a mouthful; consider patterns like:</P><pre>
a := (b? / c)*
x := (y?, z?)+
</pre><P class="docList">The first alternate <TT>b?</TT> in the first—and both <TT>y?</TT> and <TT>z?</TT> in the second—are happy to match zero characters (if a <TT>b</TT> or <TT>y</TT> or <TT>z</TT> do not occur at the current position). When you match "as many as possible" zero-width patterns, you get into an infinite loop. Unfortunately, the pattern is not always simple; it might not be b that is qualified as potential, but rather <TT>b</TT> productions (or the productions <span class="docEmphasis">in</span> <TT>b</TT> productions, etc.).</P></span></LI><LI><span style="font-weight:normal"><P class="docList">No backtracking. Based on working with regular expression, you might expect <span class="docEmphasis"><TT>SimpleParse</TT></span> productions to use backtracking. They do not. For example:</P><pre>
a := ((b/c)*, b)
</pre><P class="docList">If this were a regular expression, it would match a string of <TT>b</TT> productions, then back up one to match the final <TT>b</TT>. As a <span class="docEmphasis"><TT>SimpleParse</TT></span> production, this definition can never match. If any b productions occur, they will be claimed by <TT>(b/c)*</TT>, leaving nothing for the final <TT>b</TT> to grab.</P></span></LI></OL></span>
<A NAME="ch04lev2sec17"></A><H4 class="docSection2Title">4.3.4 High-Level Programmatic Parsing</H4>
<P><A NAME="ch04sb03"></A><TABLE CELLSPACING="0" WIDTH="90%" BORDER="1"><TR><TD>
<P class="docText"><span class="docEmphStrong">PLY</span> &#8226; <span class="docEmphStrong">Python Lex-Yacc</span></P></TD></TR></TABLE></P>
<P class="docText">One module that I considered covering to round out this chapter is John Aycock's <span class="docEmphasis"><TT>Spark</TT></span> module. This module is both widely used in the Python community and extremely powerful. However, I believe that the audience of this book is better served by working with David Beazley's <span class="docEmphasis"><TT>PLY</TT></span> module than with the older <span class="docEmphasis"><TT>Spark</TT></span> module.</P>
<P class="docText">In the documentation accompanying <span class="docEmphasis"><TT>PLY</TT></span>, Beazley consciously acknowledges the influence of <span class="docEmphasis"><TT>Spark</TT></span> on his design and development. While the <span class="docEmphasis"><TT>PLY</TT></span> module is far from being a clone of <span class="docEmphasis"><TT>Spark</TT></span>—the APIs are significantly different—there is a very similar <span class="docEmphasis">feeling</span> to working with each module. Both modules require a very different style of programming and style of thinking than do <span class="docEmphasis"><TT>mx.TextTools</TT>, <TT>SimpleParse</TT></span>, or the state machines discussed earlier in this chapter. In particular, both <span class="docEmphasis"><TT>PLY</TT></span> and <span class="docEmphasis"><TT>Spark</TT></span> make heavy use of Python introspection to create underlying state machines out of specially named variables and functions.</P>
<P class="docText">Within an overall similarity, <span class="docEmphasis"><TT>PLY</TT></span> has two main advantages over <span class="docEmphasis"><TT>Spark</TT></span> in a text processing context. The first, and probably greatest, advantage <span class="docEmphasis"><TT>PLY</TT></span> has is its far greater speed. Although <span class="docEmphasis"><TT>PLY</TT></span> has implemented some rather clever optimizations—such as preconstruction of state tables for repeated runs—the main speed difference lies in the fact that <span class="docEmphasis"><TT>PLY</TT></span> uses a far faster, albeit slightly less powerful, parsing algorithm. For text processing applications (as opposed to compiler development), <span class="docEmphasis"><TT>PLY'</TT>s</span> LR parsing is plenty powerful for almost any requirement.</P>
<P class="docText">A second advantage <span class="docEmphasis"><TT>PLY</TT></span> has over every other Python parsing library that I am aware of is a flexible and fine-grained error reporting and error correction facility. Again, in a text processing context, this is particularly important.</P>
<P class="docText">For compiling a programming language, it is generally reasonable to allow compilation to fail in the case of even small errors. But for processing a text file full of data fields and structures, you usually want to be somewhat tolerant of minor formatting errors; getting as much data as possible from a text automatically is frequently the preferred approach. <span class="docEmphasis"><TT>PLY</TT></span> does an excellent job of handling "allowable" error conditions gracefully.</P>
<P class="docText"><span class="docEmphasis"><TT>PLY</TT></span> consists of two modules: a lexer/tokenizer named <TT>lex.py</TT>, and a parser named <TT>yacc.py</TT>. The choice of names is taken from the popular C-oriented tools <TT>lex</TT> and <TT>yacc</TT>, and the behavior is correspondingly similar. Parsing with <span class="docEmphasis"><TT>PLY</TT></span> usually consists of the two steps that were discussed at the beginning of this chapter: (1) Divide the input string into a set of nonoverlapping tokens using <TT>lex.py</TT>. (2) Generate a parse tree from the series of tokens using <TT>yacc.py</TT>.</P>
<P class="docText">When processing text with <span class="docEmphasis"><TT>PLY</TT></span>, it is possible to attach "action code" to any lexing or parsing event. Depending on application requirements, this is potentially much more powerful than <span class="docEmphasis"><TT>SimpleParse</TT></span>. For example, each time a specific token is encountered during lexing, you can modify the stored token according to whatever rule you wish, or even trigger an entirely different application action. Likewise, during parsing, each time a node of a parse tree is constructed, the node can be modified and/or other actions can be taken. In contrast, <span class="docEmphasis"><TT>SimpleParse</TT></span> simply delivers a completed parse tree (called a "taglist") that must be traversed separately. However, while <span class="docEmphasis"><TT>SimpleParse</TT></span> does not provide the fine-tunable event control that <span class="docEmphasis"><TT>PLY</TT></span> does, <span class="docEmphasis"><TT>SimpleParse</TT></span> offers a higher-level and cleaner grammar language—the choice between the two modules is full of pros and cons.</P>
<A NAME="ch04lev5sec7"></A><H5 class="docSection3Title"> Example: Marking up smart ASCII (yet again)</H5>
<P class="docText">This chapter has returned several times to applications for processing smart ASCII: a state machine in <A class="docLink" HREF="0321112547_app04.html#app04">Appendix D</A>; a functionally similar example using <span class="docEmphasis"><TT>mx.TextTools</TT></span>; an EBNF grammar with <span class="docEmphasis"><TT>SimpleParse</TT></span>. This email-like markup format is not in itself all that important, but it presents just enough complications to make for a good comparison between programming techniques and libraries. In many ways, an application using <span class="docEmphasis"><TT>PLY</TT></span> is similar to the <span class="docEmphasis"><TT>SimpleParse</TT></span> version above—both use grammars and parsing strategies.</P>
<A NAME="ch04lev3sec23"></A><H5 class="docSection3Title"> GENERATING A TOKEN LIST</H5>
<P class="docText">The first step in most <span class="docEmphasis"><TT>PLY</TT></span> applications is the creation of a token stream. Tokens are identified by a series of regular expressions attached to special pattern names of the form <TT>t_RULENAME</TT>. By convention, the <span class="docEmphasis"><TT>PLY</TT></span> token types are in all caps. In the simple case, a regular expression string is merely assigned to a variable. If action code is desired when a token is recognized, the rule name is defined as a function, with the regular expression string as its docstring; passed to the function is a <TT>LexToken</TT> object (with attributes <TT>.value</TT>, <TT>.type</TT>, and .<TT>lineno)</TT>, which may be modified and returned. The pattern is clear in practice:</P>
<H5 class="docExampleTitle"><A NAME="ch04list22"></A> wordscanner.py</H5>

<PRE>
# List of token names. This is always required.
tokens = [ 'ALPHANUMS','SAFEPUNCT','BRACKET','ASTERISK',
          'UNDERSCORE','APOSTROPHE','DASH' ]

# Regular expression rules for simple tokens
t_ALPHANUMS     = r"[a-zA-ZO-9]+"
t_SAFEPUNCT     = r'[!@#$%^&amp;()+=|\{}:;&lt;&gt;,.?/"]+'
t_BRACKET       = r'[][]'
t_ASTERISK      = r'[*]'
t_UNDERSCORE    = r'_'
t_APOSTROPHE    = r"'"
t_DASH          = r'-'

# Regular expression rules with action code
def t_newline(t):
    r"\n+"
    t.lineno += len(t.value)

# Special case (faster) ignored characters
t_ignore = " \t\r"

# Error handling rule
def t_error(t):
    sys.stderr.write("Illegal character '%s' (%s)\n"
                     % (t.value[0], t.lineno))
    t.skip(1)

import lex, sys
def stdin2tokens():
    lex.input(sys.stdin.read())     # Give the lexer some input
    toklst = []                     # Tokenize
    while 1:
        t = lex.token()
        if not t: break   # No more input
        toklst.append(t)
    return toklst

if __name__=='__main__':
    lex.lex()                       # Build the lexer
    for t in stdin2tokens():
        print '%s&lt;%s&gt;' % (t.value.ljust(15), t.type)
</PRE>
<P class="docText">You are required to list the token types you wish to recognize, using the <TT>tokens</TT> variable. Each such token, and any special patterns that are not returned as tokens, is defined either as a variable or as a function. After that, you just initialize the lexer, read a string, and pull tokens off sequentially. Let us look at some results:</P>
<pre>
% cat p.txt
-Itals-, [modname]--let's add ~ underscored var_name.
% python wordscanner.py &lt; p.txt
Illegal character '~' (1)
-              &lt;DASH&gt;
Itals          &lt;ALPHANUMS&gt;
-              &lt;DASH&gt;
,              &lt;SAFEPUNCT&gt;
[              &lt;BRACKET&gt;
modname        &lt;ALPHANUMS&gt;
]              &lt;BRACKET&gt;
-              &lt;DASH&gt;
-              &lt;DASH&gt;
let            &lt;ALPHANUMS&gt;
'              &lt;APOSTROPHE&gt;
s              &lt;ALPHANUMS&gt;
add            &lt;ALPHANUMS&gt;
underscored    &lt;ALPHANUMS&gt;
var            &lt;ALPHANUMS&gt;
-              &lt;UNDERSCORE&gt;
name           &lt;ALPHANUMS&gt;
.              &lt;SAFEPUNCT&gt;
</pre>
<P class="docText">The output illustrates several features. For one thing, we have successfully tagged each nondiscarded substring as constituting some token type. Notice also that the unrecognized tilde character is handled gracefully by being omitted from the token list—you could do something different if desired, of course. Whitespace is discarded as insignificant by this tokenizer—the special <TT>t-ignore</TT> variable quickly ignores a set of characters, and the <TT>t_newline()</TT> function contains some extra code to maintain the line number during processing.</P>
<P class="docText">The simple tokenizer above has some problems, however. Dashes can be used either in an m-dash or to mark italicized phrases; apostrophes can be either part of a contraction or a marker for a function name; underscores can occur both to mark titles and within variable names. Readers who have used <span class="docEmphasis"><TT>Spark</TT></span> will know of its capability to enhance a lexer or parser by inheritance; <span class="docEmphasis"><TT>PLY</TT></span> cannot do that, but it can utilize Python namespaces to achieve almost exactly the same effect:</P>
<H5 class="docExampleTitle"><A NAME="ch04list23"></A> wordplusscanner.py</H5>

<PRE>
"Enhanced word/markup tokenization"
from wordscanner import  *
tokens.extend(['CONTRACTION','MDASH','WORDPUNCT'])
t_CONTRACTION   = r"(?&lt;=[a-zA-Z])'(am|clock|d|ll|m|re|s|t|ve)"
t_WORDPUNCT     = r'(?&lt;=[a-zA-Z0-9])[-_](?=[a-zA-Z0-9])'
def t_MDASH(t): # Use HTML style mdash
    r'--'
    t.value = '&amp;mdash;'
    return t

if __name__=='__main__':
    lex.lex()                       # Build the lexer
    for t in stdin2tokens():
        print '%s&lt;%s&gt;' % (t.value.ljust(15), t.type)
</PRE>
<P class="docText">Although the tokenization produced by <TT>wordscanner.py</TT> would work with the right choice of grammar rules, producing more specific tokens allows us to simplify the grammar accordingly. In the case of <TT>t_MDASH()</TT>, <TT>wordplusscanner.py</TT> also modifies the token itself as part of recognition:</P>
<pre>
% python wordplusscanner.py &lt; p.txt
Illegal character '~' (1)
-              &lt;DASH&gt;
Itals          &lt;ALPHANUMS&gt;
-              &lt;DASH&gt;
,              &lt;SAFEPUNCT&gt;
[              &lt;BRACKET&gt;
modname        &lt;ALPHANUMS&gt;
]              &lt;BRACKET&gt;
&amp;mdash;        &lt;MDASH&gt;
let            &lt;ALPHANUMS&gt;
's             &lt;CONTRACTION&gt;
add            &lt;ALPHANUMS&gt;
underscored    &lt;ALPHANUMS&gt;
var            &lt;ALPHANUMS&gt;
_              &lt;WORDPUNCT&gt;
name           &lt;ALPHANUMS&gt;
.              &lt;SAFEPUNCT&gt;
</pre>
<A NAME="ch04lev5sec8"></A><H5 class="docSection4Title"> Parsing a token list</H5>
<P class="docText">A parser in <span class="docEmphasis"><TT>PLY</TT></span> is defined in almost the same manner as a tokenizer. A collection of specially named functions of the form <TT>p_rulename()</TT> are defined, each containing an EBNF-style pattern to match (or a disjunction of several such patterns). These functions receive as argument a <TT>YaccSlice</TT> object, which is list-like in assigning each component of the EBNF declaration to an indexed position.</P>
<P class="docText">The code within each function should assign a useful value to <TT>t[0]</TT>, derived in some way from <TT>t[1:]</TT>. If you would like to create a parse tree out of the input source, you can define a <TT>Node</TT> class of some sort and assign each right-hand rule or token as a subnode/leaf of that node; for example:</P>
<pre>
def p_rulename(t):
    'rulename : somerule SOMETOKEN otherrule'
    #   ^          ^         ^         ^
    #  t[0]       t[1]      t[2]      t[3]
    t[0] = Node('rulename', t[l:])
</pre>
<P class="docText">Defining an appropriate <TT>Node</TT> class is left as an exercise. With this approach, the final result would be a traversable tree structure.</P>
<P class="docText">It is fairly simple to create a set of rules to combine the fairly smart token stream produced by <TT>wordplusscanner.py</TT>. In the sample application, a simpler structure than a parse tree is built. <TT>markupbuilder.py</TT> simply creates a list of matched patterns, interspersed with added markup codes. Other data structures are possible too, and/or you could simply take some action each time a rule is matched (e.g., write to STDOUT).</P>
<H5 class="docExampleTitle"><A NAME="ch04list24"></A> markupbuilder.py</H5>

<PRE>
import yacc
from wordplusscanner import *

def p_para(t):
    '''para : para plain
            | para emph
            | para strong
            | para module
            | para code
            | para title
            | plain
            | emph
            | strong
            | module
            | code
            | title '''
    try:    t[0] = t[1] + t[2]
    except: t[0] = t[1]

def p_plain(t):
    '''plain : ALPHANUMS
             | CONTRACTION
             | SAFEPUNCT
             | MDASH
             | WORDPUNCT
             | plain plain '''
    try:    t[0] = t[1] + t[2]
    except: t[0] = [t[l]]

def p_emph(t):
    '''emph : DASH plain DASH'''
    t[0] = ['&lt;i&gt;'] + t[2] + ['&lt;/i&gt;']

def p_strong(t):
    '''strong : ASTERISK plain ASTERISK"'
    t[0] = ['&lt;b&gt;'] + t[2] + ['&lt;/b&gt;']

def p_module(t):
    '''module : BRACKET plain BRACKET'''
    t[0] = ['&lt;em&gt;&lt;tt&gt;'] + t[2] + ['&lt;/tt&gt;&lt;/em&gt;']

def p_code(t):
    '''code : APOSTROPHE plain APOSTROPHE'''
    t[0] = ['&lt;code&gt;'] + t[2] + ['&lt;/code&gt;']

def p_title(t):
    '''title : UNDERSCORE plain UNDERSCORE'''
    t[0] = ['&lt;cite&gt;'] + t[2] + ['&lt;/cite&gt;']

def p_error(t):
    sys.stderr.write('Syntax error at "%s" (%s)\n'
                     % (t.value,t.lineno))

if __name__=='__main__':
    lex.lex()               # Build the lexer
    yacc.yacc()             # Build the parser
    result = yacc.parse(sys.stdin.read())
    print result
</PRE>
<P class="docText">The output of this script, using the same input as above, is:</P>
<pre>
% python markupbuilder.py &lt; p.txt
Illegal character '~' (1)
['&lt;i&gt;', 'Itals', '&lt;/i&gt;', ',', '&lt;em&gt;&lt;tt&gt;', 'modname',
'&lt;/tt&gt;&lt;/em&gt;', '&amp;mdash;', 'let', "'s", 'add', 'underscored',
'var', '_', 'name', '.']
</pre>
<P class="docText">One thing that is less than ideal in the <span class="docEmphasis"><TT>PLY</TT></span> grammar is that it has no quantifiers. In <span class="docEmphasis"><TT>SimpleParse</TT></span> or another EBNF library, we might give, for example, a <TT>plain</TT> declaration as:</P>
<pre>
plain := (ALPHANUMS | CONTRACTION | SAFEPUNCT | MDASH | WORDPUNCT)+
</pre>
<P class="docText">Quantification can make declarations more direct. But you can achieve the same effect by using self-referential rules whose left-hand terms also occur on the right-hand side. This style is similar to recursive definitions, for example:</P>
<pre>
plain : plain plain
      | OTHERSTUFF
</pre>
<P class="docText">For example, <TT>markupbuilder.py</TT>, above, uses this technique.</P>
<P class="docText">If a tree structure were generated in this parser, a <TT>plain</TT> node might wind up being a subtree containing lower <TT>plain</TT> nodes (and terminal leaves of <TT>ALPHANUMS</TT>, <TT>CONTRACTION</TT>, etc.). Traversal would need to account for this possibility. The flat list structure used simplifies the issue, in this case. A particular <TT>plain</TT> object might result from the concatenation of several smaller lists, but either way it is a list by the time another rule includes the object.</P>
<A NAME="ch04lev3sec24"></A><H5 class="docSection3Title"> LEX</H5>
<P class="docText">A <span class="docEmphasis"><TT>PLY</TT></span> lexing module that is intended as support for a parsing application must do four things. A lexing module that constitutes a stand-alone application must do two additional things:</P>
<A NAME="ch04pr02"></A>





<span style="font-weight:bold"><OL class="docList" START="1"><LI><span style="font-weight:normal" value="1"><P class="docText">Import the <span class="docEmphasis"><TT>lex</TT></span> module:</P>
<pre>
import lex
</pre></span></LI><LI><span style="font-weight:normal" value="2"><P class="docText">Define a list or tuple variable <TT>tokens</TT> that contains the name of every token type the lexer is allowed to produce. A list may be modified in-place should you wish to specialize the lexer in an importing module; for example:</P>
<pre>
tokens = ['FOO', 'BAR', 'BAZ', 'FLAM']
</pre></span></LI><LI><span style="font-weight:normal" value="3"><P class="docList">Define one or more regular expression patterns matching tokens. Each token type listed in <TT>tokens</TT> should have a corresponding pattern; other patterns may be defined also, but the corresponding substrings will not be included in the token stream.</P>
<P class="docList">Token patterns may be defined in one of two ways: (1) By assigning a regular expression string to a specially named variable. (2) By defining a specially named function whose docstring is a regular expression string. In the latter case, "action code" is run when the token is matched. In both styles, the token name is preceded by the prefix <TT>t_</TT>. If a function is used, it should return the <TT>LexToken</TT> object passed to it, possibly after some modification, unless you do not wish to include the token in the token stream. For example:</P>
<pre>
t_FOO = r"[Ff] [Oo]{1,2}"
t_BAR = r"[Bb][Aa][Rr]"
def t_BAZ(t):
    r"([Bb] [Aa] [Zz])+"
    t.value = 'BAZ'     # canonical caps BAZ
    return t
def t_FLAM(t):
    r"(FLAM|flam)*"
    # flam's are discarded (no return)
</pre>
<P class="docList">Tokens passed into a pattern function have three attributes: <TT>.type</TT>, <TT>.value</TT>, and <TT>.lineno. .lineno</TT> contains the current line number within the string being processed and may be modified to change the reported position, even if the token is not returned. The attribute <TT>.value</TT> is normally the string matched by the regular expression, but a new string, or a compound value like a tuple or instance, may be assigned instead. The <TT>.type</TT> of a <TT>LexToken</TT>, by default, is a string naming the token (the same as the part of the function name after the <TT>t_</TT> prefix).</P>
<P class="docList">There is a special order in which various token patterns will be considered. Depending on the patterns used, several patterns could grab the same substring—so it is important to allow the desired pattern first claim on a substring. Each pattern defined with a function is considered in the order it is defined in the lexer file; all patterns defined by assignment to a variable are considered <span class="docEmphasis">after</span> every function-defined pattern. Patterns defined by variable assignment, however, are not considered in the order they are defined, but rather by decreasing length. The purpose of this ordering is to let longer patterns match before their subsequences (e.g., "==" would be claimed before "=" , allowing the former comparison operator to match correctly, rather than as sequential assignments).</P>
<P class="docList">The special variable <TT>t_ignore</TT> may contain a string of characters to skip during pattern matching. These characters are skipped more efficiently than is a token function that has no return value. The token name <TT>ignore</TT> is, therefore, reserved and may not be used as a regular token (if the all-cap token name convention is followed, it assures no such conflict).</P>
<P class="docText">The special function <TT>t_error()</TT> may be used to process illegal characters. The <TT>.value</TT> attribute of the passed-in <TT>LexToken</TT> will contain the remainder of the string being processed (after the last match). If you want to skip past a problem area (perhaps after taking some corrective action in the body of <TT>t_error())</TT>, use the <TT>.skip()</TT> method of the passed-in <TT>LexToken</TT>.</P></span></LI><LI><span style="font-weight:normal" value="4"><P class="docText">Build the lexer. The <span class="docEmphasis"><TT>lex</TT></span> module performs a bit of namespace magic so that you normally do not need to name the built lexer. Most applications can use just one default lexer. However, if you wish to—or if you need multiple lexers in the same application—you may bind a built lexer to a name. For example:</P>
<pre>
mylexer = lex.lex()   # named lexer
lex.lex()             # default lexer
mylexer.input(mytext) # set input for named lexer
lex.input(othertext)  # set input for default lexer
</pre></span></LI><LI><span style="font-weight:normal" value="5"><P class="docText">Give the lexer a string to process. This step is handled by the parser when <span class="docEmphasis"><TT>yacc</TT></span> is used in conjunction with <span class="docEmphasis"><TT>lex</TT></span>, and nothing need be done explicitly. For standalone tokenizers, set the input string using <TT>lex.input()</TT> (or similarly with the <TT>.input()</TT> method of named lexers).</P></span></LI><LI><span style="font-weight:normal" value="6"><P class="docList">Read the token stream (for stand-alone tokenizers) using repeated invocation of the default <TT>lex.token()</TT> function or the <TT>.token()</TT> method of a named lexer. Unfortunately, as of version 1.1, <span class="docEmphasis"><TT>PLY</TT></span> does not treat the token stream as a Python 2.2 iterator/generator. You can create an iterator wrapper with:</P>
<pre>
from __future__ import generators
# ...define the lexer rules, etc...
def tokeniterator(lexer=lex):
    while 1:
        t = lexer.token()
        if t is None:
            raise StopIteration
        yield t
# Loop through the tokens
for t in tokeniterator():
    # ...do something with each token...
</pre>
<P class="docText">Without this wrapper, or generally in earlier versions of Python, you should use a <TT>while 1 loop</TT> with a break condition:</P>
<pre>
# ...define the lexer rules, etc...
while 1:
    t = lex.token()
    if t is None:   # No more input
        break
    # ... do something with each token...
</pre></span></LI></OL></span>
<A NAME="ch04lev3sec25"></A><H5 class="docSection3Title"> YACC</H5>
<P class="docText">A <span class="docEmphasis"><TT>PLY</TT></span> parsing module must do five things:</P>
<A NAME="ch04pr03"></A>




<span style="font-weight:bold"><OL class="docList" START="1"><LI><span style="font-weight:normal" value="1"><P class="docText">Import the <TT>yacc</TT> module:</P>
<pre>
import yacc
</pre></span></LI><LI><span style="font-weight:normal" value="2"><P class="docList">Get a token map from a lexer. Suppose a lexer module named <TT>mylexer.py</TT> includes requirements 1 through 4 in the above LEX description. You would get the token map with:</P>
<pre>
from mylexer import *
</pre>
<P class="docList">Given the special naming convention <TT>t_*</TT> used for token patterns, the risk of namespace pollution from <TT>import *</TT> is minimal.</P>
<P class="docText">You could also, of course, simply include the necessary lexer setup code in the parsing module itself.</P></span></LI><LI><span style="font-weight:normal" value="3"><P class="docList">Define a collection of grammar rules. Grammar rules are defined in a similar fashion to token functions. Specially named functions having a <TT>p_</TT> prefix contain one or more productions and corresponding action code. Whenever a production contained in the docstring of a <TT>p_*()</TT> function matches, the body of that function runs.</P>
<P class="docList">Productions in <span class="docEmphasis"><TT>PLY</TT></span> are described with a simplified EBNF notation. In particular, no quantifiers are available in rules; only sequencing and alternation is used (the rest must be simulated with recursion and component productions).</P>
<P class="docList">The left side of each rule contains a single rule name. Following the rule name is one or more spaces, a colon, and an additional one or more spaces. The right side of a rule is everything following this. The right side of a rule can occupy one or more lines; if alternative patterns are allowed to fulfill a rule name, each such pattern occurs on a new line, following a pipe symbol ("<TT>|</TT>"). Within each right side line, a production is defined by a space-separated sequence of terms—which may be either tokens generated by the lexer or parser productions. More than one production may be included in the same <TT>p_*()</TT> function, but it is generally more clear to limit each function to one production (you are free to create more functions). For example:</P>
<pre>
def p_rulename(t):
    '''rulename   : foo SPACE bar
                  | foo bar baz
                  | bar SPACE baz
       otherrule  : this that other
                  | this SPACE that '''
#...action code...
</pre>
<P class="docList">The argument to each <TT>p_*()</TT> function is a <TT>YaccSlice</TT> object, which assigns each component of the rule to an indexed position. The left side rule name is index position 0, and each term/token on the right side is listed thereafter, left to right. The list-like <TT>YaccSlice</TT> is sized just large enough to contain every term needed; this might vary depending on which alternative production is fulfilled on a particular call.</P>
<P class="docList">Empty productions are allowed by <span class="docEmphasis"><TT>yacc</TT></span> (matching zero-width); you never need more than one empty production in a grammar, but this empty production might be a component of multiple higher-level productions. An empty production is basically a way of getting around the absence of (potentiality) quantification in <span class="docEmphasis"><TT>PLY</TT></span>; for example:</P>
<pre>
def p_empty(t):
    '''empty : '''
    pass
def p_maybefoo(t):
    '''foo : FOOTOKEN
           | empty '''
    t[0] = t[1]
def p_maybebar(t):
    '''bar : BARTOKEN
           | empty '''
    t[0] = t[l]
</pre>
<P class="docList">If a fulfilled production is used in other productions (including itself recursively), the action code should assign a meaningful value to index position 0. This position <span class="docEmphasis">is</span> the value of the production. Moreover what is returned by the actual parsing is this position 0 of the top-level production. For example:</P>
<pre>
# Sum N different numbers: "1.0 + 3 + 3.14 + 17"
def p_sum(t):
    '''sum : number PLUS number'''
    #   ^      ^      ^    ^
    #  t[0]   t[1]  t[2]  t[3]
    t[0] = t[1] + t[3]
def p_number(t):
    '''number : BASICNUMBER
              | sum         '''
    #    ^        ^
    #   t[0]     t[1]
    t[0] = float(t[1])
# Create the parser and parse some strings
yacc.yacc()
print yacc.parse('1.0')
</pre>
<P class="docText">The example simply assigns a numeric value with every production, but it could also assign to position 0 of the <TT>YaccSlice</TT> a list, <TT>Node</TT> object, or some other data structure that was useful to higher-level productions.</P></span></LI><LI><span style="font-weight:normal" value="4"><P class="docList">To build the parser the <span class="docEmphasis"><TT>yacc</TT></span> module performs a bit of namespace magic so that you normally do not need to name the built parser. Most applications can use just one default parser. However, if you wish to—or if you need multiple parsers in the same application—you may bind a built parser to a name. For example:</P>
<pre>
myparser = yacc.yacc()      # named parser
yacc.yacc()                 # default parser
r1 = myparser.parse(mytext) # set input for named parser
r0 = yacc.parse(othertext)  # set input for default parser
</pre>
<P class="docText">When parsers are built, <span class="docEmphasis"><TT>yacc</TT></span> will produce diagnostic messages if any errors are encountered in the grammar.</P></span></LI><LI><span style="font-weight:normal" value="5"><P class="docText">Parse an input string. The lexer is implicitly called to get tokens as needed by the grammar rules. The return value of a parsing action can be whatever thing invocation of matched rules builds. It might be an abstract syntax tree, if a <TT>Node</TT> object is used with each parse rule; it might be a simple list as in the smart ASCII example; it might be a modified string based on concatenations and modifications during parsing; or the return value could simply be <TT>None</TT> if parsing was done wholly to trigger side effects in parse rules. In any case, what is returned is index position 0 of the root rule's <TT>LexToken</TT>.</P></span></LI></OL></span>
<A NAME="ch04lev3sec26"></A><H5 class="docSection3Title"> MORE ON PLY PARSERS</H5>
<P class="docText">Some of the finer points of <span class="docEmphasis"><TT>PLY</TT></span> parsers will not be covered in this book. The documentation accompanying <span class="docEmphasis"><TT>PLY</TT></span> contains some additional implementational discussion, and a book devoted more systematically to parsing theory will address theoretical issues. But a few aspects can at least be touched on.</P>
<A NAME="ch04lev5sec9"></A><H5 class="docSection4Title"> Error Recovery</H5>
<P class="docText">A <span class="docEmphasis"><TT>PLY</TT></span> grammar may contain a special <TT>p_error()</TT> function to catch tokens that cannot be matched (at the current position) by any other rule. The first time <TT>p_error()</TT> is invoked, <span class="docEmphasis"><TT>PLY</TT></span> enters an "error-recovery" mode. If the parser cannot process the next three tokens successfully, a traceback is generated. You may include the production error in other rules to catch errors that occur at specific points in the input.</P>
<P class="docText">To implement recovery within the <TT>p_error()</TT> function, you may use the functions/methods <TT>yacc.token()</TT>, <TT>yacc.restart()</TT>, and <TT>yacc.errok()</TT>. The first grabs the next token from the lexer; if this token—or some sequence of tokens—meets some recovery criteria, you may call <TT>yacc.restart()</TT> or <TT>yacc.errok()</TT>. The first of these, <TT>yacc.restart()</TT>, returns the parser to its initial state—basically, only the final sub-string of the input is used in this case (however, a separate data structure you have built will remain as it was). Calling <TT>yacc.errok()</TT> tells the parser to stay in its last state and just ignore any bad tokens pulled from the lexer (either via the call to <TT>p_error()</TT> itself, or via calls to <TT>yacc.token()</TT> in the body).</P>
<A NAME="ch04lev5sec10"></A><H5 class="docSection4Title"> The Parser State Machine</H5>
<P class="docText">When a parser is first compiled, the files <TT>parsetab.py</TT> and <TT>parser.out</TT> are generated. The first, <TT>parsetab.py</TT>, contains more or less unreadable compact data structures that are used by subsequent parser invocations. These structures are used even during later invocation of the applications; timestamps and signatures are compared to determine if the grammar has been changed. Pregenerating state tables speeds up later operations.</P>
<P class="docText">The file <TT>parser.out</TT> contains a fairly readable description of the actual state machine generated by <span class="docEmphasis"><TT>yacc</TT></span>. Although you cannot manually modify this state machine, examination of <TT>parser.out</TT> can help you in understanding error messages and undesirable behavior you encounter in your grammars.</P>
<A NAME="ch04lev5sec11"></A><H5 class="docSection4Title"> Precedence and Associativity</H5>
<P class="docText">To resolve ambiguous grammars, you may set the variable <TT>precedence</TT> to indicate both the precedence and the associativity of tokens. Absent an explicit indication, <span class="docEmphasis"><TT>PLY</TT></span> always shifts a new symbol rather than reduce a rule where both are allowable by some grammar rule.</P>
<P class="docText">The <span class="docEmphasis"><TT>PLY</TT></span> documentation gives an example of an ambiguous arithmetic expression, such as <TT>3 * 4 + 5</TT>. After the tokens <TT>3</TT>, <TT>*</TT>, and <TT>4</TT> have been read from the token list, a <TT>p_mul()</TT> rule might allow reduction of the product. But at the same time, a <TT>p_add()</TT> rule might contain <TT>NUMBER PLUS NUMBER</TT>, which would allow a lookahead to the <TT>PLUS</TT> token (since <TT>4</TT> is a <TT>NUMBER</TT> token). Moreover, the same token can have different meanings in different contexts, such as the unary-minus and minus operators in <TT>3 - 4 * -5</TT>.</P>
<P class="docText">To solve both the precedence ambiguity and the ambiguous meaning of the token <TT>MINUS</TT>, you can declare an explicit precedence and associativity such as:</P>
<H5 class="docExampleTitle"><A NAME="ch04list25"></A> Declaring precedence and associativity</H5>

<PRE>
precedence = (
    ('left', 'PLUS', 'MINUS'),
    ('left', 'TIMES, 'DIVIDE'),
    ('right', 'UMINUS'),
)
def p_expr_uminus(t):
    'expr : MINUS expr % prec UMINUS'
    t[0] = -1 * t[2]
def p_expr_minus(t):
    'expr : expr MINUS expr'
    t[0] = t[1] - t[3]
def p_expr_plus(t):
    'expr : expr PLUS expr'
    t[0] = t[1] + t[3]
</PRE>
<a href="0321112547_10061533.html"><img src="FILES/pixel.gif" width="1" height="1" border="0"></a><ul></ul></td></tr></table>
<td></td>
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<td class="tt1"><a href="NFO/lib.html">[ Team LiB ]</a></td><td valign="top" class="tt1" align="right">
          <a href="0321112547_ch04lev1sec2.html"><img src="FILES/btn_prev.gif" width="62" height="15" border="0" align="absmiddle" alt="Previous Section"></a>
          <a href="0321112547_ch05.html"><img src="FILES/btn_next.gif" width="41" height="15" border="0" align="absmiddle" alt="Next Section"></a>
</td></table>
</body></html>
