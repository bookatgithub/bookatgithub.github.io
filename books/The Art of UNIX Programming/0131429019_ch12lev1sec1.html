<html><head>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<!--SafClassName="docSection1Title"--><!--SafTocEntry="12.1 Don't Just Do Something, Stand There!"-->
<link rel="STYLESHEET" type="text/css" href="FILES/style.css">
<link rel="STYLESHEET" type="text/css" href="FILES/docsafari.css">
<style type="text/css">	.tt1    {font-size: 10pt;}</style>
</head>
<body>
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<td class="tt1"><a href="NFO/lib.html">[ Team LiB ]</a></td><td valign="top" class="tt1" align="right">
	<a href="0131429019_ch12.html"><img src="FILES/btn_prev.gif" width="62" height="15" border="0" align="absmiddle" alt="Previous Section"></a>
	<a href="0131429019_ch12lev1sec2.html"><img src="FILES/btn_next.gif" width="41" height="15" border="0" align="absmiddle" alt="Next Section"></a>
</td></table>
<br>
<table width="100%" border="0" cellspacing="0" cellpadding="0"><tr><td valign="top"><A NAME="ch12lev1sec1"></A>
<H3 class="docSection1Title">12.1 Don't Just Do Something, Stand There!</H3>
<P class="docText"><A NAME="idd1e27935"></A>The most powerful optimization technique in any programmer's toolbox is to do nothing.</P>
<P class="docText">This very Zen<A NAME="idd1e27944"></A> advice is true for several reasons. One is the exponential effect of Moore's<A NAME="idd1e27948"></A> Law—the smartest, cheapest, and often <span class="docEmphasis">fastest</span> way to collect performance gains is to wait a few months for your target hardware to become more capable. Given the cost ratio between hardware and programmer time, there are almost always better things to do with your time than to optimize a working system.</P>
<P class="docText">We can get mathematically specific about this. It is almost never worth doing optimizations that reduce resource use by merely a constant factor; it's smarter to concentrate effort on cases in which you can reduce average-case running time or space use from O(<span class="docEmphasis">n</span><SUP>2</SUP>) to O(<span class="docEmphasis">n</span>) or O(<span class="docEmphasis">n</span> log <span class="docEmphasis">n</span>),<sup class="docFootnote"><A class="docLink" HREF="#ch12en01">[1]</A></sup> or similarly reduce from a higher order. Linear performance gains tend to be rapidly swamped by Moore's Law.<sup class="docFootnote"><A class="docLink" HREF="#ch12en02">[2]</A></sup></P><blockquote><p class="docFootnote"><sup><A NAME="ch12en01">[1]</A></sup> For readers unfamiliar with O notation<A NAME="idd1e27975"></A>, it is a way of indicating how the average running time of an algorithm changes with the size of its inputs. An O(1) algorithm runs in constant time. An O(<span class="docEmphasis">n</span>) algorithm runs in a time that is predicted by <TT>A</TT><span class="docEmphasis"><TT>n</TT></span> <TT>+ C</TT>, where <TT>A</TT> is some unknown constant of proportionality and <TT>C</TT> is an unknown constant representing setup time. Linear search of a list for a specified value is O(<span class="docEmphasis">n</span>). An O(<span class="docEmphasis">n</span><SUP>2</SUP>) algorithm runs in time <TT>A</TT><span class="docEmphasis"><TT>n</TT></span><SUP>2</SUP> plus lower-order terms (which might be linear, or logarithmic, of any other function lower than a quadratic). Checking a list for duplicate values (by the na&iuml;ve method, not sorting it) is O(<span class="docEmphasis">n</span><SUP>2</SUP>). Similarly, O(<span class="docEmphasis">n</span><SUP>3</SUP>) algorithms have an average run time predicted by the cube of problem size; these tend to be too slow for practical use. O(log <span class="docEmphasis">n</span>) is typical of tree searches. Intelligent choice of algorithm can often reduce running time from O(<span class="docEmphasis">n</span><SUP>2</SUP>) to O(log <span class="docEmphasis">n</span>). Sometimes when we are interested in predicting an algorithm's memory utilization, we may notice that it varies as O(1) or O(<span class="docEmphasis">n</span>) or O(<span class="docEmphasis">n</span><SUP>2</SUP>); in general, algorithms with O(<span class="docEmphasis">n</span><SUP>2</SUP>) or higher memory utilization are not practical either.</p></blockquote><blockquote><p class="docFootnote"><sup><A NAME="ch12en02">[2]</A></sup> The eighteen-month doubling time usually quoted for Moore's Law implies that you can collect a 26% performance gain just by buying new hardware in six months.</p></blockquote>
<P class="docText">Another very constructive form of doing nothing is to not write code. The program can't be slowed down by code that isn't there. It can be slowed down by code that <span class="docEmphasis">is</span> there but less efficient than it could be—but that's a different matter.</P>

<a href="0131429019_18071533.html"><img src="FILES/pixel.gif" width="1" height="1" border="0"></a><ul></ul></td></tr></table>
<td></td>
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<td class="tt1"><a href="NFO/lib.html">[ Team LiB ]</a></td><td valign="top" class="tt1" align="right">
          <a href="0131429019_ch12.html"><img src="FILES/btn_prev.gif" width="62" height="15" border="0" align="absmiddle" alt="Previous Section"></a>
          <a href="0131429019_ch12lev1sec2.html"><img src="FILES/btn_next.gif" width="41" height="15" border="0" align="absmiddle" alt="Next Section"></a>
</td></table>
</body></html>
