<HTML><HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<!--SafClassName="docSection1Title"--><!--SafTocEntry="Ruthless Testing"-->
<LINK REL="stylesheet" href="FILES/proquestM.css">
</HEAD>
<BODY link="#354278" vlink="#000066" alink="#000080" topmargin="0">
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#CCCCCC">
<td><font color="black" size=1>I l<font color="#FF0000">@</font>ve RuBoard</td>
<td valign="top" class="v2" align="right"><a href="020161622x_snode77.html"><img src="FILES/previous.gif" width="62" height="15" border="0" align="absmiddle" alt="Previous Section"></a> <a href="020161622x_snode79.html"><img src="FILES/next.gif" width="41" height="15" border="0" align="absmiddle" alt="Next Section"></a></td></table>
<FONT><h3>Ruthless Testing</h3>
			<p>Most developers hate testing. They tend to test gently, subconsciously knowing where the code will break and avoiding the weak spots. Pragmatic Programmers are different. We are <i>driven</I> to find our bugs <I>now,</I> so we don't have to endure the shame of others finding our bugs later.</P>

			<p>Finding bugs is somewhat like fishing with a net. We use fine, small nets (unit tests) to catch the minnows, and big, coarse nets (integration tests) to catch the killer sharks. Sometimes the fish manage to escape, so we patch any holes that we find, in hopes of catching more and more slippery defects that are swimming about in our project pool.</p>

			<diV CLAss="note"><p cLASS="notetitle"><b>Tip 62</b></p><p>

				<p>Test Early. Test Often. Test Automatically.</p>

			</p></div>
<br>
<br>

			<p>We want to start testing as soon as we have code. Those tiny minnows have a nasty habit of becoming giant, man-eating sharks pretty fast, and catching a shark is quite a bit harder. But we don't want to have to do all that testing by hand.</p>

			<p>Many teams develop elaborate test plans for their projects. Sometimes they will even use them. But we've found that teams that use automated tests have a much better chance of success. Tests that run with every build are much more effective than test plans that sit on a shelf.</p>

			<p>The earlier a bug is found, the cheaper it is to remedy. "Code a little, test a little" is a popular saying in the Smalltalk world,<fONT Size="1"><suP><a Href="#FOOTNOTE-6">[6]</A></SUP></Font>
 and we can adopt that mantra as our own by writing test code at the same time (or even before) we write the production code.</p>
<BLOCkquoTE><FOnt siZE="1">
<P Class="footnote">
<sup><a name="FOOTNOTE-6">[6]</a></sup>
eXtreme Programming [<a href="020161622x_snode86.html#53">URL 45</A>] calls this concept "continuous Integration, relentless testing."</P>
</Font></blOcKquoTE>

				
			<P>In fact, a good project may well have <I>more</I> test code than production code. The time it takes to produce this test code is worth the effort. It ends up being much cheaper in the long run, and you actually stand a chance of producing a product with close to zero defects.</p>

			<p>Additionally, knowing that you've passed the test gives you a high degree of confidence that a piece of code is "done."</p>

			<dIV CLass="note"><p CLASs="notetitle"><b>Tip 63</b></p><P>

				<P>Coding Ain't Done 'Til All the Tests Run</P>

			</P></div>
<br>
<br>

			<p>Just because you have finished hacking out a piece of code doesn't mean you can go tell your boss or your client that it's <i>done.</i> It's not. First of all, code is never really done. More importantly, you can't claim that it is usable by anyone until it passes all of the available tests.</p>

			<p>We need to look at three main aspects of project-wide testing: what to test, how to test, and when to test.</p>

			<h4>What to Test</h4>
				<p>There are several major types of software testing that you need to perform:</p>

				<ul>
<lI><P>Unit testing</P>
</Li>
<li><p>Integration testing</p>
</Li>
<Li><p>Validation and verification</p>
</LI>
<LI><P>Resource exhaustion, errors, and recovery</p>
</li>
<lI><P>Performance testing</P>
</Li>
<li><p>Usability testing</P>

					</LI>
</Ul>
				<p>This list is by no means complete, and some specialized projects will require various other types of testing as well. But it gives us a good starting point.</p>

				<h5>Unit Testing</H5>
					<P>A <I>unit test</I> is code that exercises a module. We covered this topic by itself in <a href="020161622x_snode65.html">Code That's Easy to Test</a>. Unit testing is the foundation of all the other forms of testing that we'll discuss in this section. If the parts don't work by themselves, they probably won't work well together. All of the modules you are using must pass their own unit tests before you can proceed.</p>

					<p>Once all of the pertinent modules have passed their individual tests, you're ready for the next stage. You need to test how all the modules use and interact with each other throughout the system.</p>

				
				<h5>Integration Testing</h5>
					<p><i>Integration testing</i> shows that the major subsystems that make up the project work and play well with each other. With good contracts in place and well tested, any integration issues can be detected easily. Otherwise, integration becomes a fertile breeding ground for bugs. In fact, it is often the single largest source of bugs in the system.</p>

					<p>Integration testing is really just an extension of the unit testing we've described—only now you're testing how entire subsystems honor their contracts.</p>

				
				<h5>Validation and Verification</h5>
					<p>As soon as you have an executable user interface or prototype, you need to answer an all-important question: the users told you what they wanted, but is it what they need?</P>

					<P>Does it meet the functional requirements of the system? This, too, needs to be tested. A bug-free system that answers the wrong question isn't very useful. Be conscious of end-user access patterns and how they differ from developer test data (for an example, see the story about brush strokes on page 92).</P>

				
				<H5>Resource Exhaustion, Errors, and Recovery</h5>
					<p>Now that you have a pretty good idea that the system will behave correctly under ideal conditions, you need to discover how it will behave under <i>real-world</i> conditions. In the real world, your programs don't have limitless resources; they run out of things. A few limits your code may encounter include:</p>

					<Ul>
<Li><p>Memory</p>
</LI>
<LI><P>Disk space</p>
</li>
<lI><P>CPU bandwidth</P>
</Li>
<li><p>Wall-clock time</P>
</LI>
<Li><p>Disk bandwidth</p>
</lI>
<LI><P>Network bandwidth</p>
</li>
<li><p>Color palette</p>
</li>
<li><p>Video resolution</p>

						</li>
</ul>
					<p>You might actually check for disk space or memory allocation failures, but how often do you test for the others? Will your application fit on a 640 × 480 screen with 256 colors? Will it run on a 1600 × 1280 screen with 24-bit color without looking like a postage stamp? Will the batch job finish before the archive starts?</p>

					<p>You can detect environmental limitations, such as the video specifications, and adapt as appropriate. Not all failures are recoverable, however. If your code detects that memory has been exhausted, your options are limited: you may not have enough resources left to do anything except fail.</P>

					<P>When the system does fail,<FOnt sizE="1"><sUp><a hREF="#FOOTNOTE-7">[7]</A></Sup></foNT>
 will it fail gracefully? Will it try, as best it can, to save its state and prevent loss of work? Or will it "GPF" or "core-dump" in the user's face?</P>
<BlockQUOTe><fonT SIZe="1">
<p class="footnote">
<sup><a name="FOOTNOTE-7">[7]</a></sup>
Our copy editor wanted us to change this sentence to "<i>If</I> the system does fail…." We resisted.</P>
</FOnt></bloCkQuotE>

						
				
				<H5>Performance Testing</H5>
					<P>Performance testing, stress testing, or testing under load may be an important aspect of the project as well.</P>

					<p>Ask yourself if the software meets the performance requirements under real-world conditions—with the expected number of users, or connections, or transactions per second. Is it scalable?</p>

					<p>For some applications, you may need specialized testing hardware or software to simulate the load realistically.</p>

				
				<H5>Usability Testing</H5>
					<P>Usability testing is different from the types of testing discussed so far. It is performed with real users, under real environmental conditions.</P>

					<p>Look at usability in terms of human factors. Were there any misunderstandings during requirements analysis that need to be addressed? Does the software fit the user like an extension of the hand? (Not only do we want our own tools to fit our hands, but we want the tools we create for users to fit their hands as well.)</p>

					<p>As with validation and verification, you need to perform usability testing as early as you can, while there is still time to make corrections. For larger projects, you may want to bring in human factors specialists. (If nothing else, it's fun to play with the one-way mirrors).</p>

					<P>Failure to meet usability criteria is just as big a bug as dividing by zero.</P>

				
			
			<H4>How to Test</H4>
				<p>We've looked at <i>what</i> to test. Now we'll turn our attention to <i>how</I> to test, including:</P>

				<UL>
<li><p>Regression testing</p>
</li>
<li><p>Test data</p>
</li>
<li><p>Exercising GUI systems</p>
</li>
<li><P>Testing the tests</P>
</LI>
<li><p>Testing thoroughly</p>

					</lI>
</uL>
				<p><taBLE CEllspACINg="0" widTH="90%" BOrder="1"><TR><TD><center><h2>
Design/Methodology Testing</h2></center>
					<p>Can you test the design of the code itself and the methodology you used to build the software? After a fashion, yes you can. You do this by analyzing <i>metrics</i>—measurements of various aspects of the code. The simplest metric (and often the least interesting) is <i>lines</i> of <i>code</I>—how big is the code itself?</P>

					<P>There are a wide variety of other metrics you can use to examine code, including:</P>

					<ul>
<li><p>McCabe Cyclomatic Complexity Metric (measures complexity of decision structures)</P>
</lI>
<li><p>Inheritance fan-in (number of base classes) and fan-out (number of derived modules using this one as a parent)</P>
</LI>
<LI><p>Response set (see <a href="020161622x_snode55.html">Decoupling and the Law of Demeter</A>)</P>
</li>
<li><P>Class coupling ratios (see [<A href="020161622x_snode86.html#56">URL 48</a>])</p>

						</LI>
</UL>
					<p>Some metrics are designed to give you a "passing grade," while others are useful only by comparison. That is, you calculate these metrics for every module in the system and see how a particular module relates to its brethren. Standard statistical techniques (such as mean and standard deviation) are usually used here.</p>

					<p>If you find a module whose metrics are markedly different from all the rest, you need to ask yourself if that is appropriate. For some modules, it may be okay to "blow the curve." But for those that <i>don't</i> have a good excuse, it can indicate problems.</p>

				</td></tr></table></p>

				<h5>Regression Testing</h5>
					<p>A regression test compares the output of the current test with previous (or known) values. We can ensure that bugs we fixed today didn't break things that were working yesterday. This is an important safety net, and it cuts down on unpleasant surprises.</p>

					<P>All of the tests we've mentioned so far can be run as regression tests, ensuring that we haven't lost any ground as we develop new code. We can run regressions to verify performance, contracts, validity, and so on.</P>

				
				<H5>Test Data</H5>
					<p>Where do we get the data to run all these tests? There are only two kinds of data: real-world data and synthetic data. We actually need to use both, because the different natures of these kinds of data will expose different bugs in our software.</p>

					<p>Real-world data comes from some actual source. Possibly it has been collected from an existing system, a competitor's system, or a prototype of some sort. It represents typical user data. The big surprises come as you discover what <i>typical</i> means. This is most likely to reveal defects and misunderstandings in requirements analysis.</P>

					<p>Synthetic data is artificially generated, perhaps under certain statistical constraints. You may need to use synthetic data for any of the following reasons.</P>

					<ul>
<lI><P>You need a lot of data, possibly more than any real-world sample could provide. You might be able to use the real-world data as a seed to generate a larger sample set, and tweak certain fields that need to be unique.</P>
</LI>
<li><p>You need data to stress the boundary conditions. This data may be completely synthetic: date fields containing February 29, 1999, huge record sizes, or addresses with foreign postal codes.</p>
</LI>
<LI><p>You need data that exhibits certain statistical properties. Want to see what happens if every third transaction fails? Remember the sort algorithm that slows to a crawl when handed presorted data? You can present data in random or sorted order to expose this kind of weakness.</p>

						</li>
</UL>
				
				<H5>Exercising GUI Systems</H5>
					<p>Testing GUI-intensive systems often requires specialized testing tools. These tools may be based on a simple event capture/playback model, or they may require specially written scripts to drive the GUI. Some systems combine elements of both.</p>

					<p>Less sophisticated tools enforce a high degree of coupling between the version of software being tested and the test script itself: if you move a dialog box or make a button smaller, the test may not be able to find it, and may fall. Most modern GUI testing tools use a number of different techniques to get around this problem, and try to adjust to minor layout differences.</p>

					<P>However, you can't automate everything. Andy worked on a graphics system that allowed the user to create and display nondeterministic visual effects which simulated various natural phenomena. Unfortunately, during testing you couldn't just grab a bitmap and compare the output with a previous run, because it was designed to be different every time. For situations such as this one, you may have no choice but to rely on manual interpretation of test results.</P>

					<P>One of the many advantages of writing decoupled code (see <A href="020161622x_snode55.html">Decoupling and the Law of Demeter</a>) is more modular testing. For instance, for data processing applications that have a GUI front end, your design should be decoupled enough so that you can test the application logic <i>without</i> having a GUI present. This idea is similar to testing your subcomponents first. Once the application logic has been validated, it becomes easier to locate bugs that show up with the user interface in place (it's likely that the bugs were created by the user-interface code).</p>

				
				<h5>Testing the Tests</h5>
					<p>Because we can't write perfect software, it follows that we can't write perfect test software either. We need to test the tests.</p>

					<p>Think of our set of test suites as an elaborate security system, designed to sound the alarm when a bug shows up. How better to test a security system than to try to break in?</p>

					<p>After you have written a test to detect a particular bug, <i>cause</i> the bug deliberately and make sure the test complains. This ensures that the test will catch the bug if it happens for real.</p>

					<diV CLAss="note"><p clAsS="notetitle"><b>Tip 64</b></p><P>

						<P>Use Saboteurs to Test Your Testing</P>

					</P></Div>
<br>
<BR>

					<P>If you are <I>really</i> serious about testing, you might want to appoint a <i>project saboteur.</i> The saboteur's role is to take a separate copy of the source tree, introduce bugs on purpose, and verify that the tests will catch them.</p>

					<P>When writing tests, make sure that alarms sound when they should.</P>

				
				<H5>Testing Thoroughly</H5>
					<p>Once you are confident that your tests are correct, and are finding bugs you create, how do you know if you have tested the code base thoroughly enough?</p>

					<p>The short answer is "you don't," and you never will. But there are products on the market that can help. These <i>coverage analysis</I> tools watch your code during testing and keep track of which lines of code have been executed and which haven't. These tools help give you a general feel for how comprehensive your testing is, but don't expect to see 100% coverage.</P>

					<P>Even if you do happen to hit every line of code, that's not the whole picture. What is important is the number of states that your program may have. States are not equivalent to lines of code. For instance, suppose you have a function that takes two integers, each of which can be a number from 0 to 999.</P>

					<pre>
						
    <b>int</b> test(<b>int</b> a, <b>int</b> b) {
      <b>return</b> a / (a + b);
    }
</pre>

					<p>In theory, this three-line function has 1,000,000 logical states, 999,999 of which will work correctly and one that will not (when <tt claSS="monofont">a + b</TT> equals zero). Simply knowing that you executed this line of code doesn't tell you that—you would need to identify all possible states of the program. Unfortunately, in general this is a <i>really hard</i> problem. Hard as in, "The sun will be a cold hard lump before you can solve it."</p>

					<diV cLass="note"><P CLASs="notetitle"><b>Tip 65</b></p><P>

						<P>Test State Coverage, Not Code Coverage</P>

					</P></div>
<bR>
<BR>

					<P>Even with good code coverage, the data you use for testing still has a huge impact, and, more importantly, the <i>order</i> in which you traverse code may have the largest impact of all.</p>

				
			
			<h4>When to Test</H4>
				<P>Many projects tend to leave testing to the last minute—right where it will be cut against the sharp edge of a deadline.<FOnt size="1"><sup><a href="#FOOTNOTE-8">[8]</a></sup></foNT>
 We need to start much sooner than that. As soon as any production code exists, it needs to be tested.</P>
<BlockqUoTe><foNT SIZe="1">
<p clASS="footnote">
<Sup><a nAME="FOOTNOTE-8">[8]</A></sup>
<b>dead<SUP>.</Sup>line</b> \ded-lîn\ <i>n</i> (1864) a line drawn within or around a prison that a prisoner passes at the risk of being shot—<i>Webster's Collegiate Dictionary.</i></p>
</font></blockquoTE>

					
				<P>Most testing should be done automatically. It's important to note that by "automatically" we mean that the test <I>results</i> are interpreted automatically as well. See <a href="020161622x_snode77.html">Ubiquitous Automation</a>, for more on this subject.</P>

				<p>We like to test as frequently as we can, and always before we check code into the source repository. Some source code control systems, such as Aegis, can do this automatically. Otherwise, we just type</p>

				<pRE>
					
    % make test
</PRE>

				<p>Usually, it isn't a problem to run regressions on all of the individual unit tests and integration tests as often as needed.</p>

				<p>But some tests may not be easily run on a such a frequent basis. Stress tests, for instance, may require special setup or equipment, and some hand holding. These tests may be run less often—weekly or monthly, perhaps. But it is important that they be run on a regular, scheduled basis. If it can't be done automatically, then make sure it appears on the schedule, with all the necessary resources allocated to the task.</p>

			
			<H4>Tightening the Net</H4>
				<P>Finally, we'd like to reveal the single most important concept in testing. It is an obvious one, and virtually every textbook says to do it this way. But for some reason, most projects still do not.</P>

				<p>If a bug slips through the net of existing tests, you need to add a new test to trap it next time.</p>

				<diV CLAss="note"><p cLASS="notetitle"><b>Tip 66</b></p><p>

					<p>Find Bugs Once</p>

				</p></div>
<br>
<br>

				<p>Once a human tester finds a bug, it should be the <i>last</i> time a human tester finds that bug. The automated tests should be modified to check for that particular bug from then on, every time, with no exceptions, no matter how trivial, and no matter how much the developer complains and says, "Oh, that will never happen again."</p>

				<p>Because it will happen again. And we just don't have the time to go chasing after bugs that the automated tests could have found for us. We have to spend our time writing new code—and new bugs.</p>

				<H5>Related sections include:</H5>
					<UL>
<li><p><a href="020161622x_snode13.html">The Cat Ate My Source Code</a></p>
</lI>
<LI><P><A href="020161622x_snode37.html">Debugging</A></P>
</LI>
<li><p><a href="020161622x_snode55.html">Decoupling and the Law of Demeter</a></p>
</li>
<LI><P><A href="020161622x_snode64.html">Refactoring</a></p>
</li>
<li><p><a href="020161622x_snode65.html">Code That's Easy to Test</a></p>
</li>
<LI><P><A href="020161622x_snode77.html">Ubiquitous Automation</a></P>

						</lI>
</ul>
				
				<h5>Challenges</H5>
					<UL>
<LI><p>Can you automatically test your project? Many teams are forced to answer "no." Why? Is it too hard to define the acceptable results? Won't this make it hard to prove to the sponsors that the project is "done"?</p>

							<p>Is it too hard to test the application logic independent of the GUI? What does this say about the GUI? About coupling?</p>

						</LI>
</UL>
				
			
		</font>
 
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#CCCCCC">
<td><font color="black" size=1>I l<font color="#FF0000">@</font>ve RuBoard</td>
<td valign="top" class="v2" align="right"><a href="020161622x_snode77.html"><img src="FILES/previous.gif" width="62" height="15" border="0" align="absmiddle" alt="Previous Section"></a> <a href="020161622x_snode79.html"><img src="FILES/next.gif" width="41" height="15" border="0" align="absmiddle" alt="Next Section"></a></td></table>
</BODY></HTML>